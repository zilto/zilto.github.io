"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[4031],{4108:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"rag","metadata":{"permalink":"/blog/rag","source":"@site/blog/2023-09-08-rag/index.md","title":"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!","description":"Off-the-shelf LLMs are excellent at manipulating and generating text, but they only know general facts about the world and probably very little about your use case. Retrieval augmented generation (RAG) refers not to a single algorithm, but rather a broad approach to provide relevant context to an LLM. As industry applications mature, RAG strategies will be tailored case-by-case to optimize relevance, business outcomes, and operational concerns.","date":"2023-09-08T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"Retrieval augmented generation","permalink":"/blog/tags/retrieval-augmented-generation"},{"inline":true,"label":"OpenAI","permalink":"/blog/tags/open-ai"},{"inline":true,"label":"LLM","permalink":"/blog/tags/llm"},{"inline":true,"label":"FastAPI","permalink":"/blog/tags/fast-api"},{"inline":true,"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":18.74,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"rag","title":"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!","authors":"tj","tags":["Hamilton","Retrieval augmented generation","OpenAI","LLM","FastAPI","Docker"]},"unlisted":false,"nextItem":{"title":"Containerized PDF Summarizer with FastAPI and Hamilton","permalink":"/blog/pdf-summarizer"}},"content":"Off-the-shelf LLMs are excellent at manipulating and generating text, but they only know general facts about the world and probably very little about your use case. Retrieval augmented generation (RAG) refers not to a single algorithm, but rather a broad approach to provide relevant context to an LLM. As industry applications mature, RAG strategies will be tailored case-by-case to optimize relevance, business outcomes, and operational concerns.\\n\\n![Alt text](image.png)\\n\\n> crosspost from https://blog.dagworks.io/p/retrieval-augmented-generation-reference-arch\\n\\n\x3c!--truncate--\x3e\\n\\nIn this post, we provide a reference RAG architecture and discuss design decisions for each component. It\u2019s ready for use, and will scale with your needs. Specifically, we\u2019ll cover how to:\\n\\n* Write ingestion and retrieval dataflows using [Hamilton](https://hamilton.dagworks.io/en/latest/)\\n* Build a backend with [FastAPI](https://fastapi.tiangolo.com/) + Hamilton backend\\n* Create a browser interface with [Streamlit](https://docs.streamlit.io/)\\n* Compute embeddings and generate text with the [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)  \\n* Use and manage a [Weaviate](https://weaviate.io/developers/weaviate) vector store\\n* Build a containerized app using [Docker](https://docs.docker.com/reference/)\\n\\n> Find the code on [GitHub](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/retrieval_augmented_generation)\\n> This publication extends our previous [PDF Summarizer](https://blog.dagworks.io/p/containerized-pdf-summarizer-with)\\n\\n## What is RAG and why do you need it?\\nLarge language models (LLMs) learn to write coherent sentences by being exposed to enormous corpora of texts. In the process, the model stores facts about the world. However, determining what it knows or what it doesn\u2019t is still a key theoretical challenge. The above generally describes *pre-trained*, *foundational* or *base* models, which are models not yet refined for a particular use-case (see model card of [bert-base-uncased](https://huggingface.co/bert-base-uncased) for details). For instance, GPT stands for *generative pre-trained transformer*, and ChatGPT is a fine-tuned version for chat applications.\\n\\nA primary concern for LLM applications, outside of creative work, is the factual correctness of answers. This challenge can be mitigated by adopting one or many of the following techniques:\\n\\n* **Fine-tuning** consists of further training a pre-trained model on curated examples for a specific task. The model learns domain-specific language and facts*, which improves the quality of embeddings and text generation relative to the domain (e.g., insurance, health). \\n\\n* **Instruction-tuning** is a form of fine-tuning that uses instruction-answer pairs. The model learns *how* to respond to queries (summarize, make a list, think step by step, etc.). Models labeled as *chat* (e.g., [llama-2-7b-chat](https://huggingface.co/meta-llama/Llama-2-7b-chat)) are great for general human interaction, and can be further fine-tuned to your domain.\\n\\n* **Retrieval augmented generation (RAG)** is a multistep process to retrieve information relevant for a query, and pass it to the LLM as context to generate an answer. RAG is the most flexible approach for adding and updating knowledge since it only requires to change the available sources (e.g., files, internet pages) rather than updating the LLM.\\n\\nLooking forward, retrieval will continue to be a key architectural component of LLM applications because editing LLM knowledge directly is an unsolved problem. Fine-tuning, in most cases, should be a later concern since it would also improve your RAG strategy if you have one in place. Furthermore, the LLM is the most likely component to be improved upon and replaced, requiring fine-tuning again. For these reasons, we suggest starting with an off-the-shelf LLM and implementing your own RAG system as a first step to improve the knowledge your LLM operates over.\\n\\n## Introducing Hamilton\\n\\n[Hamilton](https://hamilton.dagworks.io/en/latest/) is a declarative micro-framework to describe [dataflows](https://en.wikipedia.org/wiki/Dataflow) in Python. Its strength is expressing the flow of data and computation in a straightforward and easy to maintain manner (much like dbt does for SQL). It has minimal dependencies and can run anywhere Python runs, meaning the same code will work in development notebooks, scripts, Spark clusters, or production web-services. Hamilton is not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows; and it extends nicely to modeling LLM workflows!\\n\\n![Alt text](image-1.png)\\n\\nThe picture above encapsulates the function-centric declarative approach of Hamilton. The function\u2019s name is tied to its outputs and its arguments define what data it depends on. This allows Hamilton to read functions found in a module and automatically generate the DAG to be executed. This paradigm incentivizes developers to write small modular functions instead of scripts or larger functions, without sacrificing iteration speed.\\n\\nAs a result, it is easier to:\\n* Write and maintain custom application logic\\n* View operation lineage and debug results   \\n* Update components of your stack\\n* Reuse function implementations across contexts (e.g., notebook, pipeline, web service)\\n\\n> If you are new to Hamilton, feel free to visit our interactive browser demo at\\nhttps://www.tryhamilton.dev/\\n\\n## Building a modular RAG application\\n![Alt text](image-2.png)\\n\\nOur example RAG application allows users to import PDF files, extract and store the text chunks, and query the system. These different operations are implemented as dataflows with Hamilton and are exposed via FastAPI endpoints. The backend communicates with OpenAI to embed documents and generate answers, and uses a local Weaviate vector store instance to store and retrieve documents. The frontend is built with Streamlit and exposes the different functionalities via a simple web user interface (UI). Everything is packaged as containers with `docker compose`, so you can run it anywhere Docker runs.\\n\\n> Find the example on [GitHub](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/retrieval_augmented_generation)\\n\\nLet\u2019s walk through its structure:\\n\\n```title=\\"Directory structure\\"\\n.\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 .env\\n\u251c\u2500\u2500 build_app.sh\\n\u251c\u2500\u2500 docker-compose.yaml\\n\u251c\u2500\u2500 backend\\n\u2502   \u251c\u2500\u2500 Dockerfile\\n\u2502   \u251c\u2500\u2500 requirements.txt\\n\u2502   \u251c\u2500\u2500 __init__.py\\n\u2502   \u251c\u2500\u2500 ingestion.py\\n\u2502   \u251c\u2500\u2500 retrieval.py\\n\u2502   \u251c\u2500\u2500 server.py\\n\u2502   \u251c\u2500\u2500 vector_db.py\\n\u2502   \u2514\u2500\u2500 docs\\n\u2502       \u251c\u2500\u2500 documents.png\\n\u2502       \u251c\u2500\u2500 rag_summary.png\\n\u2502       \u251c\u2500\u2500 store_arxiv.png\\n\u2502       \u2514\u2500\u2500 store_pdfs.png\\n\u2514\u2500\u2500 frontend\\n    \u251c\u2500\u2500 Dockerfile\\n    \u251c\u2500\u2500 requirements.txt\\n    \u251c\u2500\u2500 assets\\n    \u2502   \u2514\u2500\u2500 hamilton_logo.png\\n    \u251c\u2500\u2500 __init__.py\\n    \u251c\u2500\u2500 client.py\\n    \u251c\u2500\u2500 Information.py\\n    \u2514\u2500\u2500 pages\\n        \u251c\u2500\u2500 1_Ingestion.py\\n        \u2514\u2500\u2500 2_Retrieval.py\\n```\\n\\nThe repository is divided between the backend and the frontend, each with their own `Dockerfile` and `requirements.txt`. The backend has `server.py` which defines the FastAPI endpoints, and several `.py` files containing the Hamilton functions for the RAG workflow that interact with the vector store. You can think of each of these modules analogous to \u201cchains\u201d you\u2019d find in Langchain. The frontend has `client.py` which handles `HTTP` requests to the backend and a [multipage Streamlit app](https://docs.streamlit.io/library/get-started/multipage-apps/create-a-multipage-app) using `Information.py`, `pages/1_Ingestion.py`, and `pages/2_Retrieval.py`.  The file `Introduction.py` is the app entrypoint and landing page (it\u2019s not the clearest file name, but it defines how the page will be displayed within the Streamlit UI). \\n\\n## Hamilton dataflows a.k.a. \u201cchains\u201d\\nA RAG application can be divided into 3 main steps: ingestion, retrieval, and generation. Importantly, the ingestion step can be done at any time (e.g., continuously, periodically in batch, event-driven) while retrieval and generation always happen together in our case, i.e., when a user makes a query. For this reason, we implemented ingestion in `ingestion.py` and retrieval + generation in `retrieval.py`. This way, it would be trivial to reuse our Hamilton ingestion code in a macro-orchestrated pipeline for daily updates of, for example, any newly stored documents; see how to use Hamilton with [Airflow](../2023-06-28-airflow-hamilton/), [Prefect](../2023-07-25-prefect-hamilton/) for ideas on how that would work. \\n\\nAlso, you\u2019ll notice a separate `vector_db.py` which implements a small set of functionalities to interact with Weaviate. These functions are quite simple, but it allows us to integrate our vector store operations to the broader dataflow, have granular visibility over operations, and handle exceptions. It also enables use to replace Weaviate easily if we wanted to choose another vector store. See [our modular LLM stack post](../2023-07-11-modular-llm/) for more details on how to do that.\\n\\n> Below we\u2019ll discuss our dataflow design decisions, for a more hands-on explanation of Hamilton, see our [PDF Summarizer example](../2023-08-18-pdf-summarizer/).\\n\\n### Ingestion flow\\nThe ingestion flow allows users to upload arbitrary PDF documents, extract text content, chunk it, get embeddings from OpenAI, and store text chunks with their embedding in Weaviate. To make this demo more engaging, we added functionalities to directly search [arxiv.org](https://arxiv.org/) and store the selected scientific articles.\\n\\nBelow is the DAG for `ingestion.py`\\n![Alt text](image-3.png)\\n\\nNodes with dotted outlines and the `Input:` prefix are external to the `ingestion.py` module and need to be provided to the `Hamilton Driver` as inputs at execution time. Doubled lines and the different arrow types describe [parallelizable code paths](https://hamilton.dagworks.io/en/latest/concepts/hamilton-function-structure/#dynamic-dags), which can be used to improve processing performance. Towards the bottom, `pdf_collection` collects all the chunks and stores them in a single [structured object in Weaviate](https://weaviate.io/developers/weaviate/tutorials/schema). \\n\\nNotice the path `local_pdfs` (near the center of the image) is part of; everything above is arXiv specific, and below is generic. This design decision allows us to reuse the same logic to store  PDFs from arXiv as any other PDF. As our application grows, we could store the arxiv functions in a separate `arxiv.py` module, and add code upstream of `local_pdfs` to load files from other sources. For example, you can visually imagine \u201ccutting\u201d the above diagram at `local_pdfs`, and then swapping in a different \u201cimplementation\u201d, i.e. dataflow.\\n\\nAlso, we don\u2019t do any sophisticated text processing, but we could easily add more functions between `raw_text` and `chunked_text`. By removing irrelevant text from PDFs (e.g., article references, markdown tables), we could reduce the amount of tokens to process, send to OpenAI, and to then store in our vector store. Adopting a smart processing strategy will both improve the quality of retrieval and lead to performance and cost optimization at scale. \\n\\nChanges to the ingestion dataflow (e.g., preprocessing, chunking, embedding model) should generally be followed by reprocessing all documents and recomputing embeddings. Visualizing downstream dependencies of changes is helpful to prevent breaking changes. For example, changing the embedding model will make stored documents incompatible by changing the notion of distance for retrieval and possibly having incompatible vector dimensions. Using the [Weaviate backup features](https://weaviate.io/developers/weaviate/configuration/backups) can make rollbacks easier and prevent having to re-compute embeddings and summaries, saving dollars and headaches.\\n\\n### Retrieval flow\\nThe retrieval flow can appear complicated at first, but essentially it starts by getting the text embedding for the user query and doing a [hybrid search](https://weaviate.io/developers/academy/zero_to_mvp/queries_2/hybrid) in Weaviate to find the most relevant stored chunks. For each chunk, it checks if a text summary was already generated; if not, the chunk is passed with a prompt to OpenAI\u2019s chat model to generate a summary. The generated summaries of all chunks are collected and sorted according to the original Weaviate relevance ranking, to then be sent again to OpenAI to \u201creduce\u201d summaries, i.e., make a summary of summary.\\n\\nHere is the DAG for retrieval.py\\n![Alt text](image-4.png)\\n\\nA key decision was to specify prompt templates as function nodes (e.g., `prompt_to_summarize_chunk`, `prompt_to_reduce_summaries`) and not inputs (dotted outline). These prompt functions receive relevant context (e.g., text chunk), adds it to an f-string, and returns the formatted string. By storing them directly with the dataflow code, assessing behavior, versioning and debugging become much easier. We go into greater detail on how to manage prompts in [LLMOps: Production prompt engineering patterns with Hamilton](https://blog.dagworks.io/p/llmops-production-prompt-engineering).\\n\\nAs shown in the figure, the `rag_query` from the user only affects the initial chunk vector search and the `rag_summary` step which makes a \u201csummary of summaries\u201d. Accordingly, the function to summarize a chunk `chunk_with_new_summary` doesn\u2019t depend on the user\u2019s input allowing us to generate \u201cgeneric\u201d summaries for each chunk. This approach has the benefits of making chunk summaries reusable which can largely decrease cost and reduce latency. The downside is that the chunk summaries are less specific to the query and might decrease answer quality.\\n\\n### Weaviate vector database\\n[Weaviate](https://weaviate.io/developers/weaviate) offers a specialized type of infrastructure to efficiently store and compare text embeddings (i.e., vectors) at scale. In the context of RAG, the LLM represents the semantic of chunks of text as vectors, and the vector database defines the notion of similarity or relevance. Vector databases come in [various forms](https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696), but we decided to use Weaviate for a few reasons:\\n\\n* **Classes and structured objects**. For each PDF, we create a `Document` object and `Chunk` objects and link them together with their respective properties `containsChunk` and `fromDocument`. With structured objects, we can retrieve a `Document` based on the relevance score of its `Chunks`; for example, by computing the `groupby sum` of the relevance of its chunks.\\n\\n* **Vectors and data in one place**. Weaviate allows you to store the text along the vectors while other vector infrastructure only handle vectors. The latter requires managing and syncing a separate storage for documents which complexifies both ingestion and retrieval. Many data types are supported (e.g., strings, numbers, boolean, dates); we even store the full PDF of each `Document` as a `base64` blob.\\n\\n* **Expressive retrieval**. Weaviate has a REST API for bulk operations and a GraphQL API for object retrieval. While learning GraphQL can be daunting, using it from the Python SDK is easy to approach (see example below). Additionally, many modes of retrieval are offered (vector, multimodal, keyword, etc.). For this RAG example, we used the [hybrid search](https://weaviate.io/developers/weaviate/search/hybrid), which combines vector and keyword search.\\n\\n```python title=\\"Weaviate GraphQL query\\"\\nresponse = (\\n    weaviate_client.query.get(\\n        \\"Chunk\\",  # Class\\n        [  # Properties to retrieve\\n            \\"chunk_index\\",\\n            \\"content\\",\\n            \\"summary\\",\\n            \\"fromDocument {... on Document {_additional{id}}}\\",  # Property of linked object\\n        ],\\n    )\\n    .with_hybrid(  # hybrid search parameters\\n        query=rag_query,  # user text query\\n        properties=[\\"content\\"],  # properties of `Chunk` to search on (specified above)\\n        vector=query_embedding,  # user query embedding/vector\\n        alpha=hybrid_search_alpha,  # hybrid search parameter\\n    )\\n    .with_additional([\\"score\\"])  # compute relevance score\\n    .with_limit(retrieve_top_k)  # return top k objects\\n    .do()\\n)\\n```\\n\\n### FastAPI server\\n[FastAPI](https://fastapi.tiangolo.com/) is used to define the server endpoints: `/store_arxiv`, `/store_pdfs`, `/rag_summary`, `/documents`, but for all of them, the code executed is actually handled by Hamilton. At startup, the server instantiates a `Hamilton Driver` with all the necessary Python modules (see [FastAPI Lifespan Events](https://fastapi.tiangolo.com/advanced/events/)). Then, the function body of FastAPI endpoint consists of accessing the global `Hamilton Driver`, calling `Driver.execute()` with the necessary variables, and returning a formatted response. Using Hamilton guarantees that the code powering your web service will run identically to your development notebooks or orchestrated pipelines.\\n\\n```python title=\\"FastAPI server snippet\\"\\nfrom contextlib import asynccontextmanager\\nfrom dataclasses import dataclass\\n\\nimport fastapi\\nimport pydantic\\nfrom fastapi.responses import JSONResponse\\n\\nfrom hamilton import driver\\n\\n\\n# define a global dataclass that is shared across endpoints\\n@dataclass\\nclass GlobalContext:\\n    vector_db_url: str\\n    hamilton_driver: driver.Driver\\n\\n\\n@asynccontextmanager\\nasync def lifespan(app: fastapi.FastAPI) -> None:\\n    \\"\\"\\"Startup and shutdown logic of the FastAPI app\\n    Above yield statement is at startup and below at shutdown\\n    Import the Hamilton modules and instantiate the Hamilton driver\\n    \\"\\"\\"\\n    # import the Python modules containing your dataflows\\n    import ingestion\\n    import retrieval\\n    import vector_db\\n\\n    driver_config = dict()\\n\\n    dr = (\\n        driver.Builder()\\n        .enable_dynamic_execution(allow_experimental_mode=True)  # to allow Parallelizable/Collect\\n        .with_config(driver_config)\\n        .with_modules(ingestion, retrieval, vector_db)  # pass our dataflows\\n        .build()\\n    )\\n\\n    # make the variable global to reuse it within endpoints\\n    global global_context\\n    global_context = GlobalContext(vector_db_url=\\"http://weaviate_storage:8083\\", hamilton_driver=dr)\\n\\n    # execute Hamilton code to make sure the Weaviate class schemas is instantiated\\n    global_context.hamilton_driver.execute(\\n        [\\"initialize_weaviate_instance\\"], inputs=dict(vector_db_url=global_context.vector_db_url)\\n    )\\n\\n    # anything above yield is executed at startup\\n    yield\\n    # anything below yield is executed at teardown\\n\\n\\n# instantiate the FastAPI app\\napp = fastapi.FastAPI(\\n    title=\\"Retrieval Augmented Generation with Hamilton\\",\\n    lifespan=lifespan,  # pass the lifespan context\\n)\\n\\n# define a POST endpoint\\n@app.post(\\"/store_arxiv\\", tags=[\\"Ingestion\\"])\\nasync def store_arxiv(arxiv_ids: list[str] = fastapi.Form(...)) -> JSONResponse:\\n    \\"\\"\\"Retrieve PDF files of arxiv articles for arxiv_ids\\\\n\\n    Read the PDF as text, create chunks, and embed them using OpenAI API\\\\n\\n    Store chunks with embeddings in Weaviate.\\n    \\"\\"\\"\\n    global_context.hamilton_driver.execute(\\n        [\\"store_documents\\"],\\n        inputs=dict(\\n            arxiv_ids=arxiv_ids,\\n            embedding_model_name=\\"text-embedding-ada-002\\",\\n            data_dir=\\"./data\\",\\n            vector_db_url=global_context.vector_db_url,\\n        ),\\n    )\\n\\n    return JSONResponse(content=dict(stored_arxiv_ids=arxiv_ids))\\n```\\n\\nThe snippet also shows that we execute the `Hamilton Driver` with `\u201cinitialize_weaviate_instance\u201d` at startup to ensure the Weaviate schema exists and the vector store is available. Hamilton helps with keeping the endpoint functions brief and brings a nice separation of concerns. For example, if an error occurred, you can reproduce by using the logged request with Hamilton outside of FastAPI. If it succeeds, then the problem is likely related to the service and not the dataflow itself. We\u2019ll discuss unit testing and integration testing in a future post, make sure to [subscribe](https://blog.dagworks.io/subscribe) to be notified!\\n\\nA core feature of Hamilton is the automatically generated DAG visualization, which complements FastAPI\'s automated [Swagger UI documentation](https://fastapi.tiangolo.com/tutorial/metadata/). When running the example code, visit http://localhost:8082/docs to explore it yourself! FastAPI allows you to [add request examples](https://fastapi.tiangolo.com/tutorial/schema-extra-example/) to your code, which helps users learn your API and your team move faster as you develop and test things. \\n\\n![Alt text](image-5.png)\\n\\n### Streamlit frontend\\n[Streamlit](https://docs.streamlit.io/) allows you to build a user interface quickly using Python. Again, with the goal of modularity, we decided to build a [multipage Streamlit app](https://docs.streamlit.io/library/get-started/multipage-apps/create-a-multipage-app) with independent pages for information, ingestion, and retrieval. The file `client.py` defines the HTTP requests to interact with the server, which helps errors related to the client-server communication from those associated with the UI. An important consideration when using Streamlit is that the entire code is executed whenever a page is refreshed, so avoid having operations that are computationally intensive or incur costs (e.g., LLM API calls). \\n\\nThe design of your search UI is a significant decision as it will largely [influence how people use your application](https://www.algolia.com/blog/ux/7-examples-of-great-site-search-ui/). For the ingestion page, we made sure to display the currently stored documents and provide feedback via a \u201cspinner\u201d widget during long ingestion operations. For the retrieval page, we exposed hybrid search parameters such as alpha and top k with informational tooltips to allow users to play around, but kept hidden the prompts used to summarize documents. After making a RAG query, the app will display the query, the answer, and the source chunks with their summary to allow you to reason over the answer. After more than one query, a slider will allow you to browse through your history of searches and compare answers to different queries. Semantic search has enabled news ways to retrieve information across documents, but finding the ideal UI for it remains an unsolved problem and we encourage you to iterate over it! \\n\\n### Docker services\\nHere\u2019s the `docker-compose.yaml` file that manages three containers:\\n```yaml title=\\"Docker compose file\\"\\nversion: \\"3.4\\"\\nservices:\\n  api:\\n    container_name: fastapi_server\\n    build: backend/.\\n    command: \\"uvicorn server:app --host 0.0.0.0 --port 8082\\"\\n    ports:\\n      - \\"8082:8082\\"\\n    environment:\\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\\n      - DAGWORKS_API_KEY=${DAGWORKS_API_KEY}\\n    networks:\\n      - rag\\n\\n  app:\\n    container_name: streamlit_app\\n    build: frontend/.\\n    command: \\"streamlit run --server.port 8080 --server.enableCORS false Information.py\\"\\n    ports:\\n      - \\"8080:8080\\"\\n    networks:\\n      - rag\\n\\n  weaviate:\\n    image: semitechnologies/weaviate:1.19.8\\n    container_name: weaviate_storage\\n    command: --host 0.0.0.0 --port \'8083\' --scheme http\\n    ports:\\n    - 8083:8083\\n    restart: on-failure:0\\n    environment:\\n      QUERY_DEFAULTS_LIMIT: 25\\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: \'true\'\\n      PERSISTENCE_DATA_PATH: \'/var/lib/weaviate\'\\n      DEFAULT_VECTORIZER_MODULE: \'none\'\\n      ENABLE_MODULES: \'\'\\n      CLUSTER_HOSTNAME: \'node1\'\\n    networks:\\n     - rag\\n\\nnetworks:\\n  rag:\\n```\\nThis sets the main configuration of each service. For the FastAPI backend (api service), we pass the OpenAI key to the FastAPI backend and optionally set the DAGWorks API key. The Weaviate configuration is generated by their [interactive tool](https://weaviate.io/developers/weaviate/installation/docker-compose). Then, for each service the command section sets their entry point. They are all connected together via the `rag` [bridge network](https://docs.docker.com/network/drivers/bridge/). This allows services to communicate via a local URL of the following format `http://{container_name}:{port}` (e.g., `http://fastapi_server:8082`). You could want to have your vector store less tightly coupled if it is used by other applications.\\n\\nWhen developing locally, containers can be accessed via` http://127.0.0.1:{port}` (try not to use [localhost](https://www.youtube.com/watch?v=98SYTvNw1kw)). You can keep containers running and call `docker compose up -d \u2013build` to rebuild them as you make changes. In particular, this is useful to view frontend UI changes and test backend changes manually via the Swagger UI at http://localhost:8082/docs.  To see container logs, you can use `docker compose logs -f`  to see the logs from applications from their respective containers.\\n\\n## Limitations\\nThis example aims to be a reference architecture, and demonstrate how to build a RAG system and give you a solid basis to start your own project. However, it is by no means perfect. Here\u2019s a list of limitations or areas we could improve upon:\\n\\n**arXiv downloads**. The ingestion dataflow for arXiv files requires downloading the PDF of articles locally (on the FastAPI container) before subsequent steps. That is PDF files could fill up your docker container. A better approach would be to use the built-in [tempfile library](https://docs.python.org/3/library/tempfile.html) for `tempfile.NamedTemporaryFile` (note that tempfile is an area of active Python development and saw several changes since 3.8). For instance, PDF files sent to FastAPI (via Streamlit or POST) use temporary files via `streamlit.runtime.UploadedFile` and `fastapi.Uploadfile`.\\n\\n**Weaviate duplicates**. Having duplicate sources in your vector store can reduce the quality of retrieval. You\u2019d most likely prefer the \u201c5 most-relevant, but somewhat distinct\u201d chunks from the \u201c5-most relevant and almost identical\u201d chunks to generate your answer. However, the presented RAG system doesn\u2019t prevent you from uploading duplicate documents during ingestion and finding duplicate / nearly files is a generally complex problem. One approach would be to first select the sets of chunks with very similar embeddings (potential duplicate) then use [fuzzy matching](https://github.com/seatgeek/thefuzz) (which is much less computationally efficient) on the chunk\u2019s text to score potential duplicates. The downside is that it requires compute and can impact latency having to do this for the embedding of every chunk before deduplication.\\n\\n**REST API conventions**. This example showcases a client-server architecture with the ingestion and retrieval of a RAG workflow. However, the design and the naming of the endpoints don\u2019t follow the [REST best practices](https://restfulapi.net/resource-naming/). Following these conventions improves the semantics and the readability of your project, which is critical from proper downstream use. We will improve and update our API definition in our future post about testing. Make sure to subscribe to be notified when it we release it!\\n\\n**Client-server communication**. You could use [Pydantic](https://docs.pydantic.dev/latest/) more extensively to define the FastAPI [requests and response types](https://fastapi.tiangolo.com/tutorial/response-model). This code could be used in both the `backend/server.py` and `frontend/client.py` to make development less error prone.\\n\\n## Summary\\nWe covered a lot in this post. Most importantly, we leave you with a reference architecture blueprint to get started with RAG applications. To use it, we suggest you should define your application\u2019s dataflows, then create the endpoints for the operations supported by your server, and finally build the best-suited user interface for your application. Congratulations for getting through! Please bookmark this post, and feel free to revisit sections of this post as you make progress; if something isn\u2019t clear please leave a comment/or suggest a PR to the repo to improve this example.\\n\\nYou might be interested by other posts in this series:\\n* [LLMOps: Production prompt engineering patterns with Hamilton](https://blog.dagworks.io/p/llmops-production-prompt-engineering)\\n* [Building a maintainable and modular LLM application stack with Hamilton](../2023-07-11-modular-llm)\\n\\n## We want to hear from you!\\nIf you\u2019re excited by any of this, or have strong opinions, leave a comment, or drop by our Slack channel! Some links to do praise/complain/chat:\\n\\n\ud83d\udce3 join our community on [Slack](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email) \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started.\\n\\n\u2b50\ufe0f us on [GitHub](https://github.com/dagworks-inc/hamilton).\\n\\n\ud83d\udcdd leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you find something.\\n\\n\ud83d\udcda read our [documentation](https://hamilton.dagworks.io/en/latest/).\\n\\n\u2328\ufe0f [interactively learn](https://www.tryhamilton.dev/) about Hamilton in your browser."},{"id":"pdf-summarizer","metadata":{"permalink":"/blog/pdf-summarizer","source":"@site/blog/2023-08-18-pdf-summarizer/index.md","title":"Containerized PDF Summarizer with FastAPI and Hamilton","description":"Skip learning convoluted LLM-specific frameworks and write your first LLM application using regular Python functions and Hamilton! In this post, we\u2019ll present a containerized PDF summarizer powered by the OpenAI API. Its flow is encoded in Hamilton, which the FastAPI backend runs and exposes as an inference endpoint. The lightweight frontend uses Streamlit and exercises the backend. (GitHub repo)","date":"2023-08-18T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"OpenAI","permalink":"/blog/tags/open-ai"},{"inline":true,"label":"LLM","permalink":"/blog/tags/llm"},{"inline":true,"label":"FastAPI","permalink":"/blog/tags/fast-api"},{"inline":true,"label":"Docker","permalink":"/blog/tags/docker"}],"readingTime":14.58,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"pdf-summarizer","title":"Containerized PDF Summarizer with FastAPI and Hamilton","authors":"tj","tags":["Hamilton","OpenAI","LLM","FastAPI","Docker"]},"unlisted":false,"prevItem":{"title":"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!","permalink":"/blog/rag"},"nextItem":{"title":"Featurization: Integrating Hamilton with Feast","permalink":"/blog/feast-hamilton"}},"content":"Skip learning convoluted LLM-specific frameworks and write your first LLM application using regular Python functions and [Hamilton](https://github.com/dagWorks-Inc/hamilton)! In this post, we\u2019ll present a containerized PDF summarizer powered by the [OpenAI API](https://platform.openai.com/docs/api-reference). Its flow is encoded in Hamilton, which the [FastAPI](https://fastapi.tiangolo.com/) backend runs and exposes as an inference endpoint. The lightweight frontend uses [Streamlit](https://docs.streamlit.io/) and exercises the backend. ([GitHub repo](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer))\\n\\n![Alt text](image.png)\\n\\n> crosspost from https://blog.dagworks.io/p/containerized-pdf-summarizer-with\\n\\n\x3c!--truncate--\x3e\\n\\n## The first generation of large language models applications\\nLarge language models (LLMs) open up new opportunities to exploit unstructured text data and ways to interact with the computer, such as chat-based information retrieval, writing assistance, or text summarization. These models can complete many different tasks that need to be specified via a text prompt. This flexibility in terms of input and output differs from previous ML/AI initiatives in industry (e.g., forecasting, recommender systems, computer vision) that had precisely defined inputs and outputs formats. It is one of the central [challenges and complexities](https://huyenchip.com/2023/04/11/llm-engineering.html) introduced by LLMs, and furthers the need for traceability and validation steps for data pipelines.\\n\\nAccordingly, the tooling to solve these problems is nascent and changes rapidly. Not only is the best development paradigm undefined, it is a moving target since LLMs themselves are evolving. Building an application using LLMs today will almost certainly lead to dealing with breaking changes or migrations around the LLM APIs, prompt versioning, context management, storage infrastructure (e.g., vector databases), monitoring frameworks, and other service vendors.\\n\\nIn their current state, the available LLM frameworks might get you up and running, but are missing the modularity and the transparency required for a proper software development lifecycle that considers production operations.\\n\\n* How do you iterate & test the behavior of new prompts and version them?\\n* Do you version your prompt with the code & LLM used? How coupled are they?\\n* How do you run your workflows in a batch setting without a web-server?\\n* How do you monitor your system\u2019s performance and data artifacts produced without slowing down your development cycle?\\n* How do you structure a readable codebase that facilitates collaboration and allows you to understand the impacts of changes on your various workflows?\\n\\nWe\u2019ll explore through a series of posts how to overcome these challenges when adding LLM capabilities into your application. In this article, we ground the discussion around a PDF summarizer and future posts will extend the example with testing, Spark support, lineage, etc. **Subscribe to get updates as we publish them**!\\n\\n# Introducing Hamilton\\nLLMs-based applications can be expressed as [dataflows](https://en.wikipedia.org/wiki/Dataflow_programming), which boils down to modeling your program by focusing on the moving of data artifacts (prompts, context, knowledge base, generated response, etc.) with computation (what you do with the data).\\n\\n[Hamilton](https://github.com/dagWorks-Inc/hamilton) is a declarative micro-framework to describe dataflows in Python. Its strength is expressing the flow of data and computation in a straightforward and easy to maintain manner (much like dbt does for SQL). It has minimal dependencies and can run anywhere Python runs, meaning the same code will work in development notebooks, scripts, Spark clusters, or production web-services. Hamilton is not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows.\\n\\n![Alt text](image-1.png)\\n\\nThe picture above encapsulates the function-centric declarative approach of Hamilton. The function\u2019s name is tied to its outputs and its arguments define what data it depends on. This allows Hamilton to read functions found in a module and automatically generate the DAG to be executed. This paradigm incentivizes developers to write small modular functions instead of scripts or larger functions, without sacrificing iteration speed. As a result, it is easier to:\\n\\n* Read and understand the codebase\\n* Edit implementations and extend your business logic\\n* Do data validation after key steps\\n* Understand downstream consequences of changes\\n* Unit test and prevent breaking changes\\n* Reuse functions or groups of functions across projects\\n* Add in platform concerns independent of the logic encoded with Hamilton.\\n\\nAgain, to keep this post short and focused, we won\u2019t dive into how to do all the above.\\n\\n> If you have never tried Hamilton, feel free to visit our interactive browser demo at: https://www.tryhamilton.dev/\\n\\n## Hamilton for LLM and NLP flows\\nGiven the rapid progress in the LLM and tooling space, adopting a low abstraction framework like Hamilton for your application facilitates writing a modular and well-tested codebase, as well as a straightforward approach to versioning flows. Having your focus on modularity early on will facilitate future upgrades and migrations, and allow you to keep up with state-of-the-art without breaking production.\\n\\nFor LLM applications, being able to reuse code logic between development (e.g. notebooks, scripts) and production services (FastAPI, serverless services, Spark) has a large positive impact on development speed. Also, Hamilton decouples your LLM dataflow logic from your service/platform concerns. For example, the caching of OpenAI requests, application scaling, or monitoring, are platform concerns and should be separated as such. This decoupling is also helpful in a hand-off model if you operate in one; data scientists work on Hamilton modeling logic while engineers handle ensuring it runs with the appropriate monitoring in production. As you read this post, we invite you to think about how the code we show might develop and evolve in your organization \u2013 leave us a comment afterwards with your thoughts/reactions.\\n\\n## Building a modular PDF summarizer application\\n![Alt text](image-2.png)\\n\\nAt the core, the application loads a PDF as text, chunks it, and calls the [OpenAI API](https://platform.openai.com/docs/api-reference) to summarize the chunks and reduces them into a single summary. It is packaged as a frontend and a backend container using docker-compose. The frontend uses [Streamlit](https://docs.streamlit.io/), a library to write web UI using Python. When clicking on the `Summarize` button (see introduction image), an `HTTP POST` request `/summarize_sync` is made to the FastAPI backend. [FastAPI](https://fastapi.tiangolo.com/) is a library to create REST API endpoints to communicate with a server. When receiving the `/summarize_sync` request, the endpoint executes the relevant operation via an Hamilton driver instantiated on the server.\\n\\n> [Find the code on GitHub](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer)\\n\\nFirst, let\u2019s look at the directory structure:\\n```title=\\"Directory structure\\"\\n.\\n\u251c\u2500\u2500 README.md\\n\u251c\u2500\u2500 docker-compose.yaml\\n\u251c\u2500\u2500 backend\\n\u2502   \u251c\u2500\u2500 Dockerfile\\n\u2502   \u251c\u2500\u2500 requirements.txt\\n\u2502   \u251c\u2500\u2500 server.py\\n\u2502   \u2514\u2500\u2500 summarization.py\\n\u2514\u2500\u2500 frontend\\n    \u251c\u2500\u2500 Dockerfile\\n    \u251c\u2500\u2500 requirements.txt\\n    \u2514\u2500\u2500 app.py\\n```\\nAt a high-level, the frontend and the backend are in separate folders, each with their own `Dockerfile` and `requirements.txt`. The backend has `server.py` which contains the FastAPI endpoints definition and the calls to the Hamilton driver, while `summarization.py` contains the dataflow logic used by the `Hamilton Driver`. If you\u2019re coming from LangChain, you can think of `summarization.py` as the implementation of a more [modular chain](https://python.langchain.com/docs/modules/chains/). The directory structure should feel natural and intuitive, making it easy to understand for any junior colleague joining your team.\\n\\n--- \\nNow, let\u2019s look at snippets of `summarization.py` and then `server.py` powering the backend:\\n```python title=\\"Hamilton dataflows\\"\\n# summarization.py\\n# ... imports \\n\\ndef summarize_chunk_of_text_prompt(content_type: str = \\"an academic paper\\") -> str:\\n    \\"\\"\\"Base prompt for summarizing chunks of text.\\"\\"\\"\\n    return f\\"Summarize this text from {content_type}. Extract any key points with reasoning.\\\\n\\\\nContent:\\"\\n\\n  \\ndef summarize_text_from_summaries_prompt(content_type: str = \\"an academic paper\\") -> str:\\n    \\"\\"\\"Prompt for summarizing a paper from a list of summaries.\\"\\"\\"\\n    return f\\"\\"\\"Write a summary collated from this collection of key points extracted from {content_type}.\\n    The summary should highlight the core argument, conclusions and evidence, and answer the user\'s query.\\n    User query: {{query}}\\n    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\\n    Key points:\\\\n{{results}}\\\\nSummary:\\\\n\\"\\"\\"\\n\\n\\n@config.when(file_type=\\"pdf\\")\\ndef raw_text__pdf(pdf_source: str | bytes | tempfile.SpooledTemporaryFile) -> str:\\n    \\"\\"\\"Takes a filepath to a PDF and returns a string of the PDF\'s contents\\n    :param pdf_source: Series of filepaths to PDFs\\n    :return: Series of strings of the PDFs\' contents\\n    \\"\\"\\"\\n    reader = PdfReader(pdf_source)\\n    _pdf_text = \\"\\"\\n    page_number = 0\\n    for page in reader.pages:\\n        page_number += 1\\n        _pdf_text += page.extract_text() + f\\"\\\\nPage Number: {page_number}\\"\\n    return _pdf_text\\n\\n# ... \\n  \\ndef chunked_text(\\n    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = \\"cl100k_base\\"\\n) -> list[str]:\\n    \\"\\"\\"Chunks the pdf text into smaller chunks of size max_token_length.\\n    :param pdf_text: the Series of individual pdf texts to chunk.\\n    :param max_token_length: the maximum length of tokens in each chunk.\\n    :param tokenizer_encoding: the encoding to use for the tokenizer.\\n    :return: Series of chunked pdf text. Each element is a list of chunks.\\n    \\"\\"\\"\\n    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\\n    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\\n    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\\n    return _decoded_chunks\\n\\n# ... \\n  \\ndef summarized_text(\\n    prompt_and_text_content: str,\\n    openai_gpt_model: str,\\n) -> str:\\n    \\"\\"\\"Summarizes the text from the summarized chunks of the pdf.\\n    :param prompt_and_text_content: the prompt and content to send over.\\n    :param openai_gpt_model: which openai gpt model to use.\\n    :return: the string response from the openai API.\\n    \\"\\"\\"\\n    response = openai.ChatCompletion.create(\\n        model=openai_gpt_model,\\n        messages=[\\n            {\\n                \\"role\\": \\"user\\",\\n                \\"content\\": prompt_and_text_content,\\n            }\\n        ],\\n        temperature=0,\\n    )\\n    return response[\\"choices\\"][0][\\"message\\"][\\"content\\"]\\n\\n\\nif __name__ == \\"__main__\\":\\n    # run as a script to test Hamilton\'s execution\\n    import summarization\\n\\n    from hamilton import base, driver\\n\\n    dr = driver.Driver(\\n        {},\\n        summarization,\\n        adapter=base.SimplePythonGraphAdapter(base.DictResult()),\\n    )\\n    dr.display_all_functions(\\"summary\\", {\\"format\\": \\"png\\"})\\n```\\n\\nNotice the chunking of code into functions. Each one has a clear role and shouldn\u2019t have too many lines of code. It\u2019s easy to understand a function\u2019s purpose through its name, type annotations, docstring, and dependencies specified as arguments. As a bonus, Hamilton can produce a visualization of the module\u2019s execution DAG for free! The top-level nodes, such as `user_query` and `openai_gpt_model`, are exposed to the user through the frontend and are processed down the DAG via FastAPI.\\n\\n![Alt text](image-3.png)\\n\\nAlso, you might have noticed at the end the `if _name__ == \u201c__main__\u201d:` and the module importing itself via `import summarization`. This effectively allows you to load the module and execute it with Hamilton *outside* of the FastAPI server. This makes it easy to run and iterate over your Hamilton transformations during development, or [unit test](https://en.wikipedia.org/wiki/Unit_testing) them outside of your FastAPI web service.\\n\\n---\\n\\nNow let\u2019s look at server.py:\\n```python title=\\"FastAPI server\\"\\n# server.py\\n# ... imports\\nimport summarization\\n\\n# instantiate FastAPI app\\napp = fastapi.FastAPI()\\n\\n# define constants for Hamilton driver\\ndriver_config = dict(\\n    file_type=\\"pdf\\",\\n)\\n\\n# instantiate the Hamilton driver; it will power all API endpoints\\n# async driver for use with async functions\\nasync_dr = h_async.AsyncDriver(\\n    driver_config,\\n    summarization,  # python module containing function logic\\n    result_builder=base.DictResult(),\\n)\\n# sync driver for use with regular functions\\nsync_dr = driver.Driver(\\n    driver_config,\\n    summarization,  # python module containing function logic\\n    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\\n)\\n\\n\\nclass SummarizeResponse(pydantic.BaseModel):\\n    \\"\\"\\"Response to the /summarize endpoint\\"\\"\\"\\n    summary: str\\n\\n\\n@app.post(\\"/summarize\\")\\nasync def summarize_pdf(\\n    pdf_file: fastapi.UploadFile,\\n    openai_gpt_model: str = fastapi.Form(...),  # = \\"gpt-3.5-turbo-0613\\",\\n    content_type: str = fastapi.Form(...),  # = \\"Scientific article\\",\\n    user_query: str = fastapi.Form(...),  # = \\"Can you ELI5 the paper?\\",\\n) -> SummarizeResponse:\\n    \\"\\"\\"Request `summarized_text` from Hamilton driver with `pdf_file` and `user_query`\\"\\"\\"\\n    results = await async_dr.execute(\\n        [\\"summarized_text\\"],\\n        inputs=dict(\\n            pdf_source=pdf_file.file,\\n            openai_gpt_model=openai_gpt_model,\\n            content_type=content_type,\\n            user_query=user_query,\\n        ),\\n    )\\n    return SummarizeResponse(summary=results[\\"summarized_text\\"])\\n\\n\\n@app.post(\\"/summarize_sync\\")\\ndef summarize_pdf_sync(\\n    pdf_file: fastapi.UploadFile,\\n    openai_gpt_model: str = fastapi.Form(...),  # = \\"gpt-3.5-turbo-0613\\",\\n    content_type: str = fastapi.Form(...),  # = \\"Scientific article\\",\\n    user_query: str = fastapi.Form(...),  # = \\"Can you ELI5 the paper?\\",\\n) -> SummarizeResponse:\\n    \\"\\"\\"Request `summarized_text` from Hamilton driver with `pdf_file` and `user_query`\\"\\"\\"\\n    results = sync_dr.execute(\\n        [\\"summarized_text\\"],\\n        inputs=dict(\\n            pdf_source=pdf_file.file,\\n            openai_gpt_model=openai_gpt_model,\\n            content_type=content_type,\\n            user_query=user_query,\\n        ),\\n    )\\n    return SummarizeResponse(summary=results[\\"summarized_text\\"])\\n\\n\\n# add to SwaggerUI the execution DAG png\\n# see http://localhost:8080/docs#/default/summarize_pdf_summarize_post\\nbase64_viz = base64.b64encode(open(\\"summarization_module.png\\", \\"rb\\").read()).decode(\\"utf-8\\")\\napp.routes[\\n    -1\\n].description = f\\"\\"\\"<h1>Execution DAG</h1><img alt=\\"\\" src=\\"data:image/png;base64,{base64_viz}\\"/>\\"\\"\\"\\n\\n\\nif __name__ == \\"__main__\\":\\n    # run as a script to test server locally\\n    import uvicorn\\n\\n    uvicorn.run(app, host=\\"0.0.0.0\\", port=8080)\\n```\\n\\nThe file server.py is responsible of instantiating FastAPI and defining the server\u2019s endpoints. We also instantiate *synchronous* and *asynchronous* Hamilton Driver objects to power both types of endpoint styles that FastAPI supports (**Note**. [FastAPI manages non-async functions like async ones](https://fastapi.tiangolo.com/async/#in-a-hurry)). Then, in the endpoints definitions, it is a matter of handling the request data, passing it to Hamilton and requesting the variable summarized_text, and sending a formatted response to the client.\\n\\nTowards the end of the snippet, there is a cryptic line of code that encodes the visualization of the Hamilton Driver as `base64` and embeds it in the FastAPI generated Swagger UI for great documentation! \\n\\n![Alt text](image-4.png)\\n\\nAgain, you\u2019ll find a `if _name__ == \u201c__main__\u201d:` statement allowing you to start a local server by calling python server.py. This way, you can test your server code independently from your Hamilton transformations and then add [integration tests](https://en.wikipedia.org/wiki/Integration_testing).\\n\\nTo learn more about the frontend, we invite you to view [the full example on GitHub](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer).\\n\\n## To recap\\nIn this post we showed how to:\\n* Break down a PDF summarizer flow into individual functions with Hamilton\\n* Use Hamilton for inference with FastAPI, and exercise the required DAG inputs through POST requests\\n* Decouple the code for the UI, server, and application logic to enable faster development\\n* Structure a project\u2019s directory and code to help readability and maintainability\\n\\nThe function-centric approach of Hamilton makes it easy to update or extend your application dataflow. For example, you could swap out OpenAI API for Anthropic ([Learn how to swap stack components](https://blog.dagworks.io/p/building-a-maintainable-and-modular)), or add document processing steps by writing a few functions. With Hamilton being open source and being an extensible platform, motivated developers can tailor it to their needs for example by implementing custom caching strategies (e.g. [use this, or extend it](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/caching_nodes)), data validation steps (e.g. [use/extend this](https://hamilton.dagworks.io/en/latest/reference/decorators/check_output/#check-output)), or telemetry capture.\\n\\nLike what you heard? We\u2019d love a [star on GitHub](https://github.com/DAGWorks-Inc/hamilton) and/or subscribe to this blog to get updates.\\n\\n## What\u2019s next\\nThanks for getting this far! As we mentioned in the beginning we\u2019ve got a bunch of exciting content planned\ud83e\udd73 Here\u2019s a few lines about some of our upcoming posts on how to build LLM-based applications for production. Subscribe to get updates as we publish them!\\n\\n### Testing: it\u2019s important and with Hamilton it\u2019s simpler\\nDeploying and maintaining an application in production requires great testing practices. You\u2019ll find that the functions and the dataflows defined with Hamilton are much friendlier to test than object-oriented frameworks. In this next blog, we\u2019ll add to the PDF summarizer unit tests for Hamilton functions, integration tests to use Hamilton with FastAPI, and data validation checks to ensure correct outputs from the LLMs. We\u2019ll also show you how to pull intermediate values from the DAG to plug into your monitoring system!\\n\\n### Spark: scale up your Hamilton dataflow with the Spark executor\\nDid you know that Hamilton allows you to scale up your dataflow to run in a batch workflow without migrating your code? Your dataflow defined for FastAPI can also run on Spark! In the follow-up post, we\u2019ll show how to take the summarizer dataflow and run it on Spark to process lots of tables of data & PDFs in parallel. Operating efficiently at scale is important to be able to re-process all of your documents when upgrading your dataflow (e.g., computing embeddings with a new LLM). Also, this is a key feature to prevent [implementation skew](https://www.hopsworks.ai/dictionary/online-offline-feature-skew) between online & offline efforts.\\n\\n### RAG: expand your Hamilton application to cover ingestion and retrieval\\nInstead of being passed in the PDF to summarize, we might have a corpus of documents to choose from to answer users queries. In this upcoming post, we\u2019ll extend the PDF summarizer to a typical [Retrieval Augmented Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) workflow. You\u2019ll see how Hamilton can manage multiple flows (ingestion, retrieval, summarization) and expose them via API endpoints for an end-to-end RAG application. In the meantime, [this blog post](https://blog.dagworks.io/p/building-a-maintainable-and-modular) shows the precursor step of ingesting data into a vector database for RAG use with Hamilton.\\n\\n## Want lineage, catalog, and observability out of the box? Use Hamilton + DAGWorks.\\n![Alt text](image-5.png)\\n\\nLastly before we go, if you\u2019re interested in automatically capturing versions of flows, seeing inputs, executions, and observing what happened - you can try out what we\u2019re building at [DAGWorks](https://www.dagworks.io/) with a [one-line replacement](https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/server.py#L36-L45). \\n\\nThe DAGWorks platform helps you:\\n* **Version dataflows**. As you iterate and change the shape, contents and structure of your code, DAGWorks tracks changes and allows you to view diffs.\\n* **Monitor runs**. Understand which DAG nodes were executed, how long it took, what were the outputs, etc. from your dashboard.\\n* **Debug failures**. DAG execution errors are contextualized with other node results and you can compare across multiple runs to view DAG changes.\\n\\nTo get started, in the example we have a few commented out pieces of code you\u2019ll need to flip on (quick instructions [here](https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/README.md#connecting-to-dagworks) and DAGWorks docs [here](https://docs.dagworks.io/introduction)):\\n1. Install `dagworks-sdk` \u2013 uncomment [this line](https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/requirements.txt#L11) in `requirements.txt`.\\n2. Add the `DAGWORKS_API_KEY` to `.env` \u2013 [sign up](https://www.dagworks.io/pricing) for an account there\u2019s a free tier! Create an `API_KEY` and save it.\\n3. Create a DAGWorks project \u2013 note the project ID (see docs [here](https://docs.dagworks.io/introduction) for how to do that).\\n4. Instantiate the DAGWorks Driver \u2013 [uncomment this code](https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/server.py#L34-L45) in the server to instantiate the DAGWorks driver and fill in your project details.\\n5. Rebuild the containers and you\u2019re good to go!\\n\\n## We want to hear from you!\\nIf you\u2019re excited by any of this, or have strong opinions, drop by our Slack channel / or leave some comments here! Some resources to get you help:\\n\\n\ud83d\udce3 join our community on [Slack](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg) \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started.\\n\\n\u2b50\ufe0f us on [GitHub](https://github.com/DAGWorks-Inc/hamilton)\\n\\n\ud83d\udcdd leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you find something\\n\\nOther Hamilton posts you might be interested in:\\n* [tryhamilton.dev](https://www.tryhamilton.dev/) \u2013 an interactive tutorial in your browser!\\n* [Build a modular LLM stack with Hamilton](https://blog.dagworks.io/p/building-a-maintainable-and-modular)\\n* [Hamilton + Airflow](https://blog.dagworks.io/publish/post/130538397) ([GitHub repo](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/airflow))\\n* [Hamilton + Feast](https://blog.dagworks.io/p/featurization-integrating-hamilton) ([GitHub repo](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/feast))\\n* [Pandas data transformations in Hamilton in 5 minutes](https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)\\n* [Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)"},{"id":"feast-hamilton","metadata":{"permalink":"/blog/feast-hamilton","source":"@site/blog/2023-08-02-feast-hamilton/index.md","title":"Featurization: Integrating Hamilton with Feast","description":"Are you using Feast? or perhaps you are having trouble with it? Or perhaps you are considering adopting it? In this post, you will learn the operational benefits of using Feast with Hamilton. Feast will act as your \u201cfeature store\u201d, while Hamilton will be your in-process \u201cfeature processing engine\u201d. We start by providing an overview of Feast, then show how Hamilton fits into the picture. At the end, we give a recipe for where to go from here, depending on where you are in your \u201cfeature journey\u201d.","date":"2023-08-02T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"Feast","permalink":"/blog/tags/feast"},{"inline":true,"label":"feature store","permalink":"/blog/tags/feature-store"}],"readingTime":16.925,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"feast-hamilton","title":"Featurization: Integrating Hamilton with Feast","authors":"tj","tags":["Hamilton","Feast","feature store"]},"unlisted":false,"prevItem":{"title":"Containerized PDF Summarizer with FastAPI and Hamilton","permalink":"/blog/pdf-summarizer"},"nextItem":{"title":"Simplify Prefect Workflow Creation and Maintenance with Hamilton","permalink":"/blog/prefect-hamilton"}},"content":"Are you using Feast? or perhaps you are having trouble with it? Or perhaps you are considering adopting it? In this post, you will learn the operational benefits of using Feast with Hamilton. Feast will act as your \u201cfeature store\u201d, while Hamilton will be your in-process \u201cfeature processing engine\u201d. We start by providing an overview of Feast, then show how Hamilton fits into the picture. At the end, we give a recipe for where to go from here, depending on where you are in your \u201cfeature journey\u201d.\\n\\n> crosspost from https://blog.dagworks.io/p/featurization-integrating-hamilton\\n\\n\x3c!--truncate--\x3e\\n\\n> Note: by \u201cfeature\u201d we are referring to [this meaning of the word](https://en.wikipedia.org/wiki/Feature_(machine_learning)).\\n\\n## What problem does Feast solve?\\n\\nFeast is an open-source [feature store](https://www.featurestore.org/what-is-a-feature-store), a specialized piece of machine learning (ML) infrastructure that centralizes computed feature data and metadata. The feature store sits above the storage layer and unifies the output of various data sources (streaming, operational databases, data warehouse, app session, etc.) to provide a simple interface to query features for your ML application. Feature stores are critical to the operations of organizations serving high volumes of predictions (e.g., Uber, AirBnb, LinkedIn).\\n\\n> Note: Feature stores introduce a lot of complexity that should be justified by a decent number of projects in production. Please read [this blog](https://medium.com/data-for-ai/feature-pipelines-and-feature-stores-deep-dive-into-system-engineering-and-analytical-tradeoffs-3c208af5e05f) by FeatureStoresForML for a detailed look at the pros and cons of feature stores. You may only need Hamilton, in which case, we direct readers to our [documentation](https://hamilton.dagworks.io/en/latest/how-tos/use-for-feature-engineering/) and [example](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/feature_engineering_multiple_contexts) to get you started!\\n\\nThe feature store abstraction delineates feature producers from downstream users (feature consumers), effectively creating a contract between the two parties. Feature creation and use can get messy, especially when one wants to share them across an organization. One way to do that is to centralize features via data, i.e. put them all into the same store, and users can pull from this store to power their machine learning models. A feature store can then offer guardrails to ensure that offline model training matches online settings (i.e., preventing [training/serving skew](https://developers.google.com/machine-learning/guides/rules-of-ml#training-serving_skew)). Feature stores can help reduce system latency by caching features in an \u201conline store\u201d (more on that below) for efficient retrieval.\xa0 This [blog](https://medium.com/@endeavordata/streamlining-machine-learning-development-with-a-feature-store-680ee6d45c64) explains further the benefits of centralizing data into a feature store.\\n\\nThe [Feast documentation](https://docs.feast.dev/#example-use-cases) mentions the following typical use cases:\\n* Personalizing online recommendations by leveraging pre-computed historical user or item features.\\n* Online fraud detection, using features that compare against (pre-computed) historical transaction patterns\\n* Credit scoring, using pre-computed historical features to compute probability of default\\n* Churn prediction (an offline model), generating feature values for all users at a fixed cadence in batch\\n\\n## What is Feast?\\n\\nIf one is unfamiliar with Feast, we recommend reading the Feast [introduction page](https://docs.feast.dev/), but here are some high-level key considerations:\\n\\n**What Feast does**:\\n* Stores metadata on features that have been registered with it\\n* Provides an abstraction to push to & query for materialized feature data.\\n* Facilitates proper [point-in-time](https://docs.feast.dev/getting-started/concepts/point-in-time-joins) (i.e., \u201ctime-travel\u201d) table joins. Useful for creating training sets from features.\\n* Helps you integrate heterogeneous sources of feature data into a central place.\\n\\n**What Feast doesn\u2019t do**:\\n* Perform the transformations to compute your features.\\n* Store data itself; it relies on being set up on top of your existing infrastructure.\\n* Orchestrate your data transformation pipelines that would perform feature computation.\\n* Help you write clean transformation code & organize it.\\n* Provide lineage & provenance for your features.\\n\\nHere are a few [Feast key terms](https://docs.feast.dev/getting-started/concepts) relevant to this post:\\n* **Entity**: a real-world entity we care about (e.g., user, city, month, product SKU).\\n* **DataSource**: a physical storage of data (e.g., file, database, data warehouse).\\n* **FeatureView**: the table schema of a data source with additional metadata.\\n* **FeatureService**: a new table schema resulting from joining one or more FeatureViews.\\n* **Offline store**: an interface to read DataSources and do the point-in-time joins at query time.\\n* **Online store**: an interface to read FeatureService already stored with the joins completed.\\n* **Feast registry**: the single central catalog containing the defined Feast objects and relationships.\\n\\n## More on feature computation and lineage\\nSince Feast is only responsible for the downstream use of features, it [cannot compute them nor produce full lineage](https://docs.feast.dev/#feast-is-not) from raw data for you. Feast validates the table schema you pass to it (columns name and type), but it can\u2019t catch upstream data transformation changes that don\'t respect the desired schema. \\n\\nWithout lineage, it is not possible to enforce CI/CD checks of the schema and trigger a recompute of the online store when feature transformations are updated for example. Additionally, not having lineage makes it hard to trace source data usage, remove dead feature code, as well as deprecate unused features, all which are important to help ensure smooth and cost efficient operations.\\n\\n## How can Hamilton help?\\nHamilton is a Python micro-orchestration framework to express data transformations. It helps one write Python code that is modular and reusable, and that can be executed as a direct acyclic graph (DAG). Hamilton was initially developed and used in production to create large dataframes (100+ columns) for machine learning while preserving strong lineage capabilities (see [the origin story](https://blog.dagworks.io/p/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d)). Its strength is expressing the flow of data & computation in a way that is straightforward to create and maintain (much like DBT does for SQL). If you are considering Feast, it likely means you also have some data scale, so just to mention it, that Hamilton can be run at scale as well. It has integrations with Spark, Dask, Ray, and can even scale up your pandas code for free ([learn more here](https://hamilton.dagworks.io/en/latest/how-tos/scale-up/))!\\n\\n> If you are new to Hamilton, we invite you to an interactive overview on [tryhamilton.dev](http://www.tryhamilton.dev), or this post. Hamilton will be discussed at a high level and relevant documentation references will be shared for more details.\\n\\nHamilton is a flexible tool to express DAGs and can improve the Feast development experience in two primary ways: \\n\\n1. unifying how feature transformations are defined, executed across your stack\\n2. define and manage the Feast objects you use to register features with Feast.\\n\\n![Alt text](image.png)\\n\\n### 1. Use Hamilton for data transformations before pushing to Feast\\n\\nWith Hamilton, you write declarative and granular data transformation functions. The functions declare what they output with the **function name** and declare what they require as input with the **function arguments**, and everything is type annotated. The logic of the computation is wholly contained within the function. The Hamilton Driver, which orchestrates execution in a python process, automatically generates the execution DAG from the function definitions and allows you to query for only the set of transforms you are interested in computing.\\n\\n![Alt text](image-1.png)\\n\\nUsing this approach, you can easily scale to writing dataframes with 1000s of columns and maintain a clear lineage of upstream and downstream dependencies. To generate your features, simply call `Driver.execute()` and write the resulting dataframe to your Feast offline or online store. You can reuse the modular functions you define both in your offline and online environment to prevent training/serving skew.\xa0 Also, the defined Hamilton functions can be easily reused in Feast\u2019s [OnDemandFeatureView](https://docs.feast.dev/reference/alpha-on-demand-feature-view) when dealing with request data. In addition, Hamilton enables runtime data validation on any function by adding the `@check_output` decorator ([learn more](https://hamilton.dagworks.io/en/latest/how-tos/run-data-quality-checks/)), which can again mitigate training/serving skew and ensure feature output is checked before it is pushed to Feast.\\n\\n```python title=\\"Push data to Feast\\"\\nimport numpy as np\\nimport pandas as pd\\n\\nfrom hamilton.function_modifiers import extract_columns, save_to, source, check_output\\n\\nTRIPS_SOURCE_COLUMNS = [\\n    \\"event_timestamp\\",\\n    \\"driver_id\\",\\n    \\"rider_id\\",\\n    \\"trip_dist\\",\\n    \\"created\\",\\n]\\n\\n\\n# extract columns allows you to split a dataframe into multiple pandas Series\\n@extract_columns(*TRIPS_SOURCE_COLUMNS)\\ndef trips_raw(trips_raw_path: str) -> pd.DataFrame:\\n    \\"\\"\\"Load the driver dataset\\"\\"\\"\\n    df = pd.read_parquet(trips_raw_path)\\n    df = df.sort_values(by=\\"event_timestamp\\")\\n    return df\\n\\n\\ndef day_of_week(event_timestamp: pd.Series) -> pd.Series:\\n    \\"\\"\\"Encode day of the week as int\\"\\"\\"\\n    return pd.Series(\\n        event_timestamp.dt.day_of_week, name=\\"day_of_week\\", index=event_timestamp.index\\n    )\\n\\n\\n# see how this function depends on the return value of `day_of_week()`\\n@check_output(data_type=np.int64, data_in_range(0, 1), importance=\\"warn\\")\\ndef is_weekend(day_of_week: pd.Series) -> pd.Series:\\n    weekend = np.where(day_of_week >= 5, 1, 0)\\n    return pd.Series(weekend, name=\\"is_weekend\\", index=day_of_week.index)\\n\\n\\ndef percentile_dist_rolling_3h(trip_dist: pd.Series, event_timestamp: pd.Series) -> pd.Series:\\n    \\"\\"\\"Compute the rolling 3H percentile trip dist\\"\\"\\"\\n    df = pd.concat([trip_dist, event_timestamp], axis=1)\\n    agg = df.rolling(\\"3H\\", on=\\"event_timestamp\\")[\\"trip_dist\\"].rank(pct=True)\\n    return pd.Series(agg, name=\\"percentile_trip_dist_rolling_3h\\", index=event_timestamp.index)\\n\\n\\n# this function has many lines, but it simply explicitly assemble columns from the raw\\n# source and the computed features.\\n# the @save_to decorator allows to easily save this result to a parquet file\\n@save_to.parquet(path=source(\\"trips_stats_3h_path\\"), output_name_=\\"save_trips_stats_3h\\")\\ndef trips_stats_3h(\\n    event_timestamp: pd.Series,\\n    driver_id: pd.Series,\\n    day_of_week: pd.Series,\\n    is_weekend: pd.Series,\\n    percentile_dist_rolling_3h: pd.Series,\\n) -> pd.DataFrame:\\n    \\"\\"\\"Global trip statistics over rolling 3h\\"\\"\\"\\n    df = pd.concat(\\n        [\\n            event_timestamp,\\n            driver_id,\\n            day_of_week,\\n            is_weekend,\\n            percentile_dist_rolling_3h,\\n        ],\\n        axis=1,\\n    )\\n    return df\\n```\\n\\nThis code snippet shows broadly the Hamilton approach to defining feature transforms. There is a function that loads a dataframe and exposes its columns (i.e. individual pandas series) for downstream use; typically this could be a call to a database to return a dataframe. Then, features are created by applying functions on these pandas series. Notably, `percentile_dist_rolling_3h()` takes two pandas series as input, creates a temporary dataframe in the function\u2019s body to apply a rolling window, and returns a new pandas series. Hamilton provides a lot of utilities to avoid code duplication across similar features. You can learn more about them from the examples in the [documentation](https://hamilton.dagworks.io/en/latest/concepts/decorators-overview/).\\n\\n![Alt text](image-2.png)\\n\\nHamilton pairs so well with Feast because they were both designed around the table/dataframe abstraction. Hamilton enables you to create complex dataframes using functions that are easy to read, test, and document, and you also get [lineage as code](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)! This increased visibility makes it easier for downstream Feast users to trust data definitions and therefore feature quality.\\n\\n### 2. Use Hamilton to wrangle Feast definitions\\n\\nFeast defines its registry objects in Python. For Feast to know about features you need to explicitly register them. At registration, when calling `feast apply` objects are converted to [protobuf](https://protobuf.dev/) and feature object relationships are built. However, the dependencies between feature objects are not made explicit from the Python code definitions that Feast makes your write. To add to this, the Feast CLI is able to read feature object definitions across directories, which inadvertently allows the proliferation of feature object definitions across a codebase. This makes it hard for someone reading the codebase to develop a mental model of these relationships. Considering Feast is essentially a DAG, wrapping object definitions in Hamilton functions can greatly improve readability and reduce the chances of breaking changes.\\n\\n```python title=\\"Define Feast entities with Hamilton\\nimport feast\\nimport datetime\\n\\n\\n# an entity has no upstream dependencies; it is our join index\\ndef driver_entity() -> feast.Entity:\\n    \\"\\"\\"Feast definition: driver entity\\"\\"\\"\\n    return feast.Entity(name=\\"driver\\", join_keys=[\\"driver_id\\"], value_type=feast.ValueType.INT64)\\n  \\n\\n# the filesource only needs a file path\\n# it could have been hardcoded, but here we pass it as argument because we want to ensure\\n# that is it the same value passed as the storage for our feature_transformations.py code\\ndef driver_hourly_stats_source(driver_source_path: str) -> feast.FileSource:\\n  \\"\\"\\"Feast definition: source with hourly stats of driver\\"\\"\\"\\n  return feast.FileSource(\\n      name=\\"driver_hourly_stats\\",\\n      path=driver_source_path,\\n      timestamp_field=\\"event_timestamp\\",\\n      created_timestamp_column=\\"created\\",\\n  )\\n\\n\\n# the FeatureView is a 1-to-1 with the DataSource, but adds metadata and time-to-live (TTL)\\ndef driver_hourly_stats_fv(\\n  driver_entity: feast.Entity,\\n  driver_hourly_stats_source: feast.FileSource\\n) -> feast.FeatureView:\\n  \\"\\"\\"Feast definition: feature view with hourly stats of driver\\"\\"\\"\\n  return feast.FeatureView(\\n      name=\\"driver_hourly_stats\\",\\n      entities=[driver_entity],\\n      ttl=timedelta(days=1),\\n      schema=[\\n          feast.Field(name=\\"conv_rate\\", dtype=feast.types.Float32),\\n          feast.Field(name=\\"acc_rate\\", dtype=feast.types.Float32),\\n          feast.Field(\\n              name=\\"avg_daily_trips\\", dtype=feast.types.Int64, description=\\"Average daily trips\\"\\n          ),\\n      ],\\n      online=True,\\n      source=driver_hourly_stats_source,\\n      tags={\\"team\\": \\"driver_performance\\"},\\n  )\\n\\n\\n# the FeatureService defines how the data is stored in the online store\\n# and how it\'s retrieved\\ndef driver_activity_v1_fs(\\n   driver_hourly_stats_fv: feast.FeatureView,\\n) -> feast.FeatureService:\\n    \\"\\"\\"Feast definition: grouping of features relative to driver activity\\"\\"\\"\\n    return feast.FeatureService(\\n        name=\\"driver_activity_v1\\",\\n        features=[\\n            driver_hourly_stats_fv,\\n        ],\\n    )\\n```\\n\\nThe code shows a snippet from the `store_definitions.py` file from the Feast example in the Hamilton repository. The Hamilton approach to defining functions makes it easy to read dependencies from the code and prevents synchronization issues between Feast feature object definitions when changes are made. As a bonus, Hamilton can automatically generate visualizations that you can associate with your commit or add to your documentation. Below is the DAG generated that registers the appropriate objects with Feast; Hamilton uses the functions defined in `store_definitions.py` to create this.\\n\\n![Alt text](image-3.png)\\n\\nAs the Feast object graph grows in complexity, it becomes overwhelming to display in its entirety. With the Hamilton `driver.what_is_upstream_of()` and `what_is_downstream_of()`, one can quickly answer specific questions about dependencies, by zooming into only what\u2019s relevant to display. For example:\\n\\n![Alt text](image-4.png)\\n\\n![Alt text](image-5.png)\\n\\n## The full Hamilton + Feast experience\\n\\nSo far, we\u2019ve presented how to use Hamilton to compute the features used in Feast and how to define your Feast objects with Hamilton. By adopting both practices, you can gain visibility over the dataflow from raw data, to feature transformations, to Feast service, effectively extending the contract between data producers and downstream users. If downstream users also use Hamilton to express their data science workflow or power an inference API endpoint, all of the benefits will be further extended!\xa0\\n\\nThis integration allows you to foresee which downstream users will be affected by changes to data transformations code and how. For example: \\n\\n1. You could add to your CI/CD pipeline checks for changes in the structure of the Hamilton transformations and Feast object definitions DAGs. If detected one could send an alert or request a review from affected users. \\n\\n2. Changes to the Hamilton DAG could trigger a purge and feature recompute for the Feast online store to prevent any mismatch between a newly trained model and the already stored features.\\n\\n## Build the feature platform you need\\n\\n### Step 1: Adopt Hamilton\\nWe candidly believe that Hamilton is a great addition, no matter the stage of maturity your feature needs are in. If you have existing code, it usually only takes you 10-25% of the initial development time to refactor code into the Hamiltonian style.\\n\\n### Step 2: Feature Store? \\nBut when to do you use/adopt Feast? That\u2019s a different question. Feast has some compelling features, but it is important to evaluate a team\u2019s needs and capabilities, present and future, before making important infrastructure decisions. Make sure to read this excellent [blog](https://medium.com/data-for-ai/feature-pipelines-and-feature-stores-deep-dive-into-system-engineering-and-analytical-tradeoffs-3c208af5e05f) to review the pros and cons of feature stores; as noted in the beginning, a feature store is a complex component to integrate and manage, so you should be confident in your decision before moving forward. There are two important lines of questioning:\\n\\n#### Would your problems benefit from a feature store? \\nThis requires thinking about the nature of the problems you are working on and scoping the pain points you are facing.\\n* Are you integrating multiple batch/streaming/request data sources? If you are only pulling everything from a data warehouse, you might not need a feature store.\\n* Are you handling time-series data with complex joins and need point-in-time retrieval? If you are only doing batch predictions with a few joins, you might not need a feature store.\\n* Are you having duplicate definitions and compute of features across projects? \\n\\n#### What would be the cost of a maintaining feature store?\\nThis requires evaluating the efforts and resources that will be needed to be dedicated to setting up and maintaining the feature store.\\n* How many features and projects in production are you actively managing?\\n* Do you have a dedicated team to manage infrastructure? You will need ML or data engineering time to migrate and maintain the feature store. Also, data scientists/teams that built the existing features and pipelines will need to ensure migration correctness.\\n\\n## Some Illustrative scenarios\\n\\nNow here are a few prototypical scenarios. You might recognize yourself in one, or find yourself between two, but in all cases remember the earlier the you adopt good coding practices and write modular code, the easiest migrating to a feature store is.\\n\\n### You are just getting started with data science and machine learning and writing a proof of concept(s).\\n\\nYou probably do not need a feature store and its overhead before having any project in production as it would slow you down. You are however, more likely to need tooling around versioning your code and experimentation tracking to store and compare model performance and analysis results. Unlike a feature store, Hamilton is lightweight and can help you write better code that will be easier to version. You can use it from day 1 of any project. Use it to write your analysis and keep track of the results of `driver.execute()` using [MLFlow](https://mlflow.org/docs/latest/index.html), [Neptune](https://docs.neptune.ai/), or [Weights&Biases](https://docs.wandb.ai/) for example.\\n\\n### You are managing a few deployed projects serving predictions to users.\\nThis is typically when people start looking at tooling to standardize development across projects to reduce duplicated effort and bring some centralization for people to build off of collectively. After answering the above questions, you might conclude that you need a feature store.\\n\\nIf you have the resources to migrate to a feature store, the earlier you migrate the less refactoring you have to do. If you currently don\u2019t have the bandwidth, you can still standardize your practice moving forward and progressively refactor code to prepare for a migration.\\n\\n### You have multiple mature projects serving a large number of predictions.\\nAs the size and maturity of your data science and ML practice increases, the same logic as the previous section applies, but the cost/benefit ratio can change. The potential benefits are likely evident to you, but refactoring projects powering production services can be risky and very demanding. You should prioritize projects and migrate them one at a time. Adding data validation for data transformations,  Feast ingest, Feast retrieval and model predictions will help prevent breaking changes and diagnose potential problems. \\n\\n## You decided to migrate to Feast\\nTo help, we recommend that data scientists/teams responsible for the projects help with the migration. They will need to specify the entities and the timestamp columns for joins. Then, they should define the meaning of features to include in the Feast registry and refactor code to extract the data transformation functions. If you are not already using Hamilton, the refactoring could be more involved.\\n\\nWith that information, you should be able to write the Feast definitions DAG and register it. If the Feast objects were properly defined, retrieving features from Feast should be equivalent to your previous pipeline on downstream operations.  Then, you should test if you are able to properly materialize/push new data to the online and offline stores.\\n\\n## You decided not to migrate/adopt to Feast (yet?)\\nYou might have realized that your projects wouldn\u2019t benefit from Feast or that more important pain points need to be addressed. Here\u2019s a few examples:\\n\\n* You want lineage \u2192 Hamilton\u2019s DAG-based paradigm is ideal to clearly establish the dependencies between transformations and the steps data goes through. See [DAGWorks](http://www.dagworks.io) that takes this product experience further. \\n* You need to automate tasks or run things on a schedule \u2192 look into orchestrators. Hamilton integrates well with any of them, including [Airflow](https://blog.dagworks.io/p/supercharge-your-airflow-dag-with), [Prefect](https://blog.dagworks.io/p/simplify-prefect-workflow-creation), and [Metaflow](https://github.com/outerbounds/hamilton-metaflow).\\n* You need to gain visibility over production systems failures \u2192 look into Hamilton\u2019s data quality capabilities, and other monitoring tools like [DAGWorks](http://www.dagworks.io). \\n* You need to share results and collaborate with team members \u2192 look into experiment tracking platforms.\\n\\n## Final words\\nHaving Hamilton + Feast in your stack will quickly feel natural. Together, they improve operational efficiency by enabling you to standardize & centralize feature transformations, feature registration, storage and retrieval. So not only will teams be able to create training sets from each others\u2019 features, they will also be able to trace what data produced it and how it was transformed to more easily trust and mitigate data issues. Altogether this will mean that teams will move faster with more confidence.\\n\\nSimply fork the example [repository for Feast + Hamilton](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/feast) to get started! It will give you an idea how to: structure your directory, write Hamilton data transformations, define Feast objects, and retrieve features from Feast. \\n\\nFor users who have questions on the above, please reach out to us on Slack! We\u2019re also more than happy to consult on Feast matters.\\n\\n## Links\\n\\n\ud83d\udd17 Join our community on [Slack](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email).\\n\\n\u2b50\ufe0f Leave us a star on [GitHub](https://github.com/DAGWorks-Inc/hamilton).\\n\\n\u2328\ufe0f Try Hamilton in the browser at [tryhamilton.dev](https://tryhamilton.dev).\\n\\nIntegrate Feast and Hamilton: example [GitHub repository](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/feast).\\n\\nRead: [Writing tidy production Pandas code with Hamilton](https://blog.dagworks.io/p/tidy-production-pandas-with-hamilton-3b759a2bf562) .\\n\\nRead: [An hands-on and succinct introduction to feature stores and Feast by MadeWithML](https://madewithml.com/courses/mlops/feature-store/#when-do-i-need-a-feature-store).\\n\\nIntegrate Hamilton with your favorite macro-orchestrator: [Airflow](https://blog.dagworks.io/publish/post/130538397), [Prefect](https://substack.com/inbox/post/135342200), [Metaflow](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/)."},{"id":"prefect-hamilton","metadata":{"permalink":"/blog/prefect-hamilton","source":"@site/blog/2023-07-25-prefect-hamilton/index.md","title":"Simplify Prefect Workflow Creation and Maintenance with Hamilton","description":"This post will show you how to use Hamilton with Prefect, two open source projects. At a high level, Prefect orchestrates where/when/what/how code runs (think macro) and Hamilton helps author clean and maintainable code for data transformations (think micro).","date":"2023-07-25T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"Prefect","permalink":"/blog/tags/prefect"},{"inline":true,"label":"orchestration","permalink":"/blog/tags/orchestration"}],"readingTime":10.465,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"prefect-hamilton","title":"Simplify Prefect Workflow Creation and Maintenance with Hamilton","authors":"tj","tags":["Hamilton","Prefect","orchestration"]},"unlisted":false,"prevItem":{"title":"Featurization: Integrating Hamilton with Feast","permalink":"/blog/feast-hamilton"},"nextItem":{"title":"Building a maintainable and modular LLM application stack with Hamilton","permalink":"/blog/modular-llm"}},"content":"This post will show you how to use Hamilton with Prefect, two open source projects. At a high level, Prefect orchestrates where/when/what/how code runs (think macro) and Hamilton helps author clean and maintainable code for data transformations (think micro).\\n\\n> crosspost from https://blog.dagworks.io/p/simplify-prefect-workflow-creation\\n\\n\x3c!--truncate--\x3e\\n\\n> If you are new to Hamilton, we invite you to an interactive overview on [tryhamilton.dev](http://www.tryhamilton.dev/), or this [post](https://towardsdatascience.com/functions-dags-introducing-hamilton-a-microframework-for-dataframe-generation-more-8e34b84efc1d). Hamilton will be discussed at a high level and relevant documentation references will be shared for more details.\\n\\n[Prefect](https://docs.prefect.io/latest/) is a modern open source workflow orchestrator that uses Python. With its cloud-provider-agnostic deployment, workspaces, authentication, observability, etc., it can power any data science or machine learning project in production. That said, Prefect is not particularly focused on helping data scientists et al. write clean and maintainable workflow code, leaving users with the following challenges:\\n\\n1. Maintain ever-evolving workflows; what starts simple invariably gets complex.\\n2. Write modular, reusable, and testable code to run within Prefect tasks.\\n3. Track lineage of code and data artifacts that a workflow produces.\\n\\nThis is where we believe Hamilton can help! [Hamilton](https://github.com/dagworks-inc/hamilton) is a Python micro-framework for writing data transformations. In short, one writes Python functions in a declarative style, which Hamilton parses and connects into a graph based on their names, arguments and type annotations. Specific outputs can be requested and Hamilton will execute the required function path to produce them. By using Prefect and Hamilton, you get a solid toolbox to orchestrate your project over cloud infrastructure, and benefit from easier to reuse and maintain workflow components.\\n\\n![Alt text](image.png)\\n\\n## Write maintainable Prefect workflows\\n\\nA key decision in building a Prefect workflow is \u201c[how big should a task be](https://docs.prefect.io/2.10.21/concepts/tasks/#tasks-overview)\u201d and how to divide a project into flows and tasks for execution. Each function decorated with the Prefect `@task` decorator gains support for caching and retries, and is executed separately, but it also adds to the scheduling and execution overhead (e.g., moving lots of data). With too few tasks, you end up with a monolithic task that takes a lot of time to complete, but probably gains execution efficiency. There is a trade-off between the complexity of the macro Prefect workflow structure and the complexity of the code within each task. In either case, debugging a workflow can be difficult, especially if you are not its author since there is no standard way to write a workflow. More often than not, the initial task structure of the workflow becomes fixed by the engineering team (as they typically own this infrastructure), because refactoring the workflow without breaking things is a real challenge!\\n\\nWhile simpler workflows such as `A->B->C` are desirable, there is an inherent tension between the structure\u2019s simplicity and the amount of code per task. The more code per task, the more difficult it is to identify points of failure, at the trade-off of potential computational efficiencies, but in the case of failures, retries grow in cost with the size of the task.\\n\\n![Alt text](image-1.png)\\n\\nWhat if you could decouple the complexity of the Prefect workflow from the complexity of the code within tasks? This becomes possible with Hamilton. No matter the size of the tasks\u2019 code, Hamilton enables developers to iterate on the workflow with minimal effort.\\n\\nEssentially, you instantiate a [Hamilton Driver](https://hamilton.dagworks.io/en/latest/concepts/driver-capabilities.html) that loads data transformations from your code modules and automatically builds a Hamilton DAG. By allowing Hamilton to handle the \u201cmicro\u201d orchestration of your code within each Prefect task, it becomes easier to write functions at an arbitrary granularity and to inspect in greater detail the behavior of Prefect tasks.\\n\\nSpecifically the mechanics of the code are:\\n\\n1. Import your function modules\\n2. Pass them to the Hamilton Driver to build the DAG.\\n3. Call `Driver.execute()` with the outputs you want to execute from the DAG you\u2019ve defined.\\n\\nLet\u2019s look at some code that creates a single Prefect task, but uses Hamilton to train and evaluate a ML model:\\n\\n```python title=\\"Prefect with nested Hamilton\\nfrom hamilton import base, driver\\nfrom prefect import flow, task\\nfrom prefect.blocks.system import JSON\\n\\n# import modules containing your dataflow functions\\nimport train_model\\nimport evaluate_model\\n\\n\\n# use the @task to define Prefect tasks, which adds logging, retries, etc.\\n# the function parameters define the config and inputs needed by Hamilton\\n@task\\ndef train_and_evaluate_model_task(\\n    features_path: str,\\n    hamilton_config: str,\\n    label: str,\\n    feature_set: list[str],\\n    validation_user_ids: list[str],\\n) -> None:\\n    \\"\\"\\"Train and evaluate machine learning model\\"\\"\\"\\n    # define the Driver object with configurations and modules\\n    dr = driver.Driver(\\n        hamilton_config,\\n        train_model,  # imported data transformation module\\n        evaluate_model,  # imported data transformation module\\n        adapter=base.SimplePythonGraphAdapter(base.DictResult()),\\n    )\\n\\n    # execute the DAG to produce and outputs the requested `final_vars`\\n    dr.execute(\\n        final_vars=[\\"save_validation_preds\\", \\"model_results\\"],\\n        inputs=dict(\\n            features_path=features_path,\\n            label=label,\\n            feature_set=feature_set,\\n            validation_user_ids=validation_user_ids,\\n        ),\\n    )\\n\\n\\n# use @flow to define the Prefect flow.\\n# the function parameters define the config and inputs needed by all tasks\\n# this way, we prevent having constants being hardcoded in the flow or task body\\n@flow(\\n    name=\\"hamilton-absenteeism-prediction\\",\\n    description=\\"Predict absenteeism using Hamilton and Prefect\\",\\n)\\ndef absenteeism_prediction_flow(\\n    features_path: str = ...,\\n    feature_set: list[str] = [\\n        \\"age_zero_mean_unit_variance\\",\\n        \\"has_children\\",\\n        \\"has_pet\\",\\n        \\"is_summer\\",\\n        \\"service_time\\",\\n    ],\\n    label: str = \\"absenteeism_time_in_hours\\",\\n    validation_user_ids: list[str] = [...],\\n):\\n    \\"\\"\\"Predict absenteeism using Hamilton and Prefect\\"\\"\\"\\n    # ... more tasks\\n    \\n    # load a Prefect Block containing the Hamilton Driver config\\n    hamilton_config_block = JSON.load(\\"hamilton-train-and-evaluate-config\\")\\n    \\n    # call the Prefect task from the workflow\\n    train_and_evaluate_model_task(\\n        features_path=features_path,\\n        hamilton_config=json.load(hamilton_config_block),\\n        label=label,\\n        feature_set=feature_set,\\n        validation_user_ids=validation_user_ids,\\n    )\\n    \\n    # ... more tasks\\n\\n\\nif __name__ == \\"__main__\\":\\n    absenteeism_prediction_flow()\\n```\\n\\nNow, we didn\u2019t show the Hamilton code here, but the benefits of this approach are:\\n\\n1. **Unit & integration testing**. Hamilton, through its naming and type annotations requirements, pushes developers to write modular Python code. This results in Python modules well-suited for unit testing. Once your Python code is unit tested, you can develop integration tests to ensure it behaves properly in your Prefect tasks. In contrast, because of the aforementioned task size tradeoff, directly code from a Prefect task can be less trivial. Also, the CI/CD will have to share Prefect\u2019s dependencies.\\n\\n2. **Reuse data transformations**. This approach keeps the data transformations code in Python modules, separated from the Prefect workflow file. This means this code is also runnable *outside* of Prefect! If you come from the analytics world, it should feel similar to developing and testing SQL queries in an external `.sql` file, then loading it into a Prefect Block.\\n\\n3. **Reorganize your workflow easily**. The lift required to change your Prefect workflow becomes much lower. If you logically model everything in Hamilton, e.g. an end to end machine learning pipeline, it\u2019s just a matter of determining how much of this Hamilton DAG needs to be computed in each Prefect task. For example, you change the number of tasks from one monolithic Prefect task, to a few, or to many \u2014 all that would need to change is what you request from Hamilton since your Hamilton DAG needn\u2019t change at all!\\n\\n## Iterative development with Hamilton & Prefect\\n\\nIn most data science projects, it would be impossible to write the DAG of the final system from day 1, as requirements will change. For example, the data science team will want to try different feature sets for their model. Until this list is finalized, it\u2019s probably undesirable to have the feature set in your source code and under version control; configuration files would be preferable.\\n\\n![Alt text](image-2.png)\\n\\nPrefect will log the configuration for flow and task runs (i.e., the arguments to `@flow` and `@task` decorated function) for traceability and reproducibility. Additionally, Prefect has a feature called [Blocks](https://docs.prefect.io/2.10.21/concepts/blocks/) that allows configurations, API keys, and connectors to be defined in one place and be reused across projects.  With the right configuration, Prefect allows users to build expressive workflows that easily adapt to changing feature sets, prediction targets, etc.\\n\\nPrefect can achieve expressivity through [dynamic workflows](https://www.prefect.io/guide/blog/workflow-orchestration-without-dags/), a type of workflow where the task to be executed are determined during the run. It does so by allowing `if/else` statements and loops to be collected within the `@flow` definitions. Since `if/else` branches only exist as part of the flow\u2019s code, it can become hard to build a mental model of the workflow as it increases in complexity. During the project iterations, some logical branches can become obsolete but remain hard to remove because of unclear function dependencies, ultimately decreasing maintainability.\\n\\n![Alt text](image-3.png)\\n\\nTo achieve expressivity, Hamilton relies on static DAGs. By decorating functions with `@config.when()`, the user can define logical and specify alternative implementations of a node. By loading the Python modules, the Hamilton Driver is aware of all possible functions (A, B, or even C!) and before allowing execution, resolves all @config points into a static DAG (see figure below). Calls to `Driver.execute()` operate over this fixed DAG, given the specified configuration. Additionally, Hamilton requires alternative implementations to share a name, for example `my_function__a`, `my_function__b`, making the code easy to read and test.\\n\\n![Alt text](image-4.png)\\n\\nThese features pair really well with Prefect\u2019s configurations and Blocks. For example, one can define the configuration for the Hamilton `Driver` in a JSON Block and load it within the workflow. Ultimately, this layered approach allows for all the needed expressivity of Prefect workflows while maintaining structural simplicity.\\n\\n![Alt text](image-5.png)\\n\\n![Alt text](image-6.png)\\n\\nIf you work in a hand-off model, this approach promotes a separation of concerns between the data engineers responsible for the Prefect production system and the data scientists in charge of developing business solutions by writing Hamilton code. Having this separation can also improve data consistency and reduce code duplication. For example, a single Prefect workflow can be reused with different Hamilton modules to create different models. Similarly, the same Hamilton data transformations can be reused across different Prefect workflows to power dashboards, API, applications, etc.\\n\\nBelow are two pictures. The first illustrates the high-level Prefect workflow containing two tasks. The second displays the low-level Hamilton DAG of the Python module `evaluate_model` imported in the Prefect task `train_and_evaluate_model`.\\n\\n![Alt text](image-7.png)\\n\\n![Alt text](image-8.png)\\n\\n## Handling data artifacts\\n\\nData science projects produce a large number of data artifacts from datasets, performance evaluations, figures, trained models, etc. The artifacts needed will change over the course of the project life cycle (data exploration, model optimization, production debugging, etc.). In certain scenarios, producing unnecessary or redundant data artifacts can incur significant computation and storage costs.\\n\\nHamilton provides the needed flexibility for data artifact generation through its [data saver API](https://hamilton.dagworks.io/en/latest/reference/api-reference/decorators.html#save-to). By decorating functions with `@save_to.*` you can toggle the saving of specific artifacts by changing the list of requested outputs passed to `Driver.execute()`. In the code below, calling `validation_predictions_table` will return the table whereas calling the `output_name_` value of `save_validation_predictions` will return the table and save it to `.csv`. This pattern is well-suited to work in conjunction with [Prefect Artifacts](https://docs.prefect.io/2.10.21/concepts/artifacts/#creating-link-artifacts), which allow you to tie artifacts from your runs (e.g., a model pushed to S3 or snippets of dataframes) and link them. This selective artifact generation can be managed through the Prefect Block configurations, without editing the Prefect workflow or Hamilton modules.\\n\\nFurthermore, the fine-grained Hamilton function graph allows for precise data lineage & provenance. Utility functions `what_is_downstream_of()` and `what_is_upstream_of()` help visualize and programmatically explore data dependencies. We point interested readers for more detail [here](https://medium.com/towards-data-science/lineage-hamilton-in-10-minutes-c2b8a944e2e6).\\n\\n## To finish & an example to get started\\n\\nHopefully by now we\u2019ve impressed on you that combining Hamilton with Prefect will help you with Prefect\u2019s workflow creation & maintainability challenges.\\n\\nTo help you get up and running, we have an [example](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/prefect) on how to use Hamilton with Prefect. It should cover all the basics that you need to get started. The README includes how to create a [free tier Prefect Cloud](https://www.prefect.io/pricing/) account and run a workflow that uses Hamilton; you can also run prefect locally without using the cloud version and link to appropriate documentation. In the example, we present a small scale machine learning project with a full end-to-end pipeline of creating features and then fitting and evaluating a model.\\n\\n## We want to hear from you!\\n\\nIf you\u2019re excited by any of this, or have strong opinions, drop by our Slack channel / or leave some comments here! Some resources to get you help:\\n\\n\ud83d\udce3 join our community on [Slack\u200a](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email) \u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started.\\n\\n\u2b50\ufe0f us on [GitHub](https://github.com/DAGWorks-Inc/hamilton)\\n\\n\ud83d\udcdd leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you find something\\n\\nOther Hamilton posts you might be interested in:\\n\\n* [tryhamilton.dev](https://www.tryhamilton.dev/) \u2013 an interactive tutorial in your browser!\\n* [Build a modular LLM stack with Hamilton](https://blog.dagworks.io/p/building-a-maintainable-and-modular)\\n* [Hamilton + Airflow](https://blog.dagworks.io/publish/post/130538397) ([GitHub repo](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/airflow))\\n* [Hamilton + Metaflow](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/) ([GitHub repo](https://github.com/outerbounds/hamilton-metaflow))\\n* [Pandas data transformations in Hamilton in 5 minutes](https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)\\n* [Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)"},{"id":"modular-llm","metadata":{"permalink":"/blog/modular-llm","source":"@site/blog/2023-07-11-modular-llm/index.md","title":"Building a maintainable and modular LLM application stack with Hamilton","description":"In this post, we\u2019re going to share how Hamilton can help you write modular and maintainable code for your large language model (LLM) application stack. Hamilton is great for describing any type of dataflow, which is exactly what you\u2019re doing when building an LLM powered application. With Hamilton you get strong software maintenance ergonomics, with the added benefit of being able to easily swap and evaluate different providers/implementations for components of your application.","date":"2023-07-11T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"vector search","permalink":"/blog/tags/vector-search"},{"inline":true,"label":"OpenAI","permalink":"/blog/tags/open-ai"},{"inline":true,"label":"Cohere","permalink":"/blog/tags/cohere"},{"inline":true,"label":"Weaviate","permalink":"/blog/tags/weaviate"},{"inline":true,"label":"Pinecone","permalink":"/blog/tags/pinecone"},{"inline":true,"label":"LanceDB","permalink":"/blog/tags/lance-db"}],"readingTime":17.535,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"modular-llm","title":"Building a maintainable and modular LLM application stack with Hamilton","authors":"tj","tags":["Hamilton","vector search","OpenAI","Cohere","Weaviate","Pinecone","LanceDB"]},"unlisted":false,"prevItem":{"title":"Simplify Prefect Workflow Creation and Maintenance with Hamilton","permalink":"/blog/prefect-hamilton"},"nextItem":{"title":"Simplify Airflow DAG Creation and Maintenance with Hamilton","permalink":"/blog/airflow-hamilton"}},"content":"In this post, we\u2019re going to share how [Hamilton](https://github.com/dagWorks-Inc/hamilton) can help you write modular and maintainable code for your large language model (LLM) application stack. Hamilton is great for describing any type of [dataflow](https://en.wikipedia.org/wiki/Dataflow), which is exactly what you\u2019re doing when building an LLM powered application. With Hamilton you get strong [software maintenance ergonomics](https://ceur-ws.org/Vol-3306/paper5.pdf), with the added benefit of being able to easily swap and evaluate different providers/implementations for components of your application.\\n\\n> crosspost from https://blog.dagworks.io/p/building-a-maintainable-and-modular\\n\\n\x3c!--truncate--\x3e\\n\\nThe example we\u2019ll walk through will mirror a typical LLM application workflow you\u2019d run to populate a vector database with some text knowledge. Specifically, we\u2019ll cover pulling data from the web, creating text embeddings (vectors) and pushing them to a vector store.\\n\\n![Alt text](image.png)\\n\\n## The LLM application dataflow\\n\\nTo start, let\u2019s describe what a typical LLM dataflow consists of. The application will receive a small data input (e.g., a text, a command) and act within a larger context (e.g., chat history, documents, state). This data will move through different services (LLM, vector database, document store, etc.) to perform operations, generate new data artifacts, and return final results. Most use cases repeat this flow multiple times while iterating over different inputs.\\n\\nSome common operations include:\\n* Convert text to embeddings\\n* Store / search / retrieve embeddings\\n* Find nearest neighbors to an embedding\\n* Retrieve text for an embedding\\n* Determine context required to pass into a prompt\\n* Prompt models with context from relevant text\\n* Send results to another service (API, database, etc.)\\n* \u2026\\n* and chaining them together!\\n\\nNow, let\u2019s think about the above in a production context, and imagine a user is unsatisfied with the outputs of your application and you want to find the root cause of the issue. Your application logged both the prompt and the results. Your code allows you to figure out the sequence of operations. Yet, you have no clue where things went wrong and the system produced undesirable output\u2026 To mitigate this, we\u2019d argue it\u2019s critical then to have lineage of data artifacts and the code that produces them, so you can debug situations such as these quickly.\\n\\nTo add to the complexity of your LLM application dataflow, many operations are non-deterministic, meaning you can\u2019t rerun or reverse engineer the operation to reproduce intermediate results. For example, an API call to generate a text or image response will likely be non-reproducible even if you have access to the same input and configuration (you can mitigate some of this with options such as [temperature](https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature)). This also extends to certain vector database operations like \u201cfind nearest\u201d where the result depends on the objects currently stored in the database. In production settings, it is prohibitive to near impossible to snapshot DB states to make calls reproducible.\\n\\n---\\n\\nFor these reasons, it is important to adopt flexible tooling to create robust dataflows that allow you to:\\n\\n1. plugin in various components easily.\\n2. see how components connect to each other.\\n3. add and customize common production needs like caching, validation, and observability.\\n4. adjust the flow structure to your needs without requiring a strong engineering skill set\\n5. plug into the traditional data processing and machine learning ecosystem.\\n\\nIn this post we\u2019ll give an overview of how Hamilton fulfills points 1, 2, & 4. We refer the user to our [documentation](https://hamilton.dagworks.io/en/latest/) for points 3 & 5.\\n\\n## Current LLM application development tooling\\n\\nThe LLM space is still in its infancy, and the usage patterns and tooling are rapidly evolving. While LLM frameworks can get you started, current options are not production tested; to our knowledge, no established tech companies are using the current popular LLM frameworks in production.\\n\\nDon\u2019t get us wrong, some of the tooling out there is great for getting a quick proof of concept up and running! However, we feel they fall short in two specific areas:\\n\\n1. **How to model the LLM application\u2019s dataflow**. We strongly believe that the dataflow of \u201cactions\u201d is better modeled as functions, rather than through object oriented classes and lifecycles. Functions are much simpler to reason about, test, and change. Object oriented classes can become quite opaque and impose more mental burden.\\n\\n> When something errors, object-oriented frameworks require you to drill into the objects\u2019 source code to understand it. Whereas with Hamilton functions, a clear dependency lineage tells you where to look and helps you reason about what happened (more on this below)!\\n\\n2. **Customization/extensions**. Unfortunately you need a strong software engineering skill set to modify the current frameworks once you step outside the bounds of what they make \u201ceasy\u201d to do. If that\u2019s not an option, this means you can end up stepping outside the framework for a particular piece of custom business logic, which can inadvertently lead you to maintaining more code surface area than if you didn\u2019t use the framework in the first place.\\n\\nFor more on these two points we point you to threads such as these two ([HackerNews](https://news.ycombinator.com/item?id=36645575#36647985), [Reddit](https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/)) that have users speak in more detail.\\n\\nWhile Hamilton is not a complete replacement for current LLM frameworks (e.g. there is no \u201cagent\u201d component), it does have all the building blocks to meet your LLM application needs and both can work in conjunction. If you want a clean, clear, and customizable way to write production code, integrate several LLM stack components, and gain observability over your app, then let\u2019s move onto the next few sections!\\n\\n## Building with Hamilton\\n\\nHamilton is a declarative micro-framework to describe [dataflows](https://en.wikipedia.org/wiki/Dataflow) in Python. It\u2019s not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows. Its strength is expressing the flow of data & computation in a way that is straightforward to create and maintain (much like DBT does for SQL), which lends itself very well to support modeling the data & computational needs of LLM applications.\\n\\n![Alt text](image-1.png)\\n\\nThe basics of Hamilton are simple, and it can be extended in quite a few ways; you don\'t have to know Hamilton to get value out of this post, but if you\'re interested, check out:\\n\\n* [tryhamilton.dev](https://www.tryhamilton.dev/) \u2013 an interactive tutorial in your browser!\\n* [Pandas data transformations in Hamilton in 5 minutes](https://towardsdatascience.com/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)\\n* [Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)\\n* [Hamilton + Airflow for production](https://blog.dagworks.io/publish/post/130538397)\\n\\n## Onto our example\\n\\nTo help set some mental context, picture this. You\u2019re a small data team that is tasked with creating an LLM application to \u201cchat\u201d with your organization\u2019s document. You believe it\u2019s important to evaluate candidate architectures in terms of features, performance profile, licenses, infrastructure requirements, and costs. Ultimately, you know your organization\u2019s primary concern is providing the most relevant results and a great user experience. The best way to assess this is to build a prototype, test different stacks and compare their characteristics and outputs. Then when you transition to production, you\u2019ll want confidence that the system can be maintained and introspected easily, to consistently provide a great user experience.\\n\\nWith that in mind, in this example, we will implement part of an LLM application, specifically the data ingestion step to index a knowledge base, where we convert text to embeddings and store them in a vector database. We implement this in a modular with a few different services/technologies. The broad steps are:\\n\\n1. Load the [SQuAD dataset](https://huggingface.co/datasets/squad) from the HuggingFace Hub. You would swap this out for your corpus of preprocessed documents.\\n2. Embed text entries using the [Cohere API](https://docs.cohere.com/reference/embed), the [OpenAI API](https://platform.openai.com/docs/api-reference/embeddings), or the [SentenceTransformer library](https://www.sbert.net/examples/applications/computing-embeddings/README.html). \\n3. Store embeddings in a vector database, either [LanceDB](https://lancedb.github.io/lancedb/), [Pinecone](https://docs.pinecone.io/docs/overview), or [Weaviate](https://weaviate.io/developers/weaviate).\\n\\nIf you need to know more about embeddings & search, we direct readers to the following links:\\n\\n* [Text embeddings explained - Weaviate](https://weaviate.io/blog/vector-embeddings-explained)\\n* [How-to conduct semantic search with Pinecone](https://docs.pinecone.io/docs/semantic-text-search)\\n\\n---\\n\\nAs we\u2019re walking through this example, it would be useful for you to think about/keep in mind the following:\\n\\n* **Compare what we show you with what you\u2019re doing now**. See how Hamilton enables you to curate and structure a project without needing an explicit LLM-centric framework. \\n* **Project and application structure**. Understand how Hamilton enforces a structure that enables you to build and maintain a modular stack.\\n* **Confidence in iteration & project longevity**. Combining the above two points, Hamilton enables you to more easily maintain an LLM application in production, no matter who authored it.\\n\\nLet\u2019s start with a visualization to give you an overview of what we\u2019re talking about:\\n\\n![Alt text](image-2.png)\\n\\nHere\u2019s what the LLM Application dataflow would look like when using pinecone with sentence transformers. With Hamilton to understand how things connect is just as simple as `display_all_functions()` call on the [Hamilton driver object](https://hamilton.dagworks.io/en/latest/reference/drivers/Driver/#hamilton.driver.Driver.display_all_functions).\\n\\n## Modular Code\\n\\nLet\u2019s explain the two main ways to implement modular code with Hamilton using our example for context.\\n\\n### @config.when\\nHamilton\u2019s focus is on readability. Without explaining what `@config.when` does, you can probably tell that this is a conditional statement, and only included when the predicate is satisfied. Below you will find the implementation for converting text to embeddings with the OpenAI and the Cohere API.\\n\\nHamilton will recognize two functions as alternative implementations because of the `@config.when` decorator and the same function name `embeddings` preceding the double underscore (`__cohere`, `__openai`). Their function signatures need not be entirely the same, which means it\'s easy and clear to adopt different implementations.\\n\\n```python title=\\"Modularity via @config.when\\"\\n# functions from the same file; embedding_module.py\\n\\n@config.when(embedding_service=\\"openai\\")\\ndef embeddings__openai(\\n    embedding_provider: ModuleType,\\n    text_contents: list[str],\\n    model_name: str = \\"text-embedding-ada-002\\",\\n) -> list[np.ndarray]:\\n    \\"\\"\\"Convert text to vector representations (embeddings) using OpenAI Embeddings API\\n    reference: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\\n    \\"\\"\\"\\n    response = embedding_provider.Embedding.create(input=text_contents, engine=model_name)\\n    return [np.asarray(obj[\\"embedding\\"]) for obj in response[\\"data\\"]]\\n\\n\\n@config.when(embedding_service=\\"cohere\\")\\ndef embeddings__cohere(\\n    embedding_provider: cohere.Client,\\n    text_contents: list[str],\\n    model_name: str = \\"embed-english-light-v2.0\\",\\n) -> list[np.ndarray]:\\n    \\"\\"\\"Convert text to vector representations (embeddings) using Cohere Embed API\\n    reference: https://docs.cohere.com/reference/embed\\n    \\"\\"\\"\\n    response = embedding_provider.embed(\\n        texts=text_contents,\\n        model=model_name,\\n        truncate=\\"END\\",\\n    )\\n    return [np.asarray(embedding) for embedding in response.embeddings]\\n```\\n\\nFor this project, it made sense to have all embedding services implemented in the same file with the `@config.when` decorator since there are only 3 functions per service. However, as the project grows in complexity, functions could be moved to separate modules too, and the next section\u2019s modularity pattern employed instead. Another point to note is that each of these functions is independently unit-testable. Should you have specific needs, it\u2019s straightforward to encapsulate it in the function and test it.\\n\\n### Switching out python modules\\n\\nBelow you will find the implementation of vector database operations for Pinecone and Weaviate. Note that the snippets are from `pinecone_module.py` and `weaviate_module.py` and notice how function signatures resemble and differ.\\n\\n```python title=\\"Modularity via Python modules\\n# functions from pinecone_module.py\\nfrom types import ModuleType\\n\\nimport numpy as np\\nimport pinecone\\n\\ndef client_vector_db(vector_db_config: dict) -> ModuleType:\\n    \\"\\"\\"Instantiate Pinecone client using Environment and API key\\"\\"\\"\\n    pinecone.init(**vector_db_config)\\n    return pinecone\\n  \\ndef data_objects(\\n    ids: list[str], titles: list[str], embeddings: list[np.ndarray], metadata: dict\\n) -> list[tuple]:\\n    \\"\\"\\"Create valid pinecone objects (index, vector, metadata) tuples for upsert\\"\\"\\"\\n    assert len(ids) == len(titles) == len(embeddings)\\n    properties = [dict(title=title, **metadata) for title in titles]\\n    embeddings = [x.tolist() for x in embeddings]\\n    return list(zip(ids, embeddings, properties))\\n\\ndef push_to_vector_db(\\n    client_vector_db: ModuleType,\\n    index_name: str,\\n    data_objects: list[tuple],\\n    batch_size: int = 100,\\n) -> int:\\n    \\"\\"\\"Upsert objects to Pinecone index; return the number of objects inserted\\"\\"\\"\\n    pinecone_index = pinecone.Index(index_name)\\n\\n    for i in range(0, len(data_objects), batch_size):\\n        i_end = min(i + batch_size, len(data_objects))\\n\\n        pinecone_index.upsert(vectors=data_objects[i:i_end])\\n    return len(data_objects)\\n  \\n\\n# from weaviate_module.py\\nimport numpy as np\\nimport weaviate\\n\\ndef client_vector_db(vector_db_config: dict) -> weaviate.Client:\\n    \\"\\"\\"Instantiate Weaviate client using Environment and API key\\"\\"\\"\\n    client = weaviate.Client(**vector_db_config)\\n    if client.is_live() and client.is_ready():\\n        return client\\n    else:\\n        raise ConnectionError(\\"Error creating Weaviate client\\")\\n    \\ndef data_objects(\\n    ids: list[str], titles: list[str], text_contents: list[str], metadata: dict\\n) -> list[dict]:\\n    \\"\\"\\"Create valid weaviate objects that match the defined schema\\"\\"\\"\\n    assert len(ids) == len(titles) == len(text_contents)\\n    return [\\n        dict(squad_id=id_, title=title, context=context, **metadata)\\n        for id_, title, context in zip(ids, titles, text_contents)\\n    ]\\n\\ndef push_to_vector_db(\\n    client_vector_db: weaviate.Client,\\n    class_name: str,\\n    data_objects: list[dict],\\n    embeddings: list[np.ndarray],\\n    batch_size: int = 100,\\n) -> int:\\n    \\"\\"\\"Push batch of data objects with their respective embedding to Weaviate.\\n    Return number of objects.\\n    \\"\\"\\"\\n    assert len(data_objects) == len(embeddings)\\n    with client_vector_db.batch(batch_size=batch_size, dynamic=True) as batch:\\n        for i in range(len(data_objects)):\\n            batch.add_data_object(\\n                data_object=data_objects[i], class_name=class_name, vector=embeddings[i]\\n            )\\n    return len(data_objects)\\n```\\n\\nWith Hamilton, the dataflow is stitched together using function names and function input arguments. Therefore by sharing function names for similar operations, the two modules are easily interchangeable. Since the LanceDB, Pinecone, and Weaviate implementations reside in separate modules, it reduces the number of dependencies per file and makes them shorter, improving both readability and maintainability. The logic for each implementation is clearly encapsulated in these named functions, so unit testing is straightforward to implement for each respective module. The separate modules reinforce the idea that they shouldn\u2019t be loaded simultaneously. The Hamilton driver will actually throw an error when multiple functions with the same name are found that helps enforce this concept.\\n\\n## Driver implications\\n\\nThe key part for running Hamilton code is the `Driver` object found in `run.py`. Excluding the code for the CLI and some argument parsing, we get:\\n\\n```python title=\\"Hamilton Driver\\"\\nconfig = dict(\\n    vector_db_config=json.loads(vector_db_config),\\n    embedding_service=embedding_service,  # this triggers config.when() in embedding_module\\n    api_key=embedding_service_api_key,\\n    model_name=model_name,\\n)\\n\\ndr = driver.Driver(\\n    config,\\n    data_module,\\n    vector_db_module,  # pinecone_module, weaviate_module or lancedb_module\\n    embedding_module,  # contains cohere, openai, and sentence_transformer implementations\\n    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\\n)\\n\\ndr.execute(\\n    final_vars=[\\"initialize_vector_db_indices\\", \\"push_to_vector_db\\"],\\n    inputs=dict(\\n        class_name=\\"SQuADEntry\\",\\n    ),\\n)\\n```\\n\\nThe Hamilton Driver, which orchestrates execution and is what you manipulate your dataflow through, allows modularity through three mechanisms as seen in the above code snippet:\\n\\n1. **Driver configuration**. this is a dictionary the driver receives at instantiation containing information that should remain constant, such as which API to use, or the embedding service API key. This integrates well with a command plane that can pass JSON or strings (e.g., a Docker container, [Airflow](https://blog.dagworks.io/publish/posts/detail/130538397), [Metaflow](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/), etc.). Concretely this is where we\u2019d specify swapping out what embedding API to use.\\n\\n2. **Driver modules**. the driver can receive an arbitrary number of independent Python modules to build the dataflow from. Here, the `vector_db_module` can be swapped in for the desired vector database implementation we\u2019re connecting to. One can also import modules dynamically through [importlib](https://docs.python.org/3/library/importlib.html#importlib.import_module), which can be useful for development vs production contexts, and also enable a configuration driven way to changing the dataflow implementation\\n\\n3. **Driver execution**. The `final_vars` parameter determines what output should be returned. You do not need to restructure your code to change what output you want to get. Let\u2019s take the case of wanting to debug something within our dataflow, it is possible to request the output of any function by adding its name to `final_vars`. For example, if you have some intermediate output to debug, it\u2019s easy to request it, or stop execution at that spot entirely. Note, the driver can receive inputs and overrides values when calling `execute()`; in the code above, the `class_name` is an execution time input that indicates the embedding object we want to create and where to store it in our vector database.\\n\\n### Modularity Summary\\n\\nIn Hamilton, the key to enable swappable components is to:\\n\\n1. define functions with effectively the same name and then,\\n2. annotate them with `@config.when` and choose which one to use via configuration passed to the driver, or,\\n3. put them in separate python modules and pass in the desired module to the driver.\\n\\nSo we\u2019ve just shown how you can plugin, swap, and call various LLM components with Hamilton. We didn\u2019t need to explain what an object oriented hierarchy is, nor require you to have extensive software engineering experience to follow (we hope!). To accomplish this, we just had to match function names, and their output types. We think this way of writing and modularizing code is therefore more accessible than current LLM frameworks permit.\\n\\n## Hamilton code in practice\\n\\nTo add to our claims, here a few practical implications of writing Hamilton code for LLM workflows that we\u2019ve observed:\\n\\n### CI/CD\\nThis ability to swap out modules/`@config.when` also means that integration testing in a CI system is straightforward to think about, since you have the flexibility and freedom to swap/isolate parts of the dataflow as desired.\\n\\n### Collaboration\\n1. The modularity Hamilton enables can allow one to mirror cross team boundaries easily. The function names & their output types become a contract, which ensures one can make surgical changes and be confident in the change, as well as have the visibility into downstream dependencies with Hamilton\u2019s [visualization and lineage features](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6) (like the initial visualization we saw). For example, it\u2019s clear how to interact and consume from the vector database.\\n\\n2. Code changes are simpler to review, because the flow is defined by declarative functions. The changes are self-contained; because there is no object oriented hierarchy to learn, just a function to modify.  Anything \u201ccustom\u201d is de facto supported by Hamilton.\\n\\n### Debugging\\nWhen there is an error with Hamilton, it\u2019s clear as to what the code it maps to is, and because of how the function is defined, one knows where to place it within the dataflow.\\n\\nTake the simple example of the embeddings function using cohere. If there was a time out, or error in parsing the response it would be clear that it maps to this code, and from the function definition you\u2019d know where in the flow it fits.\\n\\n```python title=\\"Cohere embedding function\\"\\n@config.when(embedding_service=\\"cohere\\")\\ndef embeddings__cohere(\\n    embedding_provider: cohere.Client,\\n    text_contents: list[str],\\n    model_name: str = \\"embed-english-light-v2.0\\",\\n) -> list[np.ndarray]:\\n    \\"\\"\\"Convert text to vector representations (embeddings) using Cohere Embed API\\n    reference: https://docs.cohere.com/reference/embed\\n    \\"\\"\\"\\n    response = embedding_provider.embed(\\n        texts=text_contents,\\n        model=model_name,\\n        truncate=\\"END\\",\\n    )\\n    return [np.asarray(embedding) for embedding in response.embeddings]\\n```\\n\\n![Alt text](image-3.png)\\n\\n## Tips for creating a modular LLM stack\\n\\nBefore we finish, here are some ideas to guide you through building your application. Some decisions might not have an obvious best choice, but having the right approach to modularity will allow you to efficiently iterate as requirements evolve.\\n\\n1. Before writing any code, draw a DAG of the logical steps of your workflow. This sets the basis for defining common steps and interfaces that are not service-specific.\\n\\n2. Identify steps that could be swapped. By being purposeful with configuration points, you will reduce risks of [speculative generality](https://refactoring.guru/smells/speculative-generality). Concretely, this would result in functions with less arguments, default values, and grouped into thematic modules.\\n\\n3. Chunk parts of your dataflow into modules with few dependencies, if relevant. This will lead to shorter Python files with fewer package dependencies, improved readability and maintainability. Hamilton is indifferent and can build its DAG from multiple modules.\\n\\n## To close & future directions\\n\\nThanks for getting this far. We believe that Hamilton has a part to play in helping everyone express their dataflows, and LLM applications are just one use case! To summarize our messaging in this post can be boiled down to:\\n\\n1. It is useful to conceive of LLM applications as dataflows, and are therefore a great fit for using Hamilton.\\n\\n2. Object-centric LLM frameworks can be opaque and hard to extend and maintain for your production needs. Instead, one should write their own integrations with Hamilton\u2019s straightforward declarative style. Doing so will improve your code\u2019s transparency and maintainability, with clear testable functions, clear mapping of runtime errors to functions, and built-in visualization of your dataflow.\\n\\n3. The modularity prescribed by using Hamilton will make collaboration more efficient and provide you with the requisite flexibility to modify and change your LLM workflows at the speed at which the field is moving.\\n\\nWe now invite you to play around with, try, and modify the full example for yourselves [here](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/modular_llm_stack). There is a `README` that will explain the commands to run and get started. Otherwise, we are working on making the Hamilton + LLM Application experience even better by thinking about the following:\\n\\n1. **Agents**. Can we provide the same level of visibility to agents that we have for regular Hamilton dataflows?\\n\\n2. **Parallelization**. How can we make it simpler to express running a dataflow over a list of documents for example. See this work in progress PR for what we mean.\\n\\n3. **Plugins for caching and observability**. One can already implement a custom caching and observability solution on top of Hamilton. We\u2019re working on providing more standard options out of the box for common components, e.g. redis.\\n\\n4. **A user contributed dataflows section**. We see the possibility to standardize on common names for specific LLM application use cases. In which case we can start to aggregate Hamilton dataflows, and allow people to pull them down for their needs.\\n\\n## We want to hear from you! \\n\\nIf you\u2019re excited by any of this, or have strong opinions, drop by our Slack channel / or leave some comments here! Some resources to get you help:\\n\\n\ud83d\udce3 join our community on [Slack](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email) \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started.\\n\\n\u2b50\ufe0f us on [GitHub](https://github.com/DAGWorks-Inc/hamilton)\\n\\n\ud83d\udcdd leave us an [issue](https://github.com/DAGWorks-Inc/hamilton/issues) if you find something\\n\\nOther Hamilton posts you might be interested in:\\n\\n* [tryhamilton.dev](https://www.tryhamilton.dev/) \u2013 an interactive tutorial in your browser!\\n* [Pandas data transformations in Hamilton in 5 minutes](https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5)\\n* [Lineage + Hamilton in 10 minutes](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6)\\n* [Hamilton + Airflow for production](https://blog.dagworks.io/publish/post/130538397)"},{"id":"airflow-hamilton","metadata":{"permalink":"/blog/airflow-hamilton","source":"@site/blog/2023-06-28-airflow-hamilton/index.md","title":"Simplify Airflow DAG Creation and Maintenance with Hamilton","description":"This post walks you through the benefits of having Hamilton and Airflow directed acyclic graphs (DAGs) work in tandem. Airflow is responsible for orchestration (think macro) and Hamilton helps author clean and maintainable data transformations (think micro). The reason you can run Hamilton with Airflow, is that Hamilton is just a library with a small dependency footprint, so one can get started with Hamilton in no time! For those that are unfamiliar with Hamilton, we point you to an interactive overview on tryhamilton.dev. Otherwise we will talk about Hamilton at a high level and point to reference documentation for more details.","date":"2023-06-28T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"},{"inline":true,"label":"Airflow","permalink":"/blog/tags/airflow"},{"inline":true,"label":"orchestration","permalink":"/blog/tags/orchestration"}],"readingTime":9.365,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"airflow-hamilton","title":"Simplify Airflow DAG Creation and Maintenance with Hamilton","authors":"tj","tags":["Hamilton","Airflow","orchestration"]},"unlisted":false,"prevItem":{"title":"Building a maintainable and modular LLM application stack with Hamilton","permalink":"/blog/modular-llm"},"nextItem":{"title":"The perks of creating dataflows with Hamilton","permalink":"/blog/perks-of-hamilton"}},"content":"This post walks you through the benefits of having [Hamilton](https://github.com/dagworks-inc/hamilton) and [Airflow](https://airflow.apache.org/) [directed acyclic graphs](https://en.wikipedia.org/wiki/Directed_acyclic_graph) (DAGs) work in tandem. Airflow is responsible for orchestration (think macro) and Hamilton helps author clean and maintainable data transformations (think micro). The reason you can run Hamilton with Airflow, is that Hamilton is just a library with a small dependency footprint, so one can get started with Hamilton in no time! For those that are unfamiliar with Hamilton, we point you to an interactive overview on [tryhamilton.dev](tryhamilton.dev). Otherwise we will talk about Hamilton at a high level and point to reference documentation for more details.\\n![Apache Airflow Banner](airflow-banner.png)\\n\\n> crosspost from https://blog.dagworks.io/p/supercharge-your-airflow-dag-with\\n\\n\x3c!--truncate--\x3e\\n\\nAirflow is the industry standard to orchestrate data pipelines. It powers all sorts of data initiatives including ETL, ML pipelines and BI. Since its inception in 2014, Airflow users have faced certain rough edges with regards to authoring and maintaining data pipelines:\\n\\n* Maintainably managing the evolution of workflows; what starts simple can invariably get complex.\\n* Writing modular, reusable, and testable code that runs within an Airflow task.\\n* Tracking lineage of code and data artifacts that an Airflow DAG produces.\\n\\nThis is where we believe Hamilton can help! Hamilton is a Python micro-framework for writing data transformations. In short, one writes python functions in a \u201cdeclarative\u201d style, which Hamilton parses and connects into a graph based on their names, arguments and type annotations. Specific outputs can be requested and Hamilton will execute the required function path to produce them. Because it doesn\u2019t provide macro orchestrating capabilities, it pairs nicely with Airflow by helping data professionals write cleaner code and more reusable code for Airflow DAGs.\\n\\n![Hamilton 101](./hamilton_101.png)\\n\\n## Write maintainable Airflow DAGs\\n\\nA common use of Airflow is to help with machine learning/data science. Running such workloads in production often requires complex workflows. A necessary design decision is determining how to break up the workflow into Airflow tasks. Create too many and you increase scheduling and execution overhead (e.g. moving lots of data), create too few and you have monolithic tasks that can take a while to run, but probably are more efficient. The trade-off here is Airflow DAG complexity versus code complexity  within each of the tasks. This makes debugging and reasoning about the workflow harder, especially if you did not author the initial Airflow DAG. More often than not, the initial task structure of the Airflow DAG becomes fixed, because refactoring the task code becomes prohibitive! \\n\\nWhile simpler DAGs such as A->B->C are desirable, there is an inherent tension between the structure\u2019s simplicity and the amount of code per task. The more code per task, the more difficult it is to identify points of failure, at the trade-off of potential computational efficiencies, but in the case of failures, retries grow in cost with the \u201csize\u201d of the task.\\n\\n![Airflow node size](airflow_node_size.png)\\n\\nInstead, what if you could simultaneously wrangle the complexity within an Airflow task, no matter the size of code within it, and gain the flexibility to easily change the Airflow DAG shape with minimal effort? This is where Hamilton comes in. \\n\\nWith Hamilton you can replace the code within each Airflow task with a Hamilton DAG, where Hamilton handles the \u201cmicro\u201d orchestration of the code within the task. Note: Hamilton actually enables you to logically model everything that you\u2019d want an Airflow DAG to do. More on that below.\\n\\nTo use Hamilton, you load a Python module that contains your Hamilton functions, instantiate a [Hamilton Driver](https://hamilton.dagworks.io/en/latest/concepts/driver-capabilities/) and execute a Hamilton DAG within an Airflow task in a few lines of code. By using Hamilton, you can write your data transformation at an arbitrary granularity, allowing you to inspect in greater details what each Airflow task is doing. \\n\\nSpecifically the mechanics of the code are:\\n\\n1. Import your function modules\\n2. Pass them to the Hamilton driver to build the DAG. \\n3. Then, call Driver.execute() with the outputs you want to execute from the DAG you\u2019ve defined.\\n\\nLet\u2019s look at some code that create a single node Airflow DAG but uses Hamilton to train and evaluate a ML model:\\n\\n```python title=\\"Airflow DAG with nested Hamilton\\"\\nfrom airflow.decorators import dag, task\\nfrom airflow.operators.python import get_current_context\\n\\n# set the Airflow DAG parameters. This will appear in the Airflow UI. \\nDEFAULT_DAG_PARAMS = dict(\\n    label=\\"absenteeism_time_in_hours\\",\\n    feature_set=[\\n          \\"age_zero_mean_unit_variance\\",\\n          \\"has_children\\",\\n          \\"has_pet\\",\\n          \\"is_summer\\",\\n          \\"service_time\\",\\n    ],\\n    h_train_and_evaluate=dict(...),  # config for the Hamilton Driver\\n)\\n\\n@dag(\\n    dag_id=\\"hamilton-absenteeism-prediction\\",\\n    description=\\"Predict absenteeism using Hamilton and Airflow\\",\\n    start_date=datetime(2023, 6, 18),\\n    params=DEFAULT_DAG_PARAMS,  # pass the default params to the Airflow DAG\\n)\\ndef absenteeism_prediction_dag():\\n    \\"\\"\\"Predict absenteeism using Hamilton and Airflow\\"\\"\\"\\n\\n    # Below we have a single Airflow task that uses 2 Python modules (evaluate_model, train_model).\\n    # Both are loaded into the Hamilton driver in a single Airflow task, reducing the number of\\n    # Airflow task and preventing having to move data between the two steps. However, it remains\\n    # beneficial to separate the code into 2 modules since training and evaluation are independent and\\n    # might be reused in separate contexts.\\n    @task\\n    def train_and_evaluate_model(features_path: str):\\n        \\"\\"\\"Train and evaluate a machine learning model\\"\\"\\"\\n        import evaluate_model  # user defined function module\\n        import train_model  # user defined function module\\n\\n        from hamilton import base, driver\\n\\n        context = get_current_context()\\n        PARAMS = context[\\"params\\"]  # get the Airflow runtime config\\n\\n        hamilton_config = PARAMS[\\"h_train_and_evaluate_model\\"]\\n        dr = driver.Driver(\\n            hamilton_config,\\n            train_model,\\n            evaluate_model,  # pass function modules to the Hamilton driver\\n            adapter=base.SimplePythonGraphAdapter(base.DictResult()),\\n        )\\n\\n        results = dr.execute(\\n            # `final_vars` specifies Hamilton functions results we want as outputs.\\n            final_vars=[\\"save_validation_preds\\", \\"model_results\\"],\\n            inputs={\\n                \\"features_path\\": features_path,  # value retrieved from Airflow XCom\\n                \\"label\\": PARAMS[\\"label\\"],\\n                \\"feature_set\\": PARAMS[\\"feature_set\\"],\\n            },\\n        )\\n```\\n\\nNow, we didn\u2019t show the Hamilton code here, but the benefits of this approach are:\\n\\n1. **Unit & integration testing.** Hamilton, through its naming and type annotations requirements, pushes developers to write modular Python code. This results in Python modules well-suited for unit testing. Once your Python code is unit tested, you can develop integration tests to ensure it behaves properly in your Airflow tasks. In contrast, testing code contained in an Airflow task is less trivial, especially in CI/CD settings, since it requires having access to an Airflow environment.\\n\\n2. **Reuse data transformations.** This approach keeps the data transformations code in Python modules, separated from the Airflow DAG file. This means this code is also runnable outside of Airflow! If you come from the analytics world, it should feel similar to developing and testing SQL queries in an external .sql file, then loading it into your Airflow Postgres operators.\\n\\n3. **Reorganize your Airflow DAG easily.** The lift required to change your Airflow DAG is now much lower. If you logically model everything in Hamilton, e.g. an end to end machine learning pipeline, it\u2019s just a matter of determining how much of this Hamilton DAG needs to be computed in each Airflow task. For example, you change the number of tasks from one monolithic Airflow task, to a few, or to many \u2014 all that would need to change is what you request from Hamilton since your Hamilton DAG needn\u2019t change at all!\\n\\n## Iterative development with Hamilton & Airflow\\n\\nIn most data science projects, it would be impossible to write the DAG of the final system from day 1 as requirements will change. For example, the data science team might want to try different feature sets for their model. Until the list is set and finalized, it is probably undesirable to have the feature set in your source code and under version control; configuration files would be preferable.\\n\\nAirflow supports default and runtime DAG configurations and will log these settings to make every DAG run reproducible. However, adding configurable behaviors will require committing adding conditional statements and complexity to your Airflow task code. This code might become obsolete during the project or only be useful in particular scenarios, ultimately decreasing your DAGs readability.\\n\\nIn contrast, Hamilton can use Airflow\u2019s runtime configuration to execute different data transformations from the function graph on the fly. This layered approach can greatly increase the expressivity of Airflow DAGs while maintaining structural simplicity. Alternatively, Airflow can [dynamically generate new DAGs](https://airflow.apache.org/docs/apache-airflow/stable/howto/dynamic-dag-generation.html) from configurations, but this could decrease observability and some of these features remain experimental.\\n\\n![Airflow Config UI](image.png)\\n\\nIf you work in a hand-off model, this approach promotes a separation of concerns between the data engineers responsible for the Airflow production system and the data scientists in charge of developing business solutions by writing Hamilton code. Having this separation can also improve data consistency and reduce code duplication. For example, a single Airflow DAG can be reused with different Hamilton modules to create different models. Similarly, the same Hamilton data transformations can be reused across different Airflow DAGs to power dashboards, API, applications, etc.\\n\\nBelow are two pictures. The first illustrates the high-level Airflow DAG containing 2 nodes. The second displays the low-level Hamilton DAG of the Python module `evaluate_model` imported in the Airflow task `train_and_evaluate_model`.\\n\\n![Airflow DAG UI](image-1.png)\\n\\n![Hamilton DAG viz](image-2.png)\\n\\n## Handling data artifacts\\n\\nData science projects produce a large number of data artifacts from datasets, performance evaluations, figures, trained models, etc. The artifacts needed will change over the course of the project life cycle (data exploration, model optimization, production debugging, etc.). This is a problem for Airflow since removing a task from a DAG will delete its metadata history and break the artifact lineage. In certain scenarios, producing unnecessary or redundant data artifacts can incur significant computation and storage costs.\\n\\nHamilton can provide the needed flexibility for data artifact generation through its [data saver API](https://hamilton.dagworks.io/en/latest/reference/decorators/save_to/). Functions decorated with `@save_to.*` add the possibility to store their output, one need only to request this functionality via `Driver.execute()`. In the code below, calling `validation_predictions_table` will return the table whereas calling the `output_name_` value of `save_validation_predictions` will return the table and save it to `.csv`.\\n\\n```python title=\\"Saving data artifacts with Hamilton\\"\\n# from the function  module\\n@save_to.csv(path=source(\\"pred_path\\"), output_name_=\\"save_validation_predictions\\")\\ndef validation_predictions_table(y_validation: np.ndarray, val_pred: np.ndarray) -> pd.DataFrame:\\n    \\"\\"\\"Create a table with the model\'s predictions on the validation set\\"\\"\\"\\n    return pd.DataFrame({\\"y_validation\\": y_validation, \\"val_pred\\": val_pred})\\n  \\n# from the main process\\nresults = dr.execute(\\n  final_vars=[ \\n    \\"validation_predictions_table\\",  # the function name\\n    \\"save_validation_predictions\\",  # the decorator `output_name_` argument\\n  ]\\n)\\n```\\n\\nThis added flexibility allows users to easily toggle the artifacts generated and it can be done directly through the Airflow runtime configuration, without editing the Airflow DAG or Hamilton modules.\\n\\nFurthermore, the fine-grained Hamilton function graph allows for precise data lineage & provenance. Utility functions `what_is_downstream_of()` and `what_is_upstream_of()` help visualize and programmatically explore data dependencies. Hamilton co-creator, Stefan Krawczyk, goes into detail [here](https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6).\\n\\n## An example to get started\\n\\nTo help you get up and running, we have an example on how to use Hamilton with Airflow in the Hamilton GitHub repository (find it [here](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/airflow)). The README will indicate how to set up Airflow with Docker. The example contains two Airflow DAGs, one showcasing a basic Hamilton \u201chow-to\u201d and the other a more complete ML project example. If you have questions or need help - please join our [Slack](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email). \\n\\n![Example Structure](image-3.png)\\n\\nOtherwise, to learn more about Hamilton\u2019s features and functionality, please consult the documentation.\\n\\n## References\\n\\n* [Hamilton + Airflow example](https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/airflow)\\n* [Hamilton Documentation](https://hamilton.dagworks.io/)\\n* [tryhamilton.dev](https://www.tryhamilton.dev/) \u2014 an interactive way to learn more about Hamilton.\\n* For another orchestration system integrating with Hamilton, you can checkout [Hamilton + Metaflow](https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/).\\n* [Hamilton Slack community](https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email)"},{"id":"perks-of-hamilton","metadata":{"permalink":"/blog/perks-of-hamilton","source":"@site/blog/2022-08-08-perks-of-hamilton/index.md","title":"The perks of creating dataflows with Hamilton","description":"Hamilton is an open-source Python microframework developed at Stitch Fix. It automagically organizes Python functions into a directed acyclic graph (DAG) from their name and type annotations. It was originally created to facilitate working with tabular datasets containing hundreds of columns, but it\u2019s general enough to enable many data science or machine learning (ML) scenarios.","date":"2022-08-08T00:00:00.000Z","tags":[{"inline":true,"label":"Hamilton","permalink":"/blog/tags/hamilton"}],"readingTime":6.975,"hasTruncateMarker":true,"authors":[{"name":"Thierry Jean","url":"https://github.com/zilto","imageURL":"https://github.com/zilto.png","key":"tj","page":null}],"frontMatter":{"slug":"perks-of-hamilton","title":"The perks of creating dataflows with Hamilton","authors":"tj","tags":["Hamilton"]},"unlisted":false,"prevItem":{"title":"Simplify Airflow DAG Creation and Maintenance with Hamilton","permalink":"/blog/airflow-hamilton"}},"content":"[Hamilton](https://github.com/stitchfix/hamilton) is an open-source Python microframework developed at [Stitch Fix](https://www.stitchfix.com/). It automagically organizes Python functions into a *directed acyclic graph* (DAG) from their name and type annotations. It was originally created to facilitate working with tabular datasets containing hundreds of columns, but it\u2019s general enough to enable many data science or machine learning (ML) scenarios.\\n\\n> crosspost from https://medium.com/@thijean/the-perks-of-creating-dataflows-with-hamilton-36e8c56dd2a\\n\\n\x3c!--truncate--\x3e\\n\\nOver the last year, I used Hamilton both in academic research to build forecast models from smartphone sensor data, and in industry to implement dynamic bidding strategies for an ad exchange. Amongst a sea of awesome open-source tools, Hamilton happened to neatly suit my needs. Below, I will give a brief overview of Hamilton, and break down what I believe to be its key strengths:\\n\\n1. Improve code readability\\n2. Facilitate reproducible pipelines\\n3. Enable faster iteration cycles\\n4. Reduce the development-production gap\\n\\nTo learn more about the design and the development of the framework, the [Stitch Fix\u2019s engineering blog](https://multithreaded.stitchfix.com/blog/) is a gold mine.\\n\\n## Quick introduction to Hamilton\\n\\nHamilton relies on 3 main components: the *functions*, the *driver*, and the *desired outputs*.\\n\\n*Functions* are your regular Python functions, but each needs to have a unique name and type annotated inputs and outputs, and be defined within a Python module (.py file).\\n\\nOne or more modules are passed to the *driver* along a configuration object. The driver builds a DAG by linking a function\u2019s arguments (named and annotated) to other functions\u2019 name. At this point, no computation has happened yet.\\n\\n![Alt text](image.png)\\n\\nFinally, a list of *desired outputs*, which can be any node from the graph, is passed to the driver. When *executed*, the driver computes the desired output by running only the necessary functions. By default, Hamilton outputs pandas dataframe, but it can also return a dictionary containing arbitrary objects.\\n\\n## Suggested development workflow\\nMy preferred development workflow relies on opening side-to-side a Python module and a Jupyter notebook with the [%autoreload ipython magic](https://ipython.org/ipython-doc/3/config/extensions/autoreload.html) configured (code snippet below). I write my data transformation in the module, and in the notebook, I instantiate a Hamilton driver and test the DAG with a small subset of data. Because of `%autoreload`, the module is reimported with the latest changes each time the Hamilton DAG is executed. This approach prevents out-of-order notebook executions, and functions always reside in clean .py files.\\n\\n```python title=\\"Jupyter notebook autoreload\\nfrom hamilton.driver import Driver\\nimport my_module  # data transformation module\\n\\n%load_ext autoreload   # load extension\\n%autoreload 1  # configure autoreload to only affect specified files\\n%aimport my_module  # specify my_module to be reloaded\\n\\nhamilton_driver = Driver({}, my_module)\\nhamilton_driver.execute([\'desired_output1\', \'desired_output2\'])\\n```\\n\\n## Improve code readability\\n> Zen of Python #7: Readability counts\\n\\nCode readability is multifaceted, but can be summarized to making code easy to understand for colleagues, reviewers, and your future self. You may think that your pandas operations are self-evident, or that writing a separate function for 1\u20133 lines of numpy code is overkill, **but you\u2019re likely wrong**.\\n\\n```python title=\\"Hamilton ABC\\nimport pandas as pd\\n\\ndef avg_3wk_spend(spend: pd.Series) -> pd.Series:\\n    \\"\\"\\"Rolling 3 week average spend.\\"\\"\\"\\n    return spend.rolling(3).mean()\\n\\ndef spend_per_signup(spend: pd.Series, signups: pd.Series) -> pd.Series:\\n    \\"\\"\\"The cost per signup in relation to spend.\\"\\"\\"\\n    return spend / signups\\n```\\n\\nSimply by requiring unique function names and type annotations, Hamilton pushes developers to divide the pipeline into steps that each hold their own **intent**. It generates a **semantic layer** that is decoupled from the data transformation implementation. In the above example, the name `avg_3wk_spend` and the docstring communicate a clear intent compared to the pandas code `spend.rolling(3).mean()` (note that the time unit couldn\u2019t be inferred!) Communicating the intent or the business purpose of a function helps understand the broader pipeline, but also allows collaborators to improve or replace a given implementation while preserving the intent.\\n\\nBreaking down complex functions into simpler single-purpose functions has many other benefits. For one, abstracting repetitive or redundant operations makes your code [DRY](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself)-er and easier to unit test and debug. Also, meaningful results become more clearly separated from intermediary transformations. Utility functions to view the computation graph diagram can be helpful during the development process.\\n\\n![Alt text](image-1.png)\\n\\n\\n## Facilitate reproducible pipeline\\nBoth in academia and in industry, data science and ML projects generate a myriad of results and artifacts. *Experiment tracking* typically refers to the systematic and organized tracking of those artifacts. It\u2019s most often discussed in the context of ML training and hyperparameter optimization, leaving out data transformation pipelines despite their influence on the former.\\n\\nWith Hamilton, the end-to-end transformations can be tracked from a few parameters. Since the Hamilton DAG is built in a deterministic manner, it doesn\u2019t have to be logged; only the functions it\u2019s built need to. No large artifacts have to be created! When running experiments with your favorite tool (MLFlow, Weights&biases, etc.), simply log the Hamilton **driver configuration** and the **Git SHA1** of your python modules. To go a step further, you can store a **picture of the executed DAG** and track the **package version** in case of future behavior change.\\n\\n```python title=\\"Visualize the DAG\\nfrom hamilton.driver import Driver\\n\\nimport time_transform\\nimport location_transform\\n\\nif __name__ == \\"__main__\\":\\n    config = dict(connection={\\"database\\": \\":memory:\\"})  # required driver config\\n    hamilton_driver = Driver(config, time_transform, location_transform)\\n\\n    # view the complete computation DAG\\n    hamilton_driver.display_all_functions(\\"./file.dot\\", {})\\n    # view only the path to the specified outputs\\n    hamilton_driver.visualize_execution([\\"period_of_day\\", \\"location_jump_speed\\"], \\"./file.dot\\", {})\\n```\\n\\n![Alt text](image-2.png)\\n\\n## Enable faster iteration cycles\\nMany \u201cDAG-based\u201d frameworks (Airflow, Metaflow, flyte, Prefect, etc.) are gaining traction in the data science and ML community. However, most are intended for orchestration, which is broader than the data transformation problem Hamilton aims to solve. It remains non-trivial to identify frameworks that align with your needs, have a simple, clear and well-documented API, require minimal [glue code](https://en.wikipedia.org/wiki/Glue_code), and are easy to move away from.\\n\\nOrchestration frameworks rely on first defining processing steps, and then manually connecting them into a DAG. Connections have to be specified through decorators, classes, functions, or even YAML configuration files. Such approach imposes mental burden on data scientists and forces them to rewrite the DAG every time they want to investigate a new hypothesis. The problem only worsens as a project scales in complexity. Because this process is error-prone, a lot of time can be spent wrestling with the framework. Automatically building the DAG can lead to productivity improvements.\\n\\nRelying on regular Python functions, Hamilton requires minimal refactoring to get started (a [robust migration](https://hamilton-docs.gitbook.io/docs/best-practices/migrating-to-hamilton) guide is available). This allows adopters to make \u201cquick wins\u201d and eases the onboarding of colleagues. For complex scenarios, powerful features are accessible through function [decorators](https://hamilton-docs.gitbook.io/docs/reference/api-reference/available-decorators). In all cases, your code remains usable outside of Hamilton (minus the decorators).\\n\\n>**Aside**: On the opposite end of the spectrum, I worked with [Kedro](https://kedro.readthedocs.io/en/stable/) which is a holistic and opinionated framework for data pipelines, and had a positive experience. It can feel restrictive at times, but the built-in conventions and the extensive set of tools it provides (i.e., configuration, versioning, tests, notebooks, etc.) does improve team productivity and solution robustness.\\n\\n## Reduce the development-production gap\\nWhile Hamilton is a great framework for iterations, how does it fair in production? It might work for Stitch Fix, but can it handle my specific business use case? Is it computationally efficient? The TL;DR. is yes!\\n\\nIn Hamilton, the driver receives the DAG instructions and later executes the computation. Originally, it relied on the pandas library to calculate new columns, which can [become inefficient at scale](https://pandas.pydata.org/docs/user_guide/scale.html). An exciting addition was the release of [Spark, Dask, and Ray](https://multithreaded.stitchfix.com/blog/2022/02/22/scaling-hamilton/) drivers. Now, pandas data transformation can be executed by the Hamilton Dask driver and get the performance increase for free. It allows data scientists to define and test functions locally, and move to production without refactoring. What\u2019s not to love!\\n\\nRecently, [data validation](https://multithreaded.stitchfix.com/blog/2022/07/26/hamilton-data-quality/) at the node level and support for the [pandera](https://pandera.readthedocs.io/en/stable/) library (another great lightweight package!) were added. The development team is actively improving integration with open source tools. For advanced users, it\u2019s possible to extend the framework\u2019s standard interfaces (driver, decorator, result builder, etc.) to meet your requirements. People at Stitch Fix are very responsive and eager to help through [GitHub issues](https://github.com/stitchfix/hamilton/issues) and their [Slack channel](https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg).\\n\\n## Conclusion\\nHamilton is a great tool for data scientists and ML folks. I hope the overview provided convinced you to give it a try. You\u2019re only `pip install sf-hamilton` away from getting started!\\n\\nReferences:\\n\\n1. Hamilton GitHub page (2022), https://github.com/stitchfix/hamilton\\n2. Stich Fix engineering blog (2022), https://multithreaded.stitchfix.com/engineering/"}]}}')}}]);