"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5279],{9961:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>d});var i=t(5893),o=t(1151);const a={slug:"modular-llm",title:"Building a maintainable and modular LLM application stack with Hamilton",authors:"tj",tags:["Hamilton","vector search","OpenAI","Cohere","Weaviate","Pinecone","LanceDB"]},s=void 0,r={permalink:"/blog/modular-llm",source:"@site/blog/2023-07-11-modular-llm/index.md",title:"Building a maintainable and modular LLM application stack with Hamilton",description:"In this post, we\u2019re going to share how Hamilton can help you write modular and maintainable code for your large language model (LLM) application stack. Hamilton is great for describing any type of dataflow, which is exactly what you\u2019re doing when building an LLM powered application. With Hamilton you get strong software maintenance ergonomics, with the added benefit of being able to easily swap and evaluate different providers/implementations for components of your application.",date:"2023-07-11T00:00:00.000Z",tags:[{inline:!0,label:"Hamilton",permalink:"/blog/tags/hamilton"},{inline:!0,label:"vector search",permalink:"/blog/tags/vector-search"},{inline:!0,label:"OpenAI",permalink:"/blog/tags/open-ai"},{inline:!0,label:"Cohere",permalink:"/blog/tags/cohere"},{inline:!0,label:"Weaviate",permalink:"/blog/tags/weaviate"},{inline:!0,label:"Pinecone",permalink:"/blog/tags/pinecone"},{inline:!0,label:"LanceDB",permalink:"/blog/tags/lance-db"}],readingTime:17.535,hasTruncateMarker:!0,authors:[{name:"Thierry Jean",url:"https://github.com/zilto",imageURL:"https://github.com/zilto.png",key:"tj",page:null}],frontMatter:{slug:"modular-llm",title:"Building a maintainable and modular LLM application stack with Hamilton",authors:"tj",tags:["Hamilton","vector search","OpenAI","Cohere","Weaviate","Pinecone","LanceDB"]},unlisted:!1,prevItem:{title:"Simplify Prefect Workflow Creation and Maintenance with Hamilton",permalink:"/blog/prefect-hamilton"},nextItem:{title:"Simplify Airflow DAG Creation and Maintenance with Hamilton",permalink:"/blog/airflow-hamilton"}},l={authorsImageUrls:[void 0]},d=[{value:"The LLM application dataflow",id:"the-llm-application-dataflow",level:2},{value:"Current LLM application development tooling",id:"current-llm-application-development-tooling",level:2},{value:"Building with Hamilton",id:"building-with-hamilton",level:2},{value:"Onto our example",id:"onto-our-example",level:2},{value:"Modular Code",id:"modular-code",level:2},{value:"@config.when",id:"configwhen",level:3},{value:"Switching out python modules",id:"switching-out-python-modules",level:3},{value:"Driver implications",id:"driver-implications",level:2},{value:"Modularity Summary",id:"modularity-summary",level:3},{value:"Hamilton code in practice",id:"hamilton-code-in-practice",level:2},{value:"CI/CD",id:"cicd",level:3},{value:"Collaboration",id:"collaboration",level:3},{value:"Debugging",id:"debugging",level:3},{value:"Tips for creating a modular LLM stack",id:"tips-for-creating-a-modular-llm-stack",level:2},{value:"To close &amp; future directions",id:"to-close--future-directions",level:2},{value:"We want to hear from you!",id:"we-want-to-hear-from-you",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["In this post, we\u2019re going to share how ",(0,i.jsx)(n.a,{href:"https://github.com/dagWorks-Inc/hamilton",children:"Hamilton"})," can help you write modular and maintainable code for your large language model (LLM) application stack. Hamilton is great for describing any type of ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Dataflow",children:"dataflow"}),", which is exactly what you\u2019re doing when building an LLM powered application. With Hamilton you get strong ",(0,i.jsx)(n.a,{href:"https://ceur-ws.org/Vol-3306/paper5.pdf",children:"software maintenance ergonomics"}),", with the added benefit of being able to easily swap and evaluate different providers/implementations for components of your application."]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["crosspost from ",(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/p/building-a-maintainable-and-modular",children:"https://blog.dagworks.io/p/building-a-maintainable-and-modular"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The example we\u2019ll walk through will mirror a typical LLM application workflow you\u2019d run to populate a vector database with some text knowledge. Specifically, we\u2019ll cover pulling data from the web, creating text embeddings (vectors) and pushing them to a vector store."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Alt text",src:t(82).Z+"",width:"848",height:"980"})}),"\n",(0,i.jsx)(n.h2,{id:"the-llm-application-dataflow",children:"The LLM application dataflow"}),"\n",(0,i.jsx)(n.p,{children:"To start, let\u2019s describe what a typical LLM dataflow consists of. The application will receive a small data input (e.g., a text, a command) and act within a larger context (e.g., chat history, documents, state). This data will move through different services (LLM, vector database, document store, etc.) to perform operations, generate new data artifacts, and return final results. Most use cases repeat this flow multiple times while iterating over different inputs."}),"\n",(0,i.jsx)(n.p,{children:"Some common operations include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Convert text to embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Store / search / retrieve embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Find nearest neighbors to an embedding"}),"\n",(0,i.jsx)(n.li,{children:"Retrieve text for an embedding"}),"\n",(0,i.jsx)(n.li,{children:"Determine context required to pass into a prompt"}),"\n",(0,i.jsx)(n.li,{children:"Prompt models with context from relevant text"}),"\n",(0,i.jsx)(n.li,{children:"Send results to another service (API, database, etc.)"}),"\n",(0,i.jsx)(n.li,{children:"\u2026"}),"\n",(0,i.jsx)(n.li,{children:"and chaining them together!"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Now, let\u2019s think about the above in a production context, and imagine a user is unsatisfied with the outputs of your application and you want to find the root cause of the issue. Your application logged both the prompt and the results. Your code allows you to figure out the sequence of operations. Yet, you have no clue where things went wrong and the system produced undesirable output\u2026 To mitigate this, we\u2019d argue it\u2019s critical then to have lineage of data artifacts and the code that produces them, so you can debug situations such as these quickly."}),"\n",(0,i.jsxs)(n.p,{children:["To add to the complexity of your LLM application dataflow, many operations are non-deterministic, meaning you can\u2019t rerun or reverse engineer the operation to reproduce intermediate results. For example, an API call to generate a text or image response will likely be non-reproducible even if you have access to the same input and configuration (you can mitigate some of this with options such as ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/chat/create#chat/create-temperature",children:"temperature"}),"). This also extends to certain vector database operations like \u201cfind nearest\u201d where the result depends on the objects currently stored in the database. In production settings, it is prohibitive to near impossible to snapshot DB states to make calls reproducible."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"For these reasons, it is important to adopt flexible tooling to create robust dataflows that allow you to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"plugin in various components easily."}),"\n",(0,i.jsx)(n.li,{children:"see how components connect to each other."}),"\n",(0,i.jsx)(n.li,{children:"add and customize common production needs like caching, validation, and observability."}),"\n",(0,i.jsx)(n.li,{children:"adjust the flow structure to your needs without requiring a strong engineering skill set"}),"\n",(0,i.jsx)(n.li,{children:"plug into the traditional data processing and machine learning ecosystem."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["In this post we\u2019ll give an overview of how Hamilton fulfills points 1, 2, & 4. We refer the user to our ",(0,i.jsx)(n.a,{href:"https://hamilton.dagworks.io/en/latest/",children:"documentation"})," for points 3 & 5."]}),"\n",(0,i.jsx)(n.h2,{id:"current-llm-application-development-tooling",children:"Current LLM application development tooling"}),"\n",(0,i.jsx)(n.p,{children:"The LLM space is still in its infancy, and the usage patterns and tooling are rapidly evolving. While LLM frameworks can get you started, current options are not production tested; to our knowledge, no established tech companies are using the current popular LLM frameworks in production."}),"\n",(0,i.jsx)(n.p,{children:"Don\u2019t get us wrong, some of the tooling out there is great for getting a quick proof of concept up and running! However, we feel they fall short in two specific areas:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"How to model the LLM application\u2019s dataflow"}),". We strongly believe that the dataflow of \u201cactions\u201d is better modeled as functions, rather than through object oriented classes and lifecycles. Functions are much simpler to reason about, test, and change. Object oriented classes can become quite opaque and impose more mental burden."]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsx)(n.p,{children:"When something errors, object-oriented frameworks require you to drill into the objects\u2019 source code to understand it. Whereas with Hamilton functions, a clear dependency lineage tells you where to look and helps you reason about what happened (more on this below)!"}),"\n"]}),"\n",(0,i.jsxs)(n.ol,{start:"2",children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Customization/extensions"}),". Unfortunately you need a strong software engineering skill set to modify the current frameworks once you step outside the bounds of what they make \u201ceasy\u201d to do. If that\u2019s not an option, this means you can end up stepping outside the framework for a particular piece of custom business logic, which can inadvertently lead you to maintaining more code surface area than if you didn\u2019t use the framework in the first place."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["For more on these two points we point you to threads such as these two (",(0,i.jsx)(n.a,{href:"https://news.ycombinator.com/item?id=36645575#36647985",children:"HackerNews"}),", ",(0,i.jsx)(n.a,{href:"https://old.reddit.com/r/LangChain/comments/13fcw36/langchain_is_pointless/",children:"Reddit"}),") that have users speak in more detail."]}),"\n",(0,i.jsx)(n.p,{children:"While Hamilton is not a complete replacement for current LLM frameworks (e.g. there is no \u201cagent\u201d component), it does have all the building blocks to meet your LLM application needs and both can work in conjunction. If you want a clean, clear, and customizable way to write production code, integrate several LLM stack components, and gain observability over your app, then let\u2019s move onto the next few sections!"}),"\n",(0,i.jsx)(n.h2,{id:"building-with-hamilton",children:"Building with Hamilton"}),"\n",(0,i.jsxs)(n.p,{children:["Hamilton is a declarative micro-framework to describe ",(0,i.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Dataflow",children:"dataflows"})," in Python. It\u2019s not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows. Its strength is expressing the flow of data & computation in a way that is straightforward to create and maintain (much like DBT does for SQL), which lends itself very well to support modeling the data & computational needs of LLM applications."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Alt text",src:t(7546).Z+"",width:"1037",height:"682"})}),"\n",(0,i.jsx)(n.p,{children:"The basics of Hamilton are simple, and it can be extended in quite a few ways; you don't have to know Hamilton to get value out of this post, but if you're interested, check out:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.tryhamilton.dev/",children:"tryhamilton.dev"})," \u2013 an interactive tutorial in your browser!"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://towardsdatascience.com/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5",children:"Pandas data transformations in Hamilton in 5 minutes"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6",children:"Lineage + Hamilton in 10 minutes"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/publish/post/130538397",children:"Hamilton + Airflow for production"})}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"onto-our-example",children:"Onto our example"}),"\n",(0,i.jsx)(n.p,{children:"To help set some mental context, picture this. You\u2019re a small data team that is tasked with creating an LLM application to \u201cchat\u201d with your organization\u2019s document. You believe it\u2019s important to evaluate candidate architectures in terms of features, performance profile, licenses, infrastructure requirements, and costs. Ultimately, you know your organization\u2019s primary concern is providing the most relevant results and a great user experience. The best way to assess this is to build a prototype, test different stacks and compare their characteristics and outputs. Then when you transition to production, you\u2019ll want confidence that the system can be maintained and introspected easily, to consistently provide a great user experience."}),"\n",(0,i.jsx)(n.p,{children:"With that in mind, in this example, we will implement part of an LLM application, specifically the data ingestion step to index a knowledge base, where we convert text to embeddings and store them in a vector database. We implement this in a modular with a few different services/technologies. The broad steps are:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Load the ",(0,i.jsx)(n.a,{href:"https://huggingface.co/datasets/squad",children:"SQuAD dataset"})," from the HuggingFace Hub. You would swap this out for your corpus of preprocessed documents."]}),"\n",(0,i.jsxs)(n.li,{children:["Embed text entries using the ",(0,i.jsx)(n.a,{href:"https://docs.cohere.com/reference/embed",children:"Cohere API"}),", the ",(0,i.jsx)(n.a,{href:"https://platform.openai.com/docs/api-reference/embeddings",children:"OpenAI API"}),", or the ",(0,i.jsx)(n.a,{href:"https://www.sbert.net/examples/applications/computing-embeddings/README.html",children:"SentenceTransformer library"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["Store embeddings in a vector database, either ",(0,i.jsx)(n.a,{href:"https://lancedb.github.io/lancedb/",children:"LanceDB"}),", ",(0,i.jsx)(n.a,{href:"https://docs.pinecone.io/docs/overview",children:"Pinecone"}),", or ",(0,i.jsx)(n.a,{href:"https://weaviate.io/developers/weaviate",children:"Weaviate"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"If you need to know more about embeddings & search, we direct readers to the following links:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://weaviate.io/blog/vector-embeddings-explained",children:"Text embeddings explained - Weaviate"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://docs.pinecone.io/docs/semantic-text-search",children:"How-to conduct semantic search with Pinecone"})}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:"As we\u2019re walking through this example, it would be useful for you to think about/keep in mind the following:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Compare what we show you with what you\u2019re doing now"}),". See how Hamilton enables you to curate and structure a project without needing an explicit LLM-centric framework."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Project and application structure"}),". Understand how Hamilton enforces a structure that enables you to build and maintain a modular stack."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Confidence in iteration & project longevity"}),". Combining the above two points, Hamilton enables you to more easily maintain an LLM application in production, no matter who authored it."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s start with a visualization to give you an overview of what we\u2019re talking about:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Alt text",src:t(2487).Z+"",width:"1600",height:"723"})}),"\n",(0,i.jsxs)(n.p,{children:["Here\u2019s what the LLM Application dataflow would look like when using pinecone with sentence transformers. With Hamilton to understand how things connect is just as simple as ",(0,i.jsx)(n.code,{children:"display_all_functions()"})," call on the ",(0,i.jsx)(n.a,{href:"https://hamilton.dagworks.io/en/latest/reference/drivers/Driver/#hamilton.driver.Driver.display_all_functions",children:"Hamilton driver object"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"modular-code",children:"Modular Code"}),"\n",(0,i.jsx)(n.p,{children:"Let\u2019s explain the two main ways to implement modular code with Hamilton using our example for context."}),"\n",(0,i.jsx)(n.h3,{id:"configwhen",children:"@config.when"}),"\n",(0,i.jsxs)(n.p,{children:["Hamilton\u2019s focus is on readability. Without explaining what ",(0,i.jsx)(n.code,{children:"@config.when"})," does, you can probably tell that this is a conditional statement, and only included when the predicate is satisfied. Below you will find the implementation for converting text to embeddings with the OpenAI and the Cohere API."]}),"\n",(0,i.jsxs)(n.p,{children:["Hamilton will recognize two functions as alternative implementations because of the ",(0,i.jsx)(n.code,{children:"@config.when"})," decorator and the same function name ",(0,i.jsx)(n.code,{children:"embeddings"})," preceding the double underscore (",(0,i.jsx)(n.code,{children:"__cohere"}),", ",(0,i.jsx)(n.code,{children:"__openai"}),"). Their function signatures need not be entirely the same, which means it's easy and clear to adopt different implementations."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="Modularity via @config.when"',children:'# functions from the same file; embedding_module.py\n\n@config.when(embedding_service="openai")\ndef embeddings__openai(\n    embedding_provider: ModuleType,\n    text_contents: list[str],\n    model_name: str = "text-embedding-ada-002",\n) -> list[np.ndarray]:\n    """Convert text to vector representations (embeddings) using OpenAI Embeddings API\n    reference: https://github.com/openai/openai-cookbook/blob/main/examples/Get_embeddings.ipynb\n    """\n    response = embedding_provider.Embedding.create(input=text_contents, engine=model_name)\n    return [np.asarray(obj["embedding"]) for obj in response["data"]]\n\n\n@config.when(embedding_service="cohere")\ndef embeddings__cohere(\n    embedding_provider: cohere.Client,\n    text_contents: list[str],\n    model_name: str = "embed-english-light-v2.0",\n) -> list[np.ndarray]:\n    """Convert text to vector representations (embeddings) using Cohere Embed API\n    reference: https://docs.cohere.com/reference/embed\n    """\n    response = embedding_provider.embed(\n        texts=text_contents,\n        model=model_name,\n        truncate="END",\n    )\n    return [np.asarray(embedding) for embedding in response.embeddings]\n'})}),"\n",(0,i.jsxs)(n.p,{children:["For this project, it made sense to have all embedding services implemented in the same file with the ",(0,i.jsx)(n.code,{children:"@config.when"})," decorator since there are only 3 functions per service. However, as the project grows in complexity, functions could be moved to separate modules too, and the next section\u2019s modularity pattern employed instead. Another point to note is that each of these functions is independently unit-testable. Should you have specific needs, it\u2019s straightforward to encapsulate it in the function and test it."]}),"\n",(0,i.jsx)(n.h3,{id:"switching-out-python-modules",children:"Switching out python modules"}),"\n",(0,i.jsxs)(n.p,{children:["Below you will find the implementation of vector database operations for Pinecone and Weaviate. Note that the snippets are from ",(0,i.jsx)(n.code,{children:"pinecone_module.py"})," and ",(0,i.jsx)(n.code,{children:"weaviate_module.py"})," and notice how function signatures resemble and differ."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="Modularity via Python modules',children:'# functions from pinecone_module.py\nfrom types import ModuleType\n\nimport numpy as np\nimport pinecone\n\ndef client_vector_db(vector_db_config: dict) -> ModuleType:\n    """Instantiate Pinecone client using Environment and API key"""\n    pinecone.init(**vector_db_config)\n    return pinecone\n  \ndef data_objects(\n    ids: list[str], titles: list[str], embeddings: list[np.ndarray], metadata: dict\n) -> list[tuple]:\n    """Create valid pinecone objects (index, vector, metadata) tuples for upsert"""\n    assert len(ids) == len(titles) == len(embeddings)\n    properties = [dict(title=title, **metadata) for title in titles]\n    embeddings = [x.tolist() for x in embeddings]\n    return list(zip(ids, embeddings, properties))\n\ndef push_to_vector_db(\n    client_vector_db: ModuleType,\n    index_name: str,\n    data_objects: list[tuple],\n    batch_size: int = 100,\n) -> int:\n    """Upsert objects to Pinecone index; return the number of objects inserted"""\n    pinecone_index = pinecone.Index(index_name)\n\n    for i in range(0, len(data_objects), batch_size):\n        i_end = min(i + batch_size, len(data_objects))\n\n        pinecone_index.upsert(vectors=data_objects[i:i_end])\n    return len(data_objects)\n  \n\n# from weaviate_module.py\nimport numpy as np\nimport weaviate\n\ndef client_vector_db(vector_db_config: dict) -> weaviate.Client:\n    """Instantiate Weaviate client using Environment and API key"""\n    client = weaviate.Client(**vector_db_config)\n    if client.is_live() and client.is_ready():\n        return client\n    else:\n        raise ConnectionError("Error creating Weaviate client")\n    \ndef data_objects(\n    ids: list[str], titles: list[str], text_contents: list[str], metadata: dict\n) -> list[dict]:\n    """Create valid weaviate objects that match the defined schema"""\n    assert len(ids) == len(titles) == len(text_contents)\n    return [\n        dict(squad_id=id_, title=title, context=context, **metadata)\n        for id_, title, context in zip(ids, titles, text_contents)\n    ]\n\ndef push_to_vector_db(\n    client_vector_db: weaviate.Client,\n    class_name: str,\n    data_objects: list[dict],\n    embeddings: list[np.ndarray],\n    batch_size: int = 100,\n) -> int:\n    """Push batch of data objects with their respective embedding to Weaviate.\n    Return number of objects.\n    """\n    assert len(data_objects) == len(embeddings)\n    with client_vector_db.batch(batch_size=batch_size, dynamic=True) as batch:\n        for i in range(len(data_objects)):\n            batch.add_data_object(\n                data_object=data_objects[i], class_name=class_name, vector=embeddings[i]\n            )\n    return len(data_objects)\n'})}),"\n",(0,i.jsx)(n.p,{children:"With Hamilton, the dataflow is stitched together using function names and function input arguments. Therefore by sharing function names for similar operations, the two modules are easily interchangeable. Since the LanceDB, Pinecone, and Weaviate implementations reside in separate modules, it reduces the number of dependencies per file and makes them shorter, improving both readability and maintainability. The logic for each implementation is clearly encapsulated in these named functions, so unit testing is straightforward to implement for each respective module. The separate modules reinforce the idea that they shouldn\u2019t be loaded simultaneously. The Hamilton driver will actually throw an error when multiple functions with the same name are found that helps enforce this concept."}),"\n",(0,i.jsx)(n.h2,{id:"driver-implications",children:"Driver implications"}),"\n",(0,i.jsxs)(n.p,{children:["The key part for running Hamilton code is the ",(0,i.jsx)(n.code,{children:"Driver"})," object found in ",(0,i.jsx)(n.code,{children:"run.py"}),". Excluding the code for the CLI and some argument parsing, we get:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="Hamilton Driver"',children:'config = dict(\n    vector_db_config=json.loads(vector_db_config),\n    embedding_service=embedding_service,  # this triggers config.when() in embedding_module\n    api_key=embedding_service_api_key,\n    model_name=model_name,\n)\n\ndr = driver.Driver(\n    config,\n    data_module,\n    vector_db_module,  # pinecone_module, weaviate_module or lancedb_module\n    embedding_module,  # contains cohere, openai, and sentence_transformer implementations\n    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n)\n\ndr.execute(\n    final_vars=["initialize_vector_db_indices", "push_to_vector_db"],\n    inputs=dict(\n        class_name="SQuADEntry",\n    ),\n)\n'})}),"\n",(0,i.jsx)(n.p,{children:"The Hamilton Driver, which orchestrates execution and is what you manipulate your dataflow through, allows modularity through three mechanisms as seen in the above code snippet:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Driver configuration"}),". this is a dictionary the driver receives at instantiation containing information that should remain constant, such as which API to use, or the embedding service API key. This integrates well with a command plane that can pass JSON or strings (e.g., a Docker container, ",(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/publish/posts/detail/130538397",children:"Airflow"}),", ",(0,i.jsx)(n.a,{href:"https://outerbounds.com/blog/developing-scalable-feature-engineering-dags/",children:"Metaflow"}),", etc.). Concretely this is where we\u2019d specify swapping out what embedding API to use."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Driver modules"}),". the driver can receive an arbitrary number of independent Python modules to build the dataflow from. Here, the ",(0,i.jsx)(n.code,{children:"vector_db_module"})," can be swapped in for the desired vector database implementation we\u2019re connecting to. One can also import modules dynamically through ",(0,i.jsx)(n.a,{href:"https://docs.python.org/3/library/importlib.html#importlib.import_module",children:"importlib"}),", which can be useful for development vs production contexts, and also enable a configuration driven way to changing the dataflow implementation"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Driver execution"}),". The ",(0,i.jsx)(n.code,{children:"final_vars"})," parameter determines what output should be returned. You do not need to restructure your code to change what output you want to get. Let\u2019s take the case of wanting to debug something within our dataflow, it is possible to request the output of any function by adding its name to ",(0,i.jsx)(n.code,{children:"final_vars"}),". For example, if you have some intermediate output to debug, it\u2019s easy to request it, or stop execution at that spot entirely. Note, the driver can receive inputs and overrides values when calling ",(0,i.jsx)(n.code,{children:"execute()"}),"; in the code above, the ",(0,i.jsx)(n.code,{children:"class_name"})," is an execution time input that indicates the embedding object we want to create and where to store it in our vector database."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"modularity-summary",children:"Modularity Summary"}),"\n",(0,i.jsx)(n.p,{children:"In Hamilton, the key to enable swappable components is to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"define functions with effectively the same name and then,"}),"\n",(0,i.jsxs)(n.li,{children:["annotate them with ",(0,i.jsx)(n.code,{children:"@config.when"})," and choose which one to use via configuration passed to the driver, or,"]}),"\n",(0,i.jsx)(n.li,{children:"put them in separate python modules and pass in the desired module to the driver."}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"So we\u2019ve just shown how you can plugin, swap, and call various LLM components with Hamilton. We didn\u2019t need to explain what an object oriented hierarchy is, nor require you to have extensive software engineering experience to follow (we hope!). To accomplish this, we just had to match function names, and their output types. We think this way of writing and modularizing code is therefore more accessible than current LLM frameworks permit."}),"\n",(0,i.jsx)(n.h2,{id:"hamilton-code-in-practice",children:"Hamilton code in practice"}),"\n",(0,i.jsx)(n.p,{children:"To add to our claims, here a few practical implications of writing Hamilton code for LLM workflows that we\u2019ve observed:"}),"\n",(0,i.jsx)(n.h3,{id:"cicd",children:"CI/CD"}),"\n",(0,i.jsxs)(n.p,{children:["This ability to swap out modules/",(0,i.jsx)(n.code,{children:"@config.when"})," also means that integration testing in a CI system is straightforward to think about, since you have the flexibility and freedom to swap/isolate parts of the dataflow as desired."]}),"\n",(0,i.jsx)(n.h3,{id:"collaboration",children:"Collaboration"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["The modularity Hamilton enables can allow one to mirror cross team boundaries easily. The function names & their output types become a contract, which ensures one can make surgical changes and be confident in the change, as well as have the visibility into downstream dependencies with Hamilton\u2019s ",(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6",children:"visualization and lineage features"})," (like the initial visualization we saw). For example, it\u2019s clear how to interact and consume from the vector database."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Code changes are simpler to review, because the flow is defined by declarative functions. The changes are self-contained; because there is no object oriented hierarchy to learn, just a function to modify.  Anything \u201ccustom\u201d is de facto supported by Hamilton."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"debugging",children:"Debugging"}),"\n",(0,i.jsx)(n.p,{children:"When there is an error with Hamilton, it\u2019s clear as to what the code it maps to is, and because of how the function is defined, one knows where to place it within the dataflow."}),"\n",(0,i.jsx)(n.p,{children:"Take the simple example of the embeddings function using cohere. If there was a time out, or error in parsing the response it would be clear that it maps to this code, and from the function definition you\u2019d know where in the flow it fits."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",metastring:'title="Cohere embedding function"',children:'@config.when(embedding_service="cohere")\ndef embeddings__cohere(\n    embedding_provider: cohere.Client,\n    text_contents: list[str],\n    model_name: str = "embed-english-light-v2.0",\n) -> list[np.ndarray]:\n    """Convert text to vector representations (embeddings) using Cohere Embed API\n    reference: https://docs.cohere.com/reference/embed\n    """\n    response = embedding_provider.embed(\n        texts=text_contents,\n        model=model_name,\n        truncate="END",\n    )\n    return [np.asarray(embedding) for embedding in response.embeddings]\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Alt text",src:t(4588).Z+"",width:"1425",height:"443"})}),"\n",(0,i.jsx)(n.h2,{id:"tips-for-creating-a-modular-llm-stack",children:"Tips for creating a modular LLM stack"}),"\n",(0,i.jsx)(n.p,{children:"Before we finish, here are some ideas to guide you through building your application. Some decisions might not have an obvious best choice, but having the right approach to modularity will allow you to efficiently iterate as requirements evolve."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Before writing any code, draw a DAG of the logical steps of your workflow. This sets the basis for defining common steps and interfaces that are not service-specific."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Identify steps that could be swapped. By being purposeful with configuration points, you will reduce risks of ",(0,i.jsx)(n.a,{href:"https://refactoring.guru/smells/speculative-generality",children:"speculative generality"}),". Concretely, this would result in functions with less arguments, default values, and grouped into thematic modules."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Chunk parts of your dataflow into modules with few dependencies, if relevant. This will lead to shorter Python files with fewer package dependencies, improved readability and maintainability. Hamilton is indifferent and can build its DAG from multiple modules."}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"to-close--future-directions",children:"To close & future directions"}),"\n",(0,i.jsx)(n.p,{children:"Thanks for getting this far. We believe that Hamilton has a part to play in helping everyone express their dataflows, and LLM applications are just one use case! To summarize our messaging in this post can be boiled down to:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"It is useful to conceive of LLM applications as dataflows, and are therefore a great fit for using Hamilton."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Object-centric LLM frameworks can be opaque and hard to extend and maintain for your production needs. Instead, one should write their own integrations with Hamilton\u2019s straightforward declarative style. Doing so will improve your code\u2019s transparency and maintainability, with clear testable functions, clear mapping of runtime errors to functions, and built-in visualization of your dataflow."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"The modularity prescribed by using Hamilton will make collaboration more efficient and provide you with the requisite flexibility to modify and change your LLM workflows at the speed at which the field is moving."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["We now invite you to play around with, try, and modify the full example for yourselves ",(0,i.jsx)(n.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/modular_llm_stack",children:"here"}),". There is a ",(0,i.jsx)(n.code,{children:"README"})," that will explain the commands to run and get started. Otherwise, we are working on making the Hamilton + LLM Application experience even better by thinking about the following:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Agents"}),". Can we provide the same level of visibility to agents that we have for regular Hamilton dataflows?"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Parallelization"}),". How can we make it simpler to express running a dataflow over a list of documents for example. See this work in progress PR for what we mean."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Plugins for caching and observability"}),". One can already implement a custom caching and observability solution on top of Hamilton. We\u2019re working on providing more standard options out of the box for common components, e.g. redis."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"A user contributed dataflows section"}),". We see the possibility to standardize on common names for specific LLM application use cases. In which case we can start to aggregate Hamilton dataflows, and allow people to pull them down for their needs."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"we-want-to-hear-from-you",children:"We want to hear from you!"}),"\n",(0,i.jsx)(n.p,{children:"If you\u2019re excited by any of this, or have strong opinions, drop by our Slack channel / or leave some comments here! Some resources to get you help:"}),"\n",(0,i.jsxs)(n.p,{children:["\ud83d\udce3 join our community on ",(0,i.jsx)(n.a,{href:"https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email",children:"Slack"})," \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started."]}),"\n",(0,i.jsxs)(n.p,{children:["\u2b50\ufe0f us on ",(0,i.jsx)(n.a,{href:"https://github.com/DAGWorks-Inc/hamilton",children:"GitHub"})]}),"\n",(0,i.jsxs)(n.p,{children:["\ud83d\udcdd leave us an ",(0,i.jsx)(n.a,{href:"https://github.com/DAGWorks-Inc/hamilton/issues",children:"issue"})," if you find something"]}),"\n",(0,i.jsx)(n.p,{children:"Other Hamilton posts you might be interested in:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.tryhamilton.dev/",children:"tryhamilton.dev"})," \u2013 an interactive tutorial in your browser!"]}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5",children:"Pandas data transformations in Hamilton in 5 minutes"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6",children:"Lineage + Hamilton in 10 minutes"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://blog.dagworks.io/publish/post/130538397",children:"Hamilton + Airflow for production"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},7546:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/image-1-df3d92b60d706042afb8d4cece97ece8.png"},2487:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/image-2-4cf0f1fdaafb75366b56c1a6a199f5be.png"},4588:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/image-3-890f3b0a5cad95229ee3c5cd2e25d985.png"},82:(e,n,t)=>{t.d(n,{Z:()=>i});const i=t.p+"assets/images/image-ad890e07098815db841b7fb326a6b612.png"},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var i=t(7294);const o={},a=i.createContext(o);function s(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);