"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[8206],{9410:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var s=t(5893),o=t(1151);const i={title:"Voice Assistant",description:"Launch arbitrary Python functions using your microphone"},a=void 0,r={id:"personal/voice-assistant",title:"Voice Assistant",description:"Launch arbitrary Python functions using your microphone",source:"@site/docs/personal/voice-assistant.md",sourceDirName:"personal",slug:"/personal/voice-assistant",permalink:"/personal_website/docs/personal/voice-assistant",draft:!1,unlisted:!1,tags:[],version:"current",frontMatter:{title:"Voice Assistant",description:"Launch arbitrary Python functions using your microphone"},sidebar:"tutorialSidebar",previous:{title:"Semantic Blog Search",permalink:"/personal_website/docs/personal/semantic-blog-search"},next:{title:"Contributions",permalink:"/personal_website/docs/category/contributions"}},c={},l=[{value:"Design choices",id:"design-choices",level:2},{value:"Languages, Technology, and Tools",id:"languages-technology-and-tools",level:2},{value:"Snippet",id:"snippet",level:2}];function d(e){const n={blockquote:"blockquote",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"This project leverages the multilingual speech-to-text model Whisper from OpenAI. The model is loaded on a websocket server. The connected client sends an audio input stream and receives text transcriptions. If the predefined keyword is found, the following words are parsed to find a registered command using fuzzy matching. If there's a match, the command is executed on the client device."}),"\n",(0,s.jsx)(n.p,{children:"A command is an arbitrary Python function. The name of function (separated by underscores _) is what needs to be uttered to trigger the command. Words spoken after the command name are parsed as function arguments. Additional commands can be added to Python modules."}),"\n",(0,s.jsxs)(n.blockquote,{children:["\n",(0,s.jsx)(n.p,{children:"\ud83d\udcd1 NOTE. This project is pre-ChatGPT."}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"design-choices",children:"Design choices"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Client-server architecture"}),". Allows a client with limited resources to leverage speech-to-text (smartphone, remote, dev board). Clients can share a server, and a client could potentially trigger commands on another client."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Low-latency requirements"}),". Chunk the audio stream into reasonable size to transcribe with good accuracy and reduce latency. Use async, websockets, and threads to have non-blocking operations on the client while the server transcribes audio."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Structured commands"}),". The opiniated structure removes the problem of natural language understanding, which allows for faster development of new commands"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Modular commands"}),". The modular organization of commands allows to ship a core app without bloating, and to make user-created commands easy to share"]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Few dependencies"}),". thefuzz is a quality of life and could be reimplemented; the SpeechRecognition is a bit messy, but has nice features to ignore ambient noise"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"languages-technology-and-tools",children:"Languages, Technology, and Tools"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Python"}),"\n",(0,s.jsx)(n.li,{children:"OpenAI Whisper (local model)"}),"\n",(0,s.jsx)(n.li,{children:"NVIDIA CUDA"}),"\n",(0,s.jsx)(n.li,{children:"Async"}),"\n",(0,s.jsx)(n.li,{children:"Websockets"}),"\n",(0,s.jsx)(n.li,{children:"Threads"}),"\n",(0,s.jsx)(n.li,{children:"Fuzzy matching"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"snippet",children:"Snippet"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",metastring:'title="Server snippet"',children:'import asyncio\nimport base64\nimport json\nimport websockets\n\nimport numpy as np\nimport whisper\n\n\nMODEL_NAME = "small.en"\nLANGUAGE = "english"\n\n\nasync def server_handler(websocket):\n    model = whisper.load_model(MODEL_NAME)\n\n    async for message in websocket:\n        event = json.loads(message)\n        assert event["type"] == "audio_input"\n\n        try:\n            audio_bytes = base64.b64decode(event["data"].encode("ascii"))\n            data = np.frombuffer(audio_bytes, np.int16) \\\n                     .flatten() \\\n                     .astype(np.float32) / 32768\n            result = model.transcribe(data, language=LANGUAGE)\n        except Exception as e:\n            event = {"type": "error", "data": str(e)}\n            await websocket.send(json.dumps(event))\n            continue\n\n        event = {"type": "transcript", "data": result["text"]}\n        await websocket.send(json.dumps(event))\n\n\nasync def main():\n    async with websockets.serve(server_handler, "localhost", 8001):\n        await asyncio.Future()\n\n\nif __name__ == "__main__":\n    asyncio.run(main())\n'})})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},1151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>a});var s=t(7294);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);