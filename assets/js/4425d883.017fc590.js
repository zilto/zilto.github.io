"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[9320],{8460:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>d});var a=n(5893),i=n(1151);const s={slug:"rag",title:"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!",authors:"tj",tags:["Hamilton","Retrieval augmented generation","OpenAI","LLM","FastAPI","Docker"]},o=void 0,r={permalink:"/blog/rag",source:"@site/blog/2023-09-08-rag/index.md",title:"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!",description:"Off-the-shelf LLMs are excellent at manipulating and generating text, but they only know general facts about the world and probably very little about your use case. Retrieval augmented generation (RAG) refers not to a single algorithm, but rather a broad approach to provide relevant context to an LLM. As industry applications mature, RAG strategies will be tailored case-by-case to optimize relevance, business outcomes, and operational concerns.",date:"2023-09-08T00:00:00.000Z",formattedDate:"September 8, 2023",tags:[{label:"Hamilton",permalink:"/blog/tags/hamilton"},{label:"Retrieval augmented generation",permalink:"/blog/tags/retrieval-augmented-generation"},{label:"OpenAI",permalink:"/blog/tags/open-ai"},{label:"LLM",permalink:"/blog/tags/llm"},{label:"FastAPI",permalink:"/blog/tags/fast-api"},{label:"Docker",permalink:"/blog/tags/docker"}],readingTime:18.74,hasTruncateMarker:!0,authors:[{name:"Thierry Jean",url:"https://github.com/zilto",imageURL:"https://github.com/zilto.png",key:"tj"}],frontMatter:{slug:"rag",title:"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!",authors:"tj",tags:["Hamilton","Retrieval augmented generation","OpenAI","LLM","FastAPI","Docker"]},unlisted:!1,nextItem:{title:"Containerized PDF Summarizer with FastAPI and Hamilton",permalink:"/blog/pdf-summarizer"}},l={authorsImageUrls:[void 0]},d=[{value:"What is RAG and why do you need it?",id:"what-is-rag-and-why-do-you-need-it",level:2},{value:"Introducing Hamilton",id:"introducing-hamilton",level:2},{value:"Building a modular RAG application",id:"building-a-modular-rag-application",level:2},{value:"Hamilton dataflows a.k.a. \u201cchains\u201d",id:"hamilton-dataflows-aka-chains",level:2},{value:"Ingestion flow",id:"ingestion-flow",level:3},{value:"Retrieval flow",id:"retrieval-flow",level:3},{value:"Weaviate vector database",id:"weaviate-vector-database",level:3},{value:"FastAPI server",id:"fastapi-server",level:3},{value:"Streamlit frontend",id:"streamlit-frontend",level:3},{value:"Docker services",id:"docker-services",level:3},{value:"Limitations",id:"limitations",level:2},{value:"Summary",id:"summary",level:2},{value:"We want to hear from you!",id:"we-want-to-hear-from-you",level:2}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",em:"em",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(t.p,{children:"Off-the-shelf LLMs are excellent at manipulating and generating text, but they only know general facts about the world and probably very little about your use case. Retrieval augmented generation (RAG) refers not to a single algorithm, but rather a broad approach to provide relevant context to an LLM. As industry applications mature, RAG strategies will be tailored case-by-case to optimize relevance, business outcomes, and operational concerns."}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Alt text",src:n(9480).Z+"",width:"1456",height:"1115"})}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["crosspost from ",(0,a.jsx)(t.a,{href:"https://blog.dagworks.io/p/retrieval-augmented-generation-reference-arch",children:"https://blog.dagworks.io/p/retrieval-augmented-generation-reference-arch"})]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"In this post, we provide a reference RAG architecture and discuss design decisions for each component. It\u2019s ready for use, and will scale with your needs. Specifically, we\u2019ll cover how to:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["Write ingestion and retrieval dataflows using ",(0,a.jsx)(t.a,{href:"https://hamilton.dagworks.io/en/latest/",children:"Hamilton"})]}),"\n",(0,a.jsxs)(t.li,{children:["Build a backend with ",(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," + Hamilton backend"]}),"\n",(0,a.jsxs)(t.li,{children:["Create a browser interface with ",(0,a.jsx)(t.a,{href:"https://docs.streamlit.io/",children:"Streamlit"})]}),"\n",(0,a.jsxs)(t.li,{children:["Compute embeddings and generate text with the ",(0,a.jsx)(t.a,{href:"https://platform.openai.com/docs/api-reference/introduction",children:"OpenAI API"})]}),"\n",(0,a.jsxs)(t.li,{children:["Use and manage a ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate",children:"Weaviate"})," vector store"]}),"\n",(0,a.jsxs)(t.li,{children:["Build a containerized app using ",(0,a.jsx)(t.a,{href:"https://docs.docker.com/reference/",children:"Docker"})]}),"\n"]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["Find the code on ",(0,a.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/retrieval_augmented_generation",children:"GitHub"}),"\nThis publication extends our previous ",(0,a.jsx)(t.a,{href:"https://blog.dagworks.io/p/containerized-pdf-summarizer-with",children:"PDF Summarizer"})]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"what-is-rag-and-why-do-you-need-it",children:"What is RAG and why do you need it?"}),"\n",(0,a.jsxs)(t.p,{children:["Large language models (LLMs) learn to write coherent sentences by being exposed to enormous corpora of texts. In the process, the model stores facts about the world. However, determining what it knows or what it doesn\u2019t is still a key theoretical challenge. The above generally describes ",(0,a.jsx)(t.em,{children:"pre-trained"}),", ",(0,a.jsx)(t.em,{children:"foundational"})," or ",(0,a.jsx)(t.em,{children:"base"})," models, which are models not yet refined for a particular use-case (see model card of ",(0,a.jsx)(t.a,{href:"https://huggingface.co/bert-base-uncased",children:"bert-base-uncased"})," for details). For instance, GPT stands for ",(0,a.jsx)(t.em,{children:"generative pre-trained transformer"}),", and ChatGPT is a fine-tuned version for chat applications."]}),"\n",(0,a.jsx)(t.p,{children:"A primary concern for LLM applications, outside of creative work, is the factual correctness of answers. This challenge can be mitigated by adopting one or many of the following techniques:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Fine-tuning"})," consists of further training a pre-trained model on curated examples for a specific task. The model learns domain-specific language and facts*, which improves the quality of embeddings and text generation relative to the domain (e.g., insurance, health)."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Instruction-tuning"})," is a form of fine-tuning that uses instruction-answer pairs. The model learns ",(0,a.jsx)(t.em,{children:"how"})," to respond to queries (summarize, make a list, think step by step, etc.). Models labeled as ",(0,a.jsx)(t.em,{children:"chat"})," (e.g., ",(0,a.jsx)(t.a,{href:"https://huggingface.co/meta-llama/Llama-2-7b-chat",children:"llama-2-7b-chat"}),") are great for general human interaction, and can be further fine-tuned to your domain."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Retrieval augmented generation (RAG)"})," is a multistep process to retrieve information relevant for a query, and pass it to the LLM as context to generate an answer. RAG is the most flexible approach for adding and updating knowledge since it only requires to change the available sources (e.g., files, internet pages) rather than updating the LLM."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Looking forward, retrieval will continue to be a key architectural component of LLM applications because editing LLM knowledge directly is an unsolved problem. Fine-tuning, in most cases, should be a later concern since it would also improve your RAG strategy if you have one in place. Furthermore, the LLM is the most likely component to be improved upon and replaced, requiring fine-tuning again. For these reasons, we suggest starting with an off-the-shelf LLM and implementing your own RAG system as a first step to improve the knowledge your LLM operates over."}),"\n",(0,a.jsx)(t.h2,{id:"introducing-hamilton",children:"Introducing Hamilton"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://hamilton.dagworks.io/en/latest/",children:"Hamilton"})," is a declarative micro-framework to describe ",(0,a.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Dataflow",children:"dataflows"})," in Python. Its strength is expressing the flow of data and computation in a straightforward and easy to maintain manner (much like dbt does for SQL). It has minimal dependencies and can run anywhere Python runs, meaning the same code will work in development notebooks, scripts, Spark clusters, or production web-services. Hamilton is not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows; and it extends nicely to modeling LLM workflows!"]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Alt text",src:n(3533).Z+"",width:"1037",height:"682"})}),"\n",(0,a.jsx)(t.p,{children:"The picture above encapsulates the function-centric declarative approach of Hamilton. The function\u2019s name is tied to its outputs and its arguments define what data it depends on. This allows Hamilton to read functions found in a module and automatically generate the DAG to be executed. This paradigm incentivizes developers to write small modular functions instead of scripts or larger functions, without sacrificing iteration speed."}),"\n",(0,a.jsx)(t.p,{children:"As a result, it is easier to:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:"Write and maintain custom application logic"}),"\n",(0,a.jsx)(t.li,{children:"View operation lineage and debug results"}),"\n",(0,a.jsx)(t.li,{children:"Update components of your stack"}),"\n",(0,a.jsx)(t.li,{children:"Reuse function implementations across contexts (e.g., notebook, pipeline, web service)"}),"\n"]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["If you are new to Hamilton, feel free to visit our interactive browser demo at\n",(0,a.jsx)(t.a,{href:"https://www.tryhamilton.dev/",children:"https://www.tryhamilton.dev/"})]}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"building-a-modular-rag-application",children:"Building a modular RAG application"}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Alt text",src:n(7559).Z+"",width:"981",height:"671"})}),"\n",(0,a.jsxs)(t.p,{children:["Our example RAG application allows users to import PDF files, extract and store the text chunks, and query the system. These different operations are implemented as dataflows with Hamilton and are exposed via FastAPI endpoints. The backend communicates with OpenAI to embed documents and generate answers, and uses a local Weaviate vector store instance to store and retrieve documents. The frontend is built with Streamlit and exposes the different functionalities via a simple web user interface (UI). Everything is packaged as containers with ",(0,a.jsx)(t.code,{children:"docker compose"}),", so you can run it anywhere Docker runs."]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["Find the example on ",(0,a.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/retrieval_augmented_generation",children:"GitHub"})]}),"\n"]}),"\n",(0,a.jsx)(t.p,{children:"Let\u2019s walk through its structure:"}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:'language-title="Directory',metastring:'structure"',children:".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 .env\n\u251c\u2500\u2500 build_app.sh\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 backend\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 ingestion.py\n\u2502   \u251c\u2500\u2500 retrieval.py\n\u2502   \u251c\u2500\u2500 server.py\n\u2502   \u251c\u2500\u2500 vector_db.py\n\u2502   \u2514\u2500\u2500 docs\n\u2502       \u251c\u2500\u2500 documents.png\n\u2502       \u251c\u2500\u2500 rag_summary.png\n\u2502       \u251c\u2500\u2500 store_arxiv.png\n\u2502       \u2514\u2500\u2500 store_pdfs.png\n\u2514\u2500\u2500 frontend\n    \u251c\u2500\u2500 Dockerfile\n    \u251c\u2500\u2500 requirements.txt\n    \u251c\u2500\u2500 assets\n    \u2502   \u2514\u2500\u2500 hamilton_logo.png\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 client.py\n    \u251c\u2500\u2500 Information.py\n    \u2514\u2500\u2500 pages\n        \u251c\u2500\u2500 1_Ingestion.py\n        \u2514\u2500\u2500 2_Retrieval.py\n"})}),"\n",(0,a.jsxs)(t.p,{children:["The repository is divided between the backend and the frontend, each with their own ",(0,a.jsx)(t.code,{children:"Dockerfile"})," and ",(0,a.jsx)(t.code,{children:"requirements.txt"}),". The backend has ",(0,a.jsx)(t.code,{children:"server.py"})," which defines the FastAPI endpoints, and several ",(0,a.jsx)(t.code,{children:".py"})," files containing the Hamilton functions for the RAG workflow that interact with the vector store. You can think of each of these modules analogous to \u201cchains\u201d you\u2019d find in Langchain. The frontend has ",(0,a.jsx)(t.code,{children:"client.py"})," which handles ",(0,a.jsx)(t.code,{children:"HTTP"})," requests to the backend and a ",(0,a.jsx)(t.a,{href:"https://docs.streamlit.io/library/get-started/multipage-apps/create-a-multipage-app",children:"multipage Streamlit app"})," using ",(0,a.jsx)(t.code,{children:"Information.py"}),", ",(0,a.jsx)(t.code,{children:"pages/1_Ingestion.py"}),", and ",(0,a.jsx)(t.code,{children:"pages/2_Retrieval.py"}),".  The file ",(0,a.jsx)(t.code,{children:"Introduction.py"})," is the app entrypoint and landing page (it\u2019s not the clearest file name, but it defines how the page will be displayed within the Streamlit UI)."]}),"\n",(0,a.jsx)(t.h2,{id:"hamilton-dataflows-aka-chains",children:"Hamilton dataflows a.k.a. \u201cchains\u201d"}),"\n",(0,a.jsxs)(t.p,{children:["A RAG application can be divided into 3 main steps: ingestion, retrieval, and generation. Importantly, the ingestion step can be done at any time (e.g., continuously, periodically in batch, event-driven) while retrieval and generation always happen together in our case, i.e., when a user makes a query. For this reason, we implemented ingestion in ",(0,a.jsx)(t.code,{children:"ingestion.py"})," and retrieval + generation in ",(0,a.jsx)(t.code,{children:"retrieval.py"}),". This way, it would be trivial to reuse our Hamilton ingestion code in a macro-orchestrated pipeline for daily updates of, for example, any newly stored documents; see how to use Hamilton with ",(0,a.jsx)(t.a,{href:"../2023-06-28-airflow-hamilton/",children:"Airflow"}),", ",(0,a.jsx)(t.a,{href:"../2023-07-25-prefect-hamilton/",children:"Prefect"})," for ideas on how that would work."]}),"\n",(0,a.jsxs)(t.p,{children:["Also, you\u2019ll notice a separate ",(0,a.jsx)(t.code,{children:"vector_db.py"})," which implements a small set of functionalities to interact with Weaviate. These functions are quite simple, but it allows us to integrate our vector store operations to the broader dataflow, have granular visibility over operations, and handle exceptions. It also enables use to replace Weaviate easily if we wanted to choose another vector store. See ",(0,a.jsx)(t.a,{href:"../2023-07-11-modular-llm/",children:"our modular LLM stack post"})," for more details on how to do that."]}),"\n",(0,a.jsxs)(t.blockquote,{children:["\n",(0,a.jsxs)(t.p,{children:["Below we\u2019ll discuss our dataflow design decisions, for a more hands-on explanation of Hamilton, see our ",(0,a.jsx)(t.a,{href:"../2023-08-18-pdf-summarizer/",children:"PDF Summarizer example"}),"."]}),"\n"]}),"\n",(0,a.jsx)(t.h3,{id:"ingestion-flow",children:"Ingestion flow"}),"\n",(0,a.jsxs)(t.p,{children:["The ingestion flow allows users to upload arbitrary PDF documents, extract text content, chunk it, get embeddings from OpenAI, and store text chunks with their embedding in Weaviate. To make this demo more engaging, we added functionalities to directly search ",(0,a.jsx)(t.a,{href:"https://arxiv.org/",children:"arxiv.org"})," and store the selected scientific articles."]}),"\n",(0,a.jsxs)(t.p,{children:["Below is the DAG for ",(0,a.jsx)(t.code,{children:"ingestion.py"}),"\n",(0,a.jsx)(t.img,{alt:"Alt text",src:n(5310).Z+"",width:"1118",height:"1349"})]}),"\n",(0,a.jsxs)(t.p,{children:["Nodes with dotted outlines and the ",(0,a.jsx)(t.code,{children:"Input:"})," prefix are external to the ",(0,a.jsx)(t.code,{children:"ingestion.py"})," module and need to be provided to the ",(0,a.jsx)(t.code,{children:"Hamilton Driver"})," as inputs at execution time. Doubled lines and the different arrow types describe ",(0,a.jsx)(t.a,{href:"https://hamilton.dagworks.io/en/latest/concepts/hamilton-function-structure/#dynamic-dags",children:"parallelizable code paths"}),", which can be used to improve processing performance. Towards the bottom, ",(0,a.jsx)(t.code,{children:"pdf_collection"})," collects all the chunks and stores them in a single ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate/tutorials/schema",children:"structured object in Weaviate"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["Notice the path ",(0,a.jsx)(t.code,{children:"local_pdfs"})," (near the center of the image) is part of; everything above is arXiv specific, and below is generic. This design decision allows us to reuse the same logic to store  PDFs from arXiv as any other PDF. As our application grows, we could store the arxiv functions in a separate ",(0,a.jsx)(t.code,{children:"arxiv.py"})," module, and add code upstream of ",(0,a.jsx)(t.code,{children:"local_pdfs"})," to load files from other sources. For example, you can visually imagine \u201ccutting\u201d the above diagram at ",(0,a.jsx)(t.code,{children:"local_pdfs"}),", and then swapping in a different \u201cimplementation\u201d, i.e. dataflow."]}),"\n",(0,a.jsxs)(t.p,{children:["Also, we don\u2019t do any sophisticated text processing, but we could easily add more functions between ",(0,a.jsx)(t.code,{children:"raw_text"})," and ",(0,a.jsx)(t.code,{children:"chunked_text"}),". By removing irrelevant text from PDFs (e.g., article references, markdown tables), we could reduce the amount of tokens to process, send to OpenAI, and to then store in our vector store. Adopting a smart processing strategy will both improve the quality of retrieval and lead to performance and cost optimization at scale."]}),"\n",(0,a.jsxs)(t.p,{children:["Changes to the ingestion dataflow (e.g., preprocessing, chunking, embedding model) should generally be followed by reprocessing all documents and recomputing embeddings. Visualizing downstream dependencies of changes is helpful to prevent breaking changes. For example, changing the embedding model will make stored documents incompatible by changing the notion of distance for retrieval and possibly having incompatible vector dimensions. Using the ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate/configuration/backups",children:"Weaviate backup features"})," can make rollbacks easier and prevent having to re-compute embeddings and summaries, saving dollars and headaches."]}),"\n",(0,a.jsx)(t.h3,{id:"retrieval-flow",children:"Retrieval flow"}),"\n",(0,a.jsxs)(t.p,{children:["The retrieval flow can appear complicated at first, but essentially it starts by getting the text embedding for the user query and doing a ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/academy/zero_to_mvp/queries_2/hybrid",children:"hybrid search"})," in Weaviate to find the most relevant stored chunks. For each chunk, it checks if a text summary was already generated; if not, the chunk is passed with a prompt to OpenAI\u2019s chat model to generate a summary. The generated summaries of all chunks are collected and sorted according to the original Weaviate relevance ranking, to then be sent again to OpenAI to \u201creduce\u201d summaries, i.e., make a summary of summary."]}),"\n",(0,a.jsxs)(t.p,{children:["Here is the DAG for retrieval.py\n",(0,a.jsx)(t.img,{alt:"Alt text",src:n(6240).Z+"",width:"1456",height:"768"})]}),"\n",(0,a.jsxs)(t.p,{children:["A key decision was to specify prompt templates as function nodes (e.g., ",(0,a.jsx)(t.code,{children:"prompt_to_summarize_chunk"}),", ",(0,a.jsx)(t.code,{children:"prompt_to_reduce_summaries"}),") and not inputs (dotted outline). These prompt functions receive relevant context (e.g., text chunk), adds it to an f-string, and returns the formatted string. By storing them directly with the dataflow code, assessing behavior, versioning and debugging become much easier. We go into greater detail on how to manage prompts in ",(0,a.jsx)(t.a,{href:"https://blog.dagworks.io/p/llmops-production-prompt-engineering",children:"LLMOps: Production prompt engineering patterns with Hamilton"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["As shown in the figure, the ",(0,a.jsx)(t.code,{children:"rag_query"})," from the user only affects the initial chunk vector search and the ",(0,a.jsx)(t.code,{children:"rag_summary"})," step which makes a \u201csummary of summaries\u201d. Accordingly, the function to summarize a chunk ",(0,a.jsx)(t.code,{children:"chunk_with_new_summary"})," doesn\u2019t depend on the user\u2019s input allowing us to generate \u201cgeneric\u201d summaries for each chunk. This approach has the benefits of making chunk summaries reusable which can largely decrease cost and reduce latency. The downside is that the chunk summaries are less specific to the query and might decrease answer quality."]}),"\n",(0,a.jsx)(t.h3,{id:"weaviate-vector-database",children:"Weaviate vector database"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate",children:"Weaviate"})," offers a specialized type of infrastructure to efficiently store and compare text embeddings (i.e., vectors) at scale. In the context of RAG, the LLM represents the semantic of chunks of text as vectors, and the vector database defines the notion of similarity or relevance. Vector databases come in ",(0,a.jsx)(t.a,{href:"https://towardsdatascience.com/milvus-pinecone-vespa-weaviate-vald-gsi-what-unites-these-buzz-words-and-what-makes-each-9c65a3bd0696",children:"various forms"}),", but we decided to use Weaviate for a few reasons:"]}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Classes and structured objects"}),". For each PDF, we create a ",(0,a.jsx)(t.code,{children:"Document"})," object and ",(0,a.jsx)(t.code,{children:"Chunk"})," objects and link them together with their respective properties ",(0,a.jsx)(t.code,{children:"containsChunk"})," and ",(0,a.jsx)(t.code,{children:"fromDocument"}),". With structured objects, we can retrieve a ",(0,a.jsx)(t.code,{children:"Document"})," based on the relevance score of its ",(0,a.jsx)(t.code,{children:"Chunks"}),"; for example, by computing the ",(0,a.jsx)(t.code,{children:"groupby sum"})," of the relevance of its chunks."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Vectors and data in one place"}),". Weaviate allows you to store the text along the vectors while other vector infrastructure only handle vectors. The latter requires managing and syncing a separate storage for documents which complexifies both ingestion and retrieval. Many data types are supported (e.g., strings, numbers, boolean, dates); we even store the full PDF of each ",(0,a.jsx)(t.code,{children:"Document"})," as a ",(0,a.jsx)(t.code,{children:"base64"})," blob."]}),"\n"]}),"\n",(0,a.jsxs)(t.li,{children:["\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Expressive retrieval"}),". Weaviate has a REST API for bulk operations and a GraphQL API for object retrieval. While learning GraphQL can be daunting, using it from the Python SDK is easy to approach (see example below). Additionally, many modes of retrieval are offered (vector, multimodal, keyword, etc.). For this RAG example, we used the ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate/search/hybrid",children:"hybrid search"}),", which combines vector and keyword search."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="Weaviate GraphQL query"',children:'response = (\n    weaviate_client.query.get(\n        "Chunk",  # Class\n        [  # Properties to retrieve\n            "chunk_index",\n            "content",\n            "summary",\n            "fromDocument {... on Document {_additional{id}}}",  # Property of linked object\n        ],\n    )\n    .with_hybrid(  # hybrid search parameters\n        query=rag_query,  # user text query\n        properties=["content"],  # properties of `Chunk` to search on (specified above)\n        vector=query_embedding,  # user query embedding/vector\n        alpha=hybrid_search_alpha,  # hybrid search parameter\n    )\n    .with_additional(["score"])  # compute relevance score\n    .with_limit(retrieve_top_k)  # return top k objects\n    .do()\n)\n'})}),"\n",(0,a.jsx)(t.h3,{id:"fastapi-server",children:"FastAPI server"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," is used to define the server endpoints: ",(0,a.jsx)(t.code,{children:"/store_arxiv"}),", ",(0,a.jsx)(t.code,{children:"/store_pdfs"}),", ",(0,a.jsx)(t.code,{children:"/rag_summary"}),", ",(0,a.jsx)(t.code,{children:"/documents"}),", but for all of them, the code executed is actually handled by Hamilton. At startup, the server instantiates a ",(0,a.jsx)(t.code,{children:"Hamilton Driver"})," with all the necessary Python modules (see ",(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/advanced/events/",children:"FastAPI Lifespan Events"}),"). Then, the function body of FastAPI endpoint consists of accessing the global ",(0,a.jsx)(t.code,{children:"Hamilton Driver"}),", calling ",(0,a.jsx)(t.code,{children:"Driver.execute()"})," with the necessary variables, and returning a formatted response. Using Hamilton guarantees that the code powering your web service will run identically to your development notebooks or orchestrated pipelines."]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-python",metastring:'title="FastAPI server snippet"',children:'from contextlib import asynccontextmanager\nfrom dataclasses import dataclass\n\nimport fastapi\nimport pydantic\nfrom fastapi.responses import JSONResponse\n\nfrom hamilton import driver\n\n\n# define a global dataclass that is shared across endpoints\n@dataclass\nclass GlobalContext:\n    vector_db_url: str\n    hamilton_driver: driver.Driver\n\n\n@asynccontextmanager\nasync def lifespan(app: fastapi.FastAPI) -> None:\n    """Startup and shutdown logic of the FastAPI app\n    Above yield statement is at startup and below at shutdown\n    Import the Hamilton modules and instantiate the Hamilton driver\n    """\n    # import the Python modules containing your dataflows\n    import ingestion\n    import retrieval\n    import vector_db\n\n    driver_config = dict()\n\n    dr = (\n        driver.Builder()\n        .enable_dynamic_execution(allow_experimental_mode=True)  # to allow Parallelizable/Collect\n        .with_config(driver_config)\n        .with_modules(ingestion, retrieval, vector_db)  # pass our dataflows\n        .build()\n    )\n\n    # make the variable global to reuse it within endpoints\n    global global_context\n    global_context = GlobalContext(vector_db_url="http://weaviate_storage:8083", hamilton_driver=dr)\n\n    # execute Hamilton code to make sure the Weaviate class schemas is instantiated\n    global_context.hamilton_driver.execute(\n        ["initialize_weaviate_instance"], inputs=dict(vector_db_url=global_context.vector_db_url)\n    )\n\n    # anything above yield is executed at startup\n    yield\n    # anything below yield is executed at teardown\n\n\n# instantiate the FastAPI app\napp = fastapi.FastAPI(\n    title="Retrieval Augmented Generation with Hamilton",\n    lifespan=lifespan,  # pass the lifespan context\n)\n\n# define a POST endpoint\n@app.post("/store_arxiv", tags=["Ingestion"])\nasync def store_arxiv(arxiv_ids: list[str] = fastapi.Form(...)) -> JSONResponse:\n    """Retrieve PDF files of arxiv articles for arxiv_ids\\n\n    Read the PDF as text, create chunks, and embed them using OpenAI API\\n\n    Store chunks with embeddings in Weaviate.\n    """\n    global_context.hamilton_driver.execute(\n        ["store_documents"],\n        inputs=dict(\n            arxiv_ids=arxiv_ids,\n            embedding_model_name="text-embedding-ada-002",\n            data_dir="./data",\n            vector_db_url=global_context.vector_db_url,\n        ),\n    )\n\n    return JSONResponse(content=dict(stored_arxiv_ids=arxiv_ids))\n'})}),"\n",(0,a.jsxs)(t.p,{children:["The snippet also shows that we execute the ",(0,a.jsx)(t.code,{children:"Hamilton Driver"})," with ",(0,a.jsx)(t.code,{children:"\u201cinitialize_weaviate_instance\u201d"})," at startup to ensure the Weaviate schema exists and the vector store is available. Hamilton helps with keeping the endpoint functions brief and brings a nice separation of concerns. For example, if an error occurred, you can reproduce by using the logged request with Hamilton outside of FastAPI. If it succeeds, then the problem is likely related to the service and not the dataflow itself. We\u2019ll discuss unit testing and integration testing in a future post, make sure to ",(0,a.jsx)(t.a,{href:"https://blog.dagworks.io/subscribe",children:"subscribe"})," to be notified!"]}),"\n",(0,a.jsxs)(t.p,{children:["A core feature of Hamilton is the automatically generated DAG visualization, which complements FastAPI's automated ",(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/tutorial/metadata/",children:"Swagger UI documentation"}),". When running the example code, visit ",(0,a.jsx)(t.a,{href:"http://localhost:8082/docs",children:"http://localhost:8082/docs"})," to explore it yourself! FastAPI allows you to ",(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/tutorial/schema-extra-example/",children:"add request examples"})," to your code, which helps users learn your API and your team move faster as you develop and test things."]}),"\n",(0,a.jsx)(t.p,{children:(0,a.jsx)(t.img,{alt:"Alt text",src:n(5191).Z+"",width:"957",height:"1194"})}),"\n",(0,a.jsx)(t.h3,{id:"streamlit-frontend",children:"Streamlit frontend"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.a,{href:"https://docs.streamlit.io/",children:"Streamlit"})," allows you to build a user interface quickly using Python. Again, with the goal of modularity, we decided to build a ",(0,a.jsx)(t.a,{href:"https://docs.streamlit.io/library/get-started/multipage-apps/create-a-multipage-app",children:"multipage Streamlit app"})," with independent pages for information, ingestion, and retrieval. The file ",(0,a.jsx)(t.code,{children:"client.py"})," defines the HTTP requests to interact with the server, which helps errors related to the client-server communication from those associated with the UI. An important consideration when using Streamlit is that the entire code is executed whenever a page is refreshed, so avoid having operations that are computationally intensive or incur costs (e.g., LLM API calls)."]}),"\n",(0,a.jsxs)(t.p,{children:["The design of your search UI is a significant decision as it will largely ",(0,a.jsx)(t.a,{href:"https://www.algolia.com/blog/ux/7-examples-of-great-site-search-ui/",children:"influence how people use your application"}),". For the ingestion page, we made sure to display the currently stored documents and provide feedback via a \u201cspinner\u201d widget during long ingestion operations. For the retrieval page, we exposed hybrid search parameters such as alpha and top k with informational tooltips to allow users to play around, but kept hidden the prompts used to summarize documents. After making a RAG query, the app will display the query, the answer, and the source chunks with their summary to allow you to reason over the answer. After more than one query, a slider will allow you to browse through your history of searches and compare answers to different queries. Semantic search has enabled news ways to retrieve information across documents, but finding the ideal UI for it remains an unsolved problem and we encourage you to iterate over it!"]}),"\n",(0,a.jsx)(t.h3,{id:"docker-services",children:"Docker services"}),"\n",(0,a.jsxs)(t.p,{children:["Here\u2019s the ",(0,a.jsx)(t.code,{children:"docker-compose.yaml"})," file that manages three containers:"]}),"\n",(0,a.jsx)(t.pre,{children:(0,a.jsx)(t.code,{className:"language-yaml",metastring:'title="Docker compose file"',children:"version: \"3.4\"\nservices:\n  api:\n    container_name: fastapi_server\n    build: backend/.\n    command: \"uvicorn server:app --host 0.0.0.0 --port 8082\"\n    ports:\n      - \"8082:8082\"\n    environment:\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\n      - DAGWORKS_API_KEY=${DAGWORKS_API_KEY}\n    networks:\n      - rag\n\n  app:\n    container_name: streamlit_app\n    build: frontend/.\n    command: \"streamlit run --server.port 8080 --server.enableCORS false Information.py\"\n    ports:\n      - \"8080:8080\"\n    networks:\n      - rag\n\n  weaviate:\n    image: semitechnologies/weaviate:1.19.8\n    container_name: weaviate_storage\n    command: --host 0.0.0.0 --port '8083' --scheme http\n    ports:\n    - 8083:8083\n    restart: on-failure:0\n    environment:\n      QUERY_DEFAULTS_LIMIT: 25\n      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'\n      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'\n      DEFAULT_VECTORIZER_MODULE: 'none'\n      ENABLE_MODULES: ''\n      CLUSTER_HOSTNAME: 'node1'\n    networks:\n     - rag\n\nnetworks:\n  rag:\n"})}),"\n",(0,a.jsxs)(t.p,{children:["This sets the main configuration of each service. For the FastAPI backend (api service), we pass the OpenAI key to the FastAPI backend and optionally set the DAGWorks API key. The Weaviate configuration is generated by their ",(0,a.jsx)(t.a,{href:"https://weaviate.io/developers/weaviate/installation/docker-compose",children:"interactive tool"}),". Then, for each service the command section sets their entry point. They are all connected together via the ",(0,a.jsx)(t.code,{children:"rag"})," ",(0,a.jsx)(t.a,{href:"https://docs.docker.com/network/drivers/bridge/",children:"bridge network"}),". This allows services to communicate via a local URL of the following format ",(0,a.jsx)(t.code,{children:"http://{container_name}:{port}"})," (e.g., ",(0,a.jsx)(t.code,{children:"http://fastapi_server:8082"}),"). You could want to have your vector store less tightly coupled if it is used by other applications."]}),"\n",(0,a.jsxs)(t.p,{children:["When developing locally, containers can be accessed via",(0,a.jsx)(t.code,{children:" http://127.0.0.1:{port}"})," (try not to use ",(0,a.jsx)(t.a,{href:"https://www.youtube.com/watch?v=98SYTvNw1kw",children:"localhost"}),"). You can keep containers running and call ",(0,a.jsx)(t.code,{children:"docker compose up -d \u2013build"})," to rebuild them as you make changes. In particular, this is useful to view frontend UI changes and test backend changes manually via the Swagger UI at ",(0,a.jsx)(t.a,{href:"http://localhost:8082/docs",children:"http://localhost:8082/docs"}),".  To see container logs, you can use ",(0,a.jsx)(t.code,{children:"docker compose logs -f"}),"  to see the logs from applications from their respective containers."]}),"\n",(0,a.jsx)(t.h2,{id:"limitations",children:"Limitations"}),"\n",(0,a.jsx)(t.p,{children:"This example aims to be a reference architecture, and demonstrate how to build a RAG system and give you a solid basis to start your own project. However, it is by no means perfect. Here\u2019s a list of limitations or areas we could improve upon:"}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"arXiv downloads"}),". The ingestion dataflow for arXiv files requires downloading the PDF of articles locally (on the FastAPI container) before subsequent steps. That is PDF files could fill up your docker container. A better approach would be to use the built-in ",(0,a.jsx)(t.a,{href:"https://docs.python.org/3/library/tempfile.html",children:"tempfile library"})," for ",(0,a.jsx)(t.code,{children:"tempfile.NamedTemporaryFile"})," (note that tempfile is an area of active Python development and saw several changes since 3.8). For instance, PDF files sent to FastAPI (via Streamlit or POST) use temporary files via ",(0,a.jsx)(t.code,{children:"streamlit.runtime.UploadedFile"})," and ",(0,a.jsx)(t.code,{children:"fastapi.Uploadfile"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Weaviate duplicates"}),". Having duplicate sources in your vector store can reduce the quality of retrieval. You\u2019d most likely prefer the \u201c5 most-relevant, but somewhat distinct\u201d chunks from the \u201c5-most relevant and almost identical\u201d chunks to generate your answer. However, the presented RAG system doesn\u2019t prevent you from uploading duplicate documents during ingestion and finding duplicate / nearly files is a generally complex problem. One approach would be to first select the sets of chunks with very similar embeddings (potential duplicate) then use ",(0,a.jsx)(t.a,{href:"https://github.com/seatgeek/thefuzz",children:"fuzzy matching"})," (which is much less computationally efficient) on the chunk\u2019s text to score potential duplicates. The downside is that it requires compute and can impact latency having to do this for the embedding of every chunk before deduplication."]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"REST API conventions"}),". This example showcases a client-server architecture with the ingestion and retrieval of a RAG workflow. However, the design and the naming of the endpoints don\u2019t follow the ",(0,a.jsx)(t.a,{href:"https://restfulapi.net/resource-naming/",children:"REST best practices"}),". Following these conventions improves the semantics and the readability of your project, which is critical from proper downstream use. We will improve and update our API definition in our future post about testing. Make sure to subscribe to be notified when it we release it!"]}),"\n",(0,a.jsxs)(t.p,{children:[(0,a.jsx)(t.strong,{children:"Client-server communication"}),". You could use ",(0,a.jsx)(t.a,{href:"https://docs.pydantic.dev/latest/",children:"Pydantic"})," more extensively to define the FastAPI ",(0,a.jsx)(t.a,{href:"https://fastapi.tiangolo.com/tutorial/response-model",children:"requests and response types"}),". This code could be used in both the ",(0,a.jsx)(t.code,{children:"backend/server.py"})," and ",(0,a.jsx)(t.code,{children:"frontend/client.py"})," to make development less error prone."]}),"\n",(0,a.jsx)(t.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(t.p,{children:"We covered a lot in this post. Most importantly, we leave you with a reference architecture blueprint to get started with RAG applications. To use it, we suggest you should define your application\u2019s dataflows, then create the endpoints for the operations supported by your server, and finally build the best-suited user interface for your application. Congratulations for getting through! Please bookmark this post, and feel free to revisit sections of this post as you make progress; if something isn\u2019t clear please leave a comment/or suggest a PR to the repo to improve this example."}),"\n",(0,a.jsx)(t.p,{children:"You might be interested by other posts in this series:"}),"\n",(0,a.jsxs)(t.ul,{children:["\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"https://blog.dagworks.io/p/llmops-production-prompt-engineering",children:"LLMOps: Production prompt engineering patterns with Hamilton"})}),"\n",(0,a.jsx)(t.li,{children:(0,a.jsx)(t.a,{href:"../2023-07-11-modular-llm",children:"Building a maintainable and modular LLM application stack with Hamilton"})}),"\n"]}),"\n",(0,a.jsx)(t.h2,{id:"we-want-to-hear-from-you",children:"We want to hear from you!"}),"\n",(0,a.jsx)(t.p,{children:"If you\u2019re excited by any of this, or have strong opinions, leave a comment, or drop by our Slack channel! Some links to do praise/complain/chat:"}),"\n",(0,a.jsxs)(t.p,{children:["\ud83d\udce3 join our community on ",(0,a.jsx)(t.a,{href:"https://hamilton-opensource.slack.com/join/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg#/shared-invite/email",children:"Slack"})," \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started."]}),"\n",(0,a.jsxs)(t.p,{children:["\u2b50\ufe0f us on ",(0,a.jsx)(t.a,{href:"https://github.com/dagworks-inc/hamilton",children:"GitHub"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["\ud83d\udcdd leave us an ",(0,a.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/issues",children:"issue"})," if you find something."]}),"\n",(0,a.jsxs)(t.p,{children:["\ud83d\udcda read our ",(0,a.jsx)(t.a,{href:"https://hamilton.dagworks.io/en/latest/",children:"documentation"}),"."]}),"\n",(0,a.jsxs)(t.p,{children:["\u2328\ufe0f ",(0,a.jsx)(t.a,{href:"https://www.tryhamilton.dev/",children:"interactively learn"})," about Hamilton in your browser."]})]})}function h(e={}){const{wrapper:t}={...(0,i.a)(),...e.components};return t?(0,a.jsx)(t,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},3533:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-1-7516bf8851da2dedbffb09aebb4067a0.png"},7559:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-2-f3315f9fb72a623c8af50b067c4f2bdf.png"},5310:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-3-de79033c13d93f574841d88b5155715d.png"},6240:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-4-af39b77cbdec1eb458868cd45e206917.png"},5191:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-5-b51113d9f008e4d16e3d42d5da66e5cf.png"},9480:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/image-69d8bcd6fcc2be80b633fc87e3772d5b.png"},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>o});var a=n(7294);const i={},s=a.createContext(i);function o(e){const t=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(s.Provider,{value:t},e.children)}}}]);