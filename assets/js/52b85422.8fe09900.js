"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[6485],{8187:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>s,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var i=n(5893),a=n(1151);const o={slug:"pdf-summarizer",title:"Containerized PDF Summarizer with FastAPI and Hamilton",authors:"tj",tags:["Hamilton","OpenAI","LLM","FastAPI","Docker"]},s=void 0,r={permalink:"/personal_website/blog/pdf-summarizer",source:"@site/blog/2023-08-18-pdf-summarizer/index.md",title:"Containerized PDF Summarizer with FastAPI and Hamilton",description:"Skip learning convoluted LLM-specific frameworks and write your first LLM application using regular Python functions and Hamilton! In this post, we\u2019ll present a containerized PDF summarizer powered by the OpenAI API. Its flow is encoded in Hamilton, which the FastAPI backend runs and exposes as an inference endpoint. The lightweight frontend uses Streamlit and exercises the backend. (GitHub repo)",date:"2023-08-18T00:00:00.000Z",formattedDate:"August 18, 2023",tags:[{label:"Hamilton",permalink:"/personal_website/blog/tags/hamilton"},{label:"OpenAI",permalink:"/personal_website/blog/tags/open-ai"},{label:"LLM",permalink:"/personal_website/blog/tags/llm"},{label:"FastAPI",permalink:"/personal_website/blog/tags/fast-api"},{label:"Docker",permalink:"/personal_website/blog/tags/docker"}],readingTime:14.58,hasTruncateMarker:!0,authors:[{name:"Thierry Jean",url:"https://github.com/zilto",imageURL:"https://github.com/zilto.png",key:"tj"}],frontMatter:{slug:"pdf-summarizer",title:"Containerized PDF Summarizer with FastAPI and Hamilton",authors:"tj",tags:["Hamilton","OpenAI","LLM","FastAPI","Docker"]},unlisted:!1,prevItem:{title:"Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and Hamilton!",permalink:"/personal_website/blog/rag"},nextItem:{title:"Featurization: Integrating Hamilton with Feast",permalink:"/personal_website/blog/feast-hamilton"}},l={authorsImageUrls:[void 0]},d=[{value:"The first generation of large language models applications",id:"the-first-generation-of-large-language-models-applications",level:2},{value:"Hamilton for LLM and NLP flows",id:"hamilton-for-llm-and-nlp-flows",level:2},{value:"Building a modular PDF summarizer application",id:"building-a-modular-pdf-summarizer-application",level:2},{value:"To recap",id:"to-recap",level:2},{value:"What\u2019s next",id:"whats-next",level:2},{value:"Testing: it\u2019s important and with Hamilton it\u2019s simpler",id:"testing-its-important-and-with-hamilton-its-simpler",level:3},{value:"Spark: scale up your Hamilton dataflow with the Spark executor",id:"spark-scale-up-your-hamilton-dataflow-with-the-spark-executor",level:3},{value:"RAG: expand your Hamilton application to cover ingestion and retrieval",id:"rag-expand-your-hamilton-application-to-cover-ingestion-and-retrieval",level:3},{value:"Want lineage, catalog, and observability out of the box? Use Hamilton + DAGWorks.",id:"want-lineage-catalog-and-observability-out-of-the-box-use-hamilton--dagworks",level:2},{value:"We want to hear from you!",id:"we-want-to-hear-from-you",level:2}];function c(e){const t={a:"a",blockquote:"blockquote",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.p,{children:["Skip learning convoluted LLM-specific frameworks and write your first LLM application using regular Python functions and ",(0,i.jsx)(t.a,{href:"https://github.com/dagWorks-Inc/hamilton",children:"Hamilton"}),"! In this post, we\u2019ll present a containerized PDF summarizer powered by the ",(0,i.jsx)(t.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API"}),". Its flow is encoded in Hamilton, which the ",(0,i.jsx)(t.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," backend runs and exposes as an inference endpoint. The lightweight frontend uses ",(0,i.jsx)(t.a,{href:"https://docs.streamlit.io/",children:"Streamlit"})," and exercises the backend. (",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer",children:"GitHub repo"}),")"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(228).Z+"",width:"975",height:"646"})}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:["crosspost from ",(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/containerized-pdf-summarizer-with",children:"https://blog.dagworks.io/p/containerized-pdf-summarizer-with"})]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"the-first-generation-of-large-language-models-applications",children:"The first generation of large language models applications"}),"\n",(0,i.jsxs)(t.p,{children:["Large language models (LLMs) open up new opportunities to exploit unstructured text data and ways to interact with the computer, such as chat-based information retrieval, writing assistance, or text summarization. These models can complete many different tasks that need to be specified via a text prompt. This flexibility in terms of input and output differs from previous ML/AI initiatives in industry (e.g., forecasting, recommender systems, computer vision) that had precisely defined inputs and outputs formats. It is one of the central ",(0,i.jsx)(t.a,{href:"https://huyenchip.com/2023/04/11/llm-engineering.html",children:"challenges and complexities"})," introduced by LLMs, and furthers the need for traceability and validation steps for data pipelines."]}),"\n",(0,i.jsx)(t.p,{children:"Accordingly, the tooling to solve these problems is nascent and changes rapidly. Not only is the best development paradigm undefined, it is a moving target since LLMs themselves are evolving. Building an application using LLMs today will almost certainly lead to dealing with breaking changes or migrations around the LLM APIs, prompt versioning, context management, storage infrastructure (e.g., vector databases), monitoring frameworks, and other service vendors."}),"\n",(0,i.jsx)(t.p,{children:"In their current state, the available LLM frameworks might get you up and running, but are missing the modularity and the transparency required for a proper software development lifecycle that considers production operations."}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"How do you iterate & test the behavior of new prompts and version them?"}),"\n",(0,i.jsx)(t.li,{children:"Do you version your prompt with the code & LLM used? How coupled are they?"}),"\n",(0,i.jsx)(t.li,{children:"How do you run your workflows in a batch setting without a web-server?"}),"\n",(0,i.jsx)(t.li,{children:"How do you monitor your system\u2019s performance and data artifacts produced without slowing down your development cycle?"}),"\n",(0,i.jsx)(t.li,{children:"How do you structure a readable codebase that facilitates collaboration and allows you to understand the impacts of changes on your various workflows?"}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["We\u2019ll explore through a series of posts how to overcome these challenges when adding LLM capabilities into your application. In this article, we ground the discussion around a PDF summarizer and future posts will extend the example with testing, Spark support, lineage, etc. ",(0,i.jsx)(t.strong,{children:"Subscribe to get updates as we publish them"}),"!"]}),"\n",(0,i.jsx)(t.h1,{id:"introducing-hamilton",children:"Introducing Hamilton"}),"\n",(0,i.jsxs)(t.p,{children:["LLMs-based applications can be expressed as ",(0,i.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Dataflow_programming",children:"dataflows"}),", which boils down to modeling your program by focusing on the moving of data artifacts (prompts, context, knowledge base, generated response, etc.) with computation (what you do with the data)."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.a,{href:"https://github.com/dagWorks-Inc/hamilton",children:"Hamilton"})," is a declarative micro-framework to describe dataflows in Python. Its strength is expressing the flow of data and computation in a straightforward and easy to maintain manner (much like dbt does for SQL). It has minimal dependencies and can run anywhere Python runs, meaning the same code will work in development notebooks, scripts, Spark clusters, or production web-services. Hamilton is not a new framework (3.5+ years old), and has been used for years in production modeling data & machine learning dataflows."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(3087).Z+"",width:"1037",height:"682"})}),"\n",(0,i.jsx)(t.p,{children:"The picture above encapsulates the function-centric declarative approach of Hamilton. The function\u2019s name is tied to its outputs and its arguments define what data it depends on. This allows Hamilton to read functions found in a module and automatically generate the DAG to be executed. This paradigm incentivizes developers to write small modular functions instead of scripts or larger functions, without sacrificing iteration speed. As a result, it is easier to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Read and understand the codebase"}),"\n",(0,i.jsx)(t.li,{children:"Edit implementations and extend your business logic"}),"\n",(0,i.jsx)(t.li,{children:"Do data validation after key steps"}),"\n",(0,i.jsx)(t.li,{children:"Understand downstream consequences of changes"}),"\n",(0,i.jsx)(t.li,{children:"Unit test and prevent breaking changes"}),"\n",(0,i.jsx)(t.li,{children:"Reuse functions or groups of functions across projects"}),"\n",(0,i.jsx)(t.li,{children:"Add in platform concerns independent of the logic encoded with Hamilton."}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Again, to keep this post short and focused, we won\u2019t dive into how to do all the above."}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsxs)(t.p,{children:["If you have never tried Hamilton, feel free to visit our interactive browser demo at: ",(0,i.jsx)(t.a,{href:"https://www.tryhamilton.dev/",children:"https://www.tryhamilton.dev/"})]}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"hamilton-for-llm-and-nlp-flows",children:"Hamilton for LLM and NLP flows"}),"\n",(0,i.jsx)(t.p,{children:"Given the rapid progress in the LLM and tooling space, adopting a low abstraction framework like Hamilton for your application facilitates writing a modular and well-tested codebase, as well as a straightforward approach to versioning flows. Having your focus on modularity early on will facilitate future upgrades and migrations, and allow you to keep up with state-of-the-art without breaking production."}),"\n",(0,i.jsx)(t.p,{children:"For LLM applications, being able to reuse code logic between development (e.g. notebooks, scripts) and production services (FastAPI, serverless services, Spark) has a large positive impact on development speed. Also, Hamilton decouples your LLM dataflow logic from your service/platform concerns. For example, the caching of OpenAI requests, application scaling, or monitoring, are platform concerns and should be separated as such. This decoupling is also helpful in a hand-off model if you operate in one; data scientists work on Hamilton modeling logic while engineers handle ensuring it runs with the appropriate monitoring in production. As you read this post, we invite you to think about how the code we show might develop and evolve in your organization \u2013 leave us a comment afterwards with your thoughts/reactions."}),"\n",(0,i.jsx)(t.h2,{id:"building-a-modular-pdf-summarizer-application",children:"Building a modular PDF summarizer application"}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(2230).Z+"",width:"961",height:"461"})}),"\n",(0,i.jsxs)(t.p,{children:["At the core, the application loads a PDF as text, chunks it, and calls the ",(0,i.jsx)(t.a,{href:"https://platform.openai.com/docs/api-reference",children:"OpenAI API"})," to summarize the chunks and reduces them into a single summary. It is packaged as a frontend and a backend container using docker-compose. The frontend uses ",(0,i.jsx)(t.a,{href:"https://docs.streamlit.io/",children:"Streamlit"}),", a library to write web UI using Python. When clicking on the ",(0,i.jsx)(t.code,{children:"Summarize"})," button (see introduction image), an ",(0,i.jsx)(t.code,{children:"HTTP POST"})," request ",(0,i.jsx)(t.code,{children:"/summarize_sync"})," is made to the FastAPI backend. ",(0,i.jsx)(t.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI"})," is a library to create REST API endpoints to communicate with a server. When receiving the ",(0,i.jsx)(t.code,{children:"/summarize_sync"})," request, the endpoint executes the relevant operation via an Hamilton driver instantiated on the server."]}),"\n",(0,i.jsxs)(t.blockquote,{children:["\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer",children:"Find the code on GitHub"})}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"First, let\u2019s look at the directory structure:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:'language-title="Directory',metastring:'structure"',children:".\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 docker-compose.yaml\n\u251c\u2500\u2500 backend\n\u2502   \u251c\u2500\u2500 Dockerfile\n\u2502   \u251c\u2500\u2500 requirements.txt\n\u2502   \u251c\u2500\u2500 server.py\n\u2502   \u2514\u2500\u2500 summarization.py\n\u2514\u2500\u2500 frontend\n    \u251c\u2500\u2500 Dockerfile\n    \u251c\u2500\u2500 requirements.txt\n    \u2514\u2500\u2500 app.py\n"})}),"\n",(0,i.jsxs)(t.p,{children:["At a high-level, the frontend and the backend are in separate folders, each with their own ",(0,i.jsx)(t.code,{children:"Dockerfile"})," and ",(0,i.jsx)(t.code,{children:"requirements.txt"}),". The backend has ",(0,i.jsx)(t.code,{children:"server.py"})," which contains the FastAPI endpoints definition and the calls to the Hamilton driver, while ",(0,i.jsx)(t.code,{children:"summarization.py"})," contains the dataflow logic used by the ",(0,i.jsx)(t.code,{children:"Hamilton Driver"}),". If you\u2019re coming from LangChain, you can think of ",(0,i.jsx)(t.code,{children:"summarization.py"})," as the implementation of a more ",(0,i.jsx)(t.a,{href:"https://python.langchain.com/docs/modules/chains/",children:"modular chain"}),". The directory structure should feel natural and intuitive, making it easy to understand for any junior colleague joining your team."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsxs)(t.p,{children:["Now, let\u2019s look at snippets of ",(0,i.jsx)(t.code,{children:"summarization.py"})," and then ",(0,i.jsx)(t.code,{children:"server.py"})," powering the backend:"]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",metastring:'title="Hamilton dataflows"',children:'# summarization.py\n# ... imports \n\ndef summarize_chunk_of_text_prompt(content_type: str = "an academic paper") -> str:\n    """Base prompt for summarizing chunks of text."""\n    return f"Summarize this text from {content_type}. Extract any key points with reasoning.\\n\\nContent:"\n\n  \ndef summarize_text_from_summaries_prompt(content_type: str = "an academic paper") -> str:\n    """Prompt for summarizing a paper from a list of summaries."""\n    return f"""Write a summary collated from this collection of key points extracted from {content_type}.\n    The summary should highlight the core argument, conclusions and evidence, and answer the user\'s query.\n    User query: {{query}}\n    The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n    Key points:\\n{{results}}\\nSummary:\\n"""\n\n\n@config.when(file_type="pdf")\ndef raw_text__pdf(pdf_source: str | bytes | tempfile.SpooledTemporaryFile) -> str:\n    """Takes a filepath to a PDF and returns a string of the PDF\'s contents\n    :param pdf_source: Series of filepaths to PDFs\n    :return: Series of strings of the PDFs\' contents\n    """\n    reader = PdfReader(pdf_source)\n    _pdf_text = ""\n    page_number = 0\n    for page in reader.pages:\n        page_number += 1\n        _pdf_text += page.extract_text() + f"\\nPage Number: {page_number}"\n    return _pdf_text\n\n# ... \n  \ndef chunked_text(\n    raw_text: str, max_token_length: int = 1500, tokenizer_encoding: str = "cl100k_base"\n) -> list[str]:\n    """Chunks the pdf text into smaller chunks of size max_token_length.\n    :param pdf_text: the Series of individual pdf texts to chunk.\n    :param max_token_length: the maximum length of tokens in each chunk.\n    :param tokenizer_encoding: the encoding to use for the tokenizer.\n    :return: Series of chunked pdf text. Each element is a list of chunks.\n    """\n    tokenizer = tiktoken.get_encoding(tokenizer_encoding)\n    _encoded_chunks = _create_chunks(raw_text, max_token_length, tokenizer)\n    _decoded_chunks = [tokenizer.decode(chunk) for chunk in _encoded_chunks]\n    return _decoded_chunks\n\n# ... \n  \ndef summarized_text(\n    prompt_and_text_content: str,\n    openai_gpt_model: str,\n) -> str:\n    """Summarizes the text from the summarized chunks of the pdf.\n    :param prompt_and_text_content: the prompt and content to send over.\n    :param openai_gpt_model: which openai gpt model to use.\n    :return: the string response from the openai API.\n    """\n    response = openai.ChatCompletion.create(\n        model=openai_gpt_model,\n        messages=[\n            {\n                "role": "user",\n                "content": prompt_and_text_content,\n            }\n        ],\n        temperature=0,\n    )\n    return response["choices"][0]["message"]["content"]\n\n\nif __name__ == "__main__":\n    # run as a script to test Hamilton\'s execution\n    import summarization\n\n    from hamilton import base, driver\n\n    dr = driver.Driver(\n        {},\n        summarization,\n        adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n    )\n    dr.display_all_functions("summary", {"format": "png"})\n'})}),"\n",(0,i.jsxs)(t.p,{children:["Notice the chunking of code into functions. Each one has a clear role and shouldn\u2019t have too many lines of code. It\u2019s easy to understand a function\u2019s purpose through its name, type annotations, docstring, and dependencies specified as arguments. As a bonus, Hamilton can produce a visualization of the module\u2019s execution DAG for free! The top-level nodes, such as ",(0,i.jsx)(t.code,{children:"user_query"})," and ",(0,i.jsx)(t.code,{children:"openai_gpt_model"}),", are exposed to the user through the frontend and are processed down the DAG via FastAPI."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(4516).Z+"",width:"1308",height:"347"})}),"\n",(0,i.jsxs)(t.p,{children:["Also, you might have noticed at the end the ",(0,i.jsx)(t.code,{children:"if _name__ == \u201c__main__\u201d:"})," and the module importing itself via ",(0,i.jsx)(t.code,{children:"import summarization"}),". This effectively allows you to load the module and execute it with Hamilton ",(0,i.jsx)(t.em,{children:"outside"})," of the FastAPI server. This makes it easy to run and iterate over your Hamilton transformations during development, or ",(0,i.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Unit_testing",children:"unit test"})," them outside of your FastAPI web service."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.p,{children:"Now let\u2019s look at server.py:"}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",metastring:'title="FastAPI server"',children:'# server.py\n# ... imports\nimport summarization\n\n# instantiate FastAPI app\napp = fastapi.FastAPI()\n\n# define constants for Hamilton driver\ndriver_config = dict(\n    file_type="pdf",\n)\n\n# instantiate the Hamilton driver; it will power all API endpoints\n# async driver for use with async functions\nasync_dr = h_async.AsyncDriver(\n    driver_config,\n    summarization,  # python module containing function logic\n    result_builder=base.DictResult(),\n)\n# sync driver for use with regular functions\nsync_dr = driver.Driver(\n    driver_config,\n    summarization,  # python module containing function logic\n    adapter=base.SimplePythonGraphAdapter(base.DictResult()),\n)\n\n\nclass SummarizeResponse(pydantic.BaseModel):\n    """Response to the /summarize endpoint"""\n    summary: str\n\n\n@app.post("/summarize")\nasync def summarize_pdf(\n    pdf_file: fastapi.UploadFile,\n    openai_gpt_model: str = fastapi.Form(...),  # = "gpt-3.5-turbo-0613",\n    content_type: str = fastapi.Form(...),  # = "Scientific article",\n    user_query: str = fastapi.Form(...),  # = "Can you ELI5 the paper?",\n) -> SummarizeResponse:\n    """Request `summarized_text` from Hamilton driver with `pdf_file` and `user_query`"""\n    results = await async_dr.execute(\n        ["summarized_text"],\n        inputs=dict(\n            pdf_source=pdf_file.file,\n            openai_gpt_model=openai_gpt_model,\n            content_type=content_type,\n            user_query=user_query,\n        ),\n    )\n    return SummarizeResponse(summary=results["summarized_text"])\n\n\n@app.post("/summarize_sync")\ndef summarize_pdf_sync(\n    pdf_file: fastapi.UploadFile,\n    openai_gpt_model: str = fastapi.Form(...),  # = "gpt-3.5-turbo-0613",\n    content_type: str = fastapi.Form(...),  # = "Scientific article",\n    user_query: str = fastapi.Form(...),  # = "Can you ELI5 the paper?",\n) -> SummarizeResponse:\n    """Request `summarized_text` from Hamilton driver with `pdf_file` and `user_query`"""\n    results = sync_dr.execute(\n        ["summarized_text"],\n        inputs=dict(\n            pdf_source=pdf_file.file,\n            openai_gpt_model=openai_gpt_model,\n            content_type=content_type,\n            user_query=user_query,\n        ),\n    )\n    return SummarizeResponse(summary=results["summarized_text"])\n\n\n# add to SwaggerUI the execution DAG png\n# see http://localhost:8080/docs#/default/summarize_pdf_summarize_post\nbase64_viz = base64.b64encode(open("summarization_module.png", "rb").read()).decode("utf-8")\napp.routes[\n    -1\n].description = f"""<h1>Execution DAG</h1><img alt="" src="data:image/png;base64,{base64_viz}"/>"""\n\n\nif __name__ == "__main__":\n    # run as a script to test server locally\n    import uvicorn\n\n    uvicorn.run(app, host="0.0.0.0", port=8080)\n'})}),"\n",(0,i.jsxs)(t.p,{children:["The file server.py is responsible of instantiating FastAPI and defining the server\u2019s endpoints. We also instantiate ",(0,i.jsx)(t.em,{children:"synchronous"})," and ",(0,i.jsx)(t.em,{children:"asynchronous"})," Hamilton Driver objects to power both types of endpoint styles that FastAPI supports (",(0,i.jsx)(t.strong,{children:"Note"}),". ",(0,i.jsx)(t.a,{href:"https://fastapi.tiangolo.com/async/#in-a-hurry",children:"FastAPI manages non-async functions like async ones"}),"). Then, in the endpoints definitions, it is a matter of handling the request data, passing it to Hamilton and requesting the variable summarized_text, and sending a formatted response to the client."]}),"\n",(0,i.jsxs)(t.p,{children:["Towards the end of the snippet, there is a cryptic line of code that encodes the visualization of the Hamilton Driver as ",(0,i.jsx)(t.code,{children:"base64"})," and embeds it in the FastAPI generated Swagger UI for great documentation!"]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(9418).Z+"",width:"975",height:"1104"})}),"\n",(0,i.jsxs)(t.p,{children:["Again, you\u2019ll find a ",(0,i.jsx)(t.code,{children:"if _name__ == \u201c__main__\u201d:"})," statement allowing you to start a local server by calling python server.py. This way, you can test your server code independently from your Hamilton transformations and then add ",(0,i.jsx)(t.a,{href:"https://en.wikipedia.org/wiki/Integration_testing",children:"integration tests"}),"."]}),"\n",(0,i.jsxs)(t.p,{children:["To learn more about the frontend, we invite you to view ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/LLM_Workflows/pdf_summarizer",children:"the full example on GitHub"}),"."]}),"\n",(0,i.jsx)(t.h2,{id:"to-recap",children:"To recap"}),"\n",(0,i.jsx)(t.p,{children:"In this post we showed how to:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsx)(t.li,{children:"Break down a PDF summarizer flow into individual functions with Hamilton"}),"\n",(0,i.jsx)(t.li,{children:"Use Hamilton for inference with FastAPI, and exercise the required DAG inputs through POST requests"}),"\n",(0,i.jsx)(t.li,{children:"Decouple the code for the UI, server, and application logic to enable faster development"}),"\n",(0,i.jsx)(t.li,{children:"Structure a project\u2019s directory and code to help readability and maintainability"}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["The function-centric approach of Hamilton makes it easy to update or extend your application dataflow. For example, you could swap out OpenAI API for Anthropic (",(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/building-a-maintainable-and-modular",children:"Learn how to swap stack components"}),"), or add document processing steps by writing a few functions. With Hamilton being open source and being an extensible platform, motivated developers can tailor it to their needs for example by implementing custom caching strategies (e.g. ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/caching_nodes",children:"use this, or extend it"}),"), data validation steps (e.g. ",(0,i.jsx)(t.a,{href:"https://hamilton.dagworks.io/en/latest/reference/decorators/check_output/#check-output",children:"use/extend this"}),"), or telemetry capture."]}),"\n",(0,i.jsxs)(t.p,{children:["Like what you heard? We\u2019d love a ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton",children:"star on GitHub"})," and/or subscribe to this blog to get updates."]}),"\n",(0,i.jsx)(t.h2,{id:"whats-next",children:"What\u2019s next"}),"\n",(0,i.jsx)(t.p,{children:"Thanks for getting this far! As we mentioned in the beginning we\u2019ve got a bunch of exciting content planned\ud83e\udd73 Here\u2019s a few lines about some of our upcoming posts on how to build LLM-based applications for production. Subscribe to get updates as we publish them!"}),"\n",(0,i.jsx)(t.h3,{id:"testing-its-important-and-with-hamilton-its-simpler",children:"Testing: it\u2019s important and with Hamilton it\u2019s simpler"}),"\n",(0,i.jsx)(t.p,{children:"Deploying and maintaining an application in production requires great testing practices. You\u2019ll find that the functions and the dataflows defined with Hamilton are much friendlier to test than object-oriented frameworks. In this next blog, we\u2019ll add to the PDF summarizer unit tests for Hamilton functions, integration tests to use Hamilton with FastAPI, and data validation checks to ensure correct outputs from the LLMs. We\u2019ll also show you how to pull intermediate values from the DAG to plug into your monitoring system!"}),"\n",(0,i.jsx)(t.h3,{id:"spark-scale-up-your-hamilton-dataflow-with-the-spark-executor",children:"Spark: scale up your Hamilton dataflow with the Spark executor"}),"\n",(0,i.jsxs)(t.p,{children:["Did you know that Hamilton allows you to scale up your dataflow to run in a batch workflow without migrating your code? Your dataflow defined for FastAPI can also run on Spark! In the follow-up post, we\u2019ll show how to take the summarizer dataflow and run it on Spark to process lots of tables of data & PDFs in parallel. Operating efficiently at scale is important to be able to re-process all of your documents when upgrading your dataflow (e.g., computing embeddings with a new LLM). Also, this is a key feature to prevent ",(0,i.jsx)(t.a,{href:"https://www.hopsworks.ai/dictionary/online-offline-feature-skew",children:"implementation skew"})," between online & offline efforts."]}),"\n",(0,i.jsx)(t.h3,{id:"rag-expand-your-hamilton-application-to-cover-ingestion-and-retrieval",children:"RAG: expand your Hamilton application to cover ingestion and retrieval"}),"\n",(0,i.jsxs)(t.p,{children:["Instead of being passed in the PDF to summarize, we might have a corpus of documents to choose from to answer users queries. In this upcoming post, we\u2019ll extend the PDF summarizer to a typical ",(0,i.jsx)(t.a,{href:"https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html",children:"Retrieval Augmented Generation (RAG)"})," workflow. You\u2019ll see how Hamilton can manage multiple flows (ingestion, retrieval, summarization) and expose them via API endpoints for an end-to-end RAG application. In the meantime, ",(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/building-a-maintainable-and-modular",children:"this blog post"})," shows the precursor step of ingesting data into a vector database for RAG use with Hamilton."]}),"\n",(0,i.jsx)(t.h2,{id:"want-lineage-catalog-and-observability-out-of-the-box-use-hamilton--dagworks",children:"Want lineage, catalog, and observability out of the box? Use Hamilton + DAGWorks."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.img,{alt:"Alt text",src:n(7920).Z+"",width:"1456",height:"314"})}),"\n",(0,i.jsxs)(t.p,{children:["Lastly before we go, if you\u2019re interested in automatically capturing versions of flows, seeing inputs, executions, and observing what happened - you can try out what we\u2019re building at ",(0,i.jsx)(t.a,{href:"https://www.dagworks.io/",children:"DAGWorks"})," with a ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/server.py#L36-L45",children:"one-line replacement"}),"."]}),"\n",(0,i.jsx)(t.p,{children:"The DAGWorks platform helps you:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Version dataflows"}),". As you iterate and change the shape, contents and structure of your code, DAGWorks tracks changes and allows you to view diffs."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Monitor runs"}),". Understand which DAG nodes were executed, how long it took, what were the outputs, etc. from your dashboard."]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Debug failures"}),". DAG execution errors are contextualized with other node results and you can compare across multiple runs to view DAG changes."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["To get started, in the example we have a few commented out pieces of code you\u2019ll need to flip on (quick instructions ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/README.md#connecting-to-dagworks",children:"here"})," and DAGWorks docs ",(0,i.jsx)(t.a,{href:"https://docs.dagworks.io/introduction",children:"here"}),"):"]}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Install ",(0,i.jsx)(t.code,{children:"dagworks-sdk"})," \u2013 uncomment ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/requirements.txt#L11",children:"this line"})," in ",(0,i.jsx)(t.code,{children:"requirements.txt"}),"."]}),"\n",(0,i.jsxs)(t.li,{children:["Add the ",(0,i.jsx)(t.code,{children:"DAGWORKS_API_KEY"})," to ",(0,i.jsx)(t.code,{children:".env"})," \u2013 ",(0,i.jsx)(t.a,{href:"https://www.dagworks.io/pricing",children:"sign up"})," for an account there\u2019s a free tier! Create an ",(0,i.jsx)(t.code,{children:"API_KEY"})," and save it."]}),"\n",(0,i.jsxs)(t.li,{children:["Create a DAGWorks project \u2013 note the project ID (see docs ",(0,i.jsx)(t.a,{href:"https://docs.dagworks.io/introduction",children:"here"})," for how to do that)."]}),"\n",(0,i.jsxs)(t.li,{children:["Instantiate the DAGWorks Driver \u2013 ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/blob/main/examples/LLM_Workflows/pdf_summarizer/backend/server.py#L34-L45",children:"uncomment this code"})," in the server to instantiate the DAGWorks driver and fill in your project details."]}),"\n",(0,i.jsx)(t.li,{children:"Rebuild the containers and you\u2019re good to go!"}),"\n"]}),"\n",(0,i.jsx)(t.h2,{id:"we-want-to-hear-from-you",children:"We want to hear from you!"}),"\n",(0,i.jsx)(t.p,{children:"If you\u2019re excited by any of this, or have strong opinions, drop by our Slack channel / or leave some comments here! Some resources to get you help:"}),"\n",(0,i.jsxs)(t.p,{children:["\ud83d\udce3 join our community on ",(0,i.jsx)(t.a,{href:"https://join.slack.com/t/hamilton-opensource/shared_invite/zt-1bjs72asx-wcUTgH7q7QX1igiQ5bbdcg",children:"Slack"})," \u200a\u2014 \u200awe\u2019re more than happy to help answer questions you might have or get you started."]}),"\n",(0,i.jsxs)(t.p,{children:["\u2b50\ufe0f us on ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton",children:"GitHub"})]}),"\n",(0,i.jsxs)(t.p,{children:["\ud83d\udcdd leave us an ",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/issues",children:"issue"})," if you find something"]}),"\n",(0,i.jsx)(t.p,{children:"Other Hamilton posts you might be interested in:"}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://www.tryhamilton.dev/",children:"tryhamilton.dev"})," \u2013 an interactive tutorial in your browser!"]}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/building-a-maintainable-and-modular",children:"Build a modular LLM stack with Hamilton"})}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/publish/post/130538397",children:"Hamilton + Airflow"})," (",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/airflow",children:"GitHub repo"}),")"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/featurization-integrating-hamilton",children:"Hamilton + Feast"})," (",(0,i.jsx)(t.a,{href:"https://github.com/DAGWorks-Inc/hamilton/tree/main/examples/feast",children:"GitHub repo"}),")"]}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/how-to-use-hamilton-with-pandas-in-5-minutes-89f63e5af8f5",children:"Pandas data transformations in Hamilton in 5 minutes"})}),"\n",(0,i.jsx)(t.li,{children:(0,i.jsx)(t.a,{href:"https://blog.dagworks.io/p/lineage-hamilton-in-10-minutes-c2b8a944e2e6",children:"Lineage + Hamilton in 10 minutes"})}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,a.a)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},3087:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-1-6b730d286b040562659cc780fc12a680.png"},2230:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-2-820b7941404ea8a723889fc61be177ea.png"},4516:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-3-66d249a1970e698b173673a186f2b794.png"},9418:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-4-cb2d104268523d5785cac6c9d78e7dc6.png"},7920:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-5-3bf77f3353d59b0942067067948a777d.png"},228:(e,t,n)=>{n.d(t,{Z:()=>i});const i=n.p+"assets/images/image-8afd5b643e3596877a57c4549fba352b.png"},1151:(e,t,n)=>{n.d(t,{Z:()=>r,a:()=>s});var i=n(7294);const a={},o=i.createContext(a);function s(e){const t=i.useContext(o);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(o.Provider,{value:t},e.children)}}}]);